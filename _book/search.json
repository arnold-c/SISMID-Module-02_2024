[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SISMID Module 2 Materials (2024)",
    "section": "",
    "text": "Welcome\nHello!\nWelcome to this website that accompanies the 2024 Summer Institute in Modeling for Infectious Diseases (SISMID) Module 2: Mathematical Models of Infectious Diseases. This website contains both the lecture notes, so you can refer back to them at a later date, and the exercises that we will be completing throughout the module.",
    "crumbs": [
      "**Welcome**"
    ]
  },
  {
    "objectID": "index.html#workshop-pre-requisites",
    "href": "index.html#workshop-pre-requisites",
    "title": "SISMID Module 2 Materials (2024)",
    "section": "Workshop Pre-Requisites",
    "text": "Workshop Pre-Requisites\nAs there is a reasonable amount of material to cover, and a relatively short amount of time to cover it in, there are some pre-requisite tasks to get set up ahead of the workshop.\n\n\n\n\n\n\nImportant\n\n\n\nEveryone should refer to the Exercise Requirements page, particularly the R Packages and Data Files sections to ensure you have the neccessary packages and data files installed for the exercises.\n\n\nFirstly, this workshop requires some prior understanding of the R programming languages, as well as a working installation of R. If you do not have any experience, please refer to the Installing R and Just Enough RStudio & Just Enought R pages of the Pre-Requisites section.\nIt is also highly recommended that you read the Organizing A Project page for how to structure your code as you go through the exercises: we provide some guidelines to make your code easier to understand and navigate, both for yourself, and for others.\nPlease set aside about an hour to go through these materials and get set up. You should be able to do this in about 15 minutes if you are already experienced with R, but as with all things computational, it’s worth including some buffer time in case you run into issues. If you are completely new to R, this could take an hour or so, but is certainly worth the time investment: it will be hard to follow some of the exercises if you do not understand what the code is doing.",
    "crumbs": [
      "**Welcome**"
    ]
  },
  {
    "objectID": "index.html#tips-about-the-website",
    "href": "index.html#tips-about-the-website",
    "title": "SISMID Module 2 Materials (2024)",
    "section": "Tips About the Website",
    "text": "Tips About the Website\nThere are a number of useful features throughout this website to help you.\nFirstly, in sections where there is R code showing, clicking on the text ▶ Code above the code block will hide the code if open (the default), or show the code if hidden.\nSecondly, in the top right corner of each code block, there is a button that looks like a clipboard. Clicking this button will copy the code to your clipboard, so you can paste it into your own R session.\nFinally, within the code blocks (and, in fact, in the regular text like this section), functions (e.g. list(), pivot_longer()) that come from a package (i.e., ones we didn’t write) show up in a different color. In most cases, you can hover your cursor over them (on the part next the the parentheses e.g., ode() from the line deSolve::ode()), and if they become underlined, you can click on them to go to the documentation for that function. This is like searching for the documentation from your R console using ?ode.",
    "crumbs": [
      "**Welcome**"
    ]
  },
  {
    "objectID": "index.html#keywords-code-and-other-formatting",
    "href": "index.html#keywords-code-and-other-formatting",
    "title": "SISMID Module 2 Materials (2024)",
    "section": "Keywords, Code, and Other Formatting",
    "text": "Keywords, Code, and Other Formatting\nThroughout the book, you’ll see some keywords, code, and other points that I’ll try to delineate with the following formatting:\n\n\n\n\n\n\nNote\n\n\n\nThis will be a note, and will be used to highlight important points, or to provide additional information.\n\n\n\n\n\n\n\n\nSET\n\n\n\nThis will be an instruction to set certain parameters or values in the R code, or for the interactive plots.\n\n\n\n\n\n\n\n\nInstruction\n\n\n\nThis will be a general instruction.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis will be used to highlight a useful tip.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis will provide a warning that you may get an unexpected result if you’re not careful.\n\n\nIt is worth noting that some of these callouts may be collapsible. You can tell a callout is collapsible if there is a little &gt; or ⋁ in the top right corner of the callout i.e.,\n\n\n\n\n\n\nNote\n\n\n\n\n\nThis is an example of a collapsible callout that defaults to being collapsed.\n\n\n\n\n\nExercise:\n\nExercises that require you to complete missing sections of code, or answer a question, will be highlighted like this in R Session 2 and R Session 3\n\n\ncode will be used to highlight code.\n{package::function()} will be used to denote a specific package and function, e.g., {dplyr::mutate()} denotes the mutate() function from the {dplyr} package.\n\nOften the function() will be listed without the package, as there are library() calls at the top of each page indicating the packages used in that page, and the majority of functions will not cause conflicts, i.e., there are not packages with functions of the same name.\n\nBold will be used to highlight keywords and phrases, e.g., Git or GitHub.\n\nBold will also be highlighted in this way, e.g., commits or pushed being the result of the code git commit or git push\n\nBold-italics will be used to highlight file names, e.g., README.md or LICENSE.\nItalics will be used for emphasis in certain circumstances, e.g., signifying a question from an interactive terminal command.",
    "crumbs": [
      "**Welcome**"
    ]
  },
  {
    "objectID": "index.html#about-the-instructors",
    "href": "index.html#about-the-instructors",
    "title": "SISMID Module 2 Materials (2024)",
    "section": "About the Instructors",
    "text": "About the Instructors\n\n\n\nDr. Micaela Martinez\n\n\nDr. Martinez is the Director of Environmental Health at WE ACT for Environmental Justice.\nDr. Martinez earned her Ph.D. in Ecology & Evolution in 2015 at the University of Michigan, and did her postdoctoral training at Princeton University. Before joining WE ACT, she served as an Assistant Professor at Columbia University Mailman School of Public Health, followed by Emory University, in the Dept. of Biology. Since 2017, Dr. Martinez has been supported by the prestigious NIH Director’s Early Independence Award. Over the past decade, her research has focused on infectious disease ecology, climate change, maternal and infant health, social justice, and environmental impacts on health (including biological rhythms).\n\n\n\nDr. Matt Ferrari\n\n\nDr. Ferrari is the Director of the Center for Infectious Disease Dynamics at The Pennsylvania State University.\nDr Ferrari’s lab does research on both the application of quantitative modeling and analysis to inform public health policy and the basic ecology of parasites and infectious diseases at the Center for Infectious Disease Dynamics at The Pennsylvania State University.\n\n\n\n\n\n\n\n\n\nDeepit Bhatia\n\n\nDeepit is a 4th Year PhD student in Dr. Ferrari’s lab at The Pennsylvania State University.\nDeepit’s work aims to understand how dynamics of vaccine-preventable disease outbreaks change as we approach elimination.",
    "crumbs": [
      "**Welcome**"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Workshop Schedule",
    "section": "",
    "text": "Before class\nPlease see pre-requisites on the course website",
    "crumbs": [
      "Workshop Schedule"
    ]
  },
  {
    "objectID": "schedule.html#day-01-monday",
    "href": "schedule.html#day-01-monday",
    "title": "Workshop Schedule",
    "section": "Day 01 – Monday",
    "text": "Day 01 – Monday\n\n\n\n\n\n\n\nTime\nSection\n\n\n\n\n8:30 - 9:30am\nLecture 1: Intro to Modeling by Micaela Martinez\n\n\n9:30 - 10am\nLecture 2: Basics of SIR Models by Matt Ferrari (first half)\n\n\n10 - 10:30pm\ncoffee break\n\n\n10:30am - 11am\nLecture 2: Basics of SIR Models by Matt Ferrari (second half)\n\n\n11 - 12pm\nR Session 1: Intro to modeling by Deepit Bhatia\n\n\n12 - 1:30pm\nlunch on your own\n\n\n1:30pm - 2pm\nR Session 1: Intro to modeling (continued) by Deepit Bhatia\n\n\n2pm - 3pm\nLecture 3: Vaccination & Interventions by Micaela Martinez\n\n\n3pm - 3:30pm\nCoffee break\n\n\n3:30pm - 4:15pm\nFinish R exercises and work on group assignment for R Session 1.\n\n\n4:15pm - 5pm\nGroup Assignment debrief for R Session 1\n\n\n5pm - 7pm\nSISMID-wide networking mixer",
    "crumbs": [
      "Workshop Schedule"
    ]
  },
  {
    "objectID": "schedule.html#day-02-tuesday",
    "href": "schedule.html#day-02-tuesday",
    "title": "Workshop Schedule",
    "section": "Day 02 – Tuesday",
    "text": "Day 02 – Tuesday\n\n\n\n\n\n\n\nTime\nSection\n\n\n\n\n8:30am - 9:30am\nLecture 4: Heterogeneity in Models by Deepit Bhatia\n\n\n9:30 - 10am\nLecture 5: Heterogeneity expanded (age structure and the force of infection) by Matt Ferrari (first half)\n\n\n10 - 10:30am\nCoffee Break\n\n\n10:30am - 11am\nLecture 5: Heterogeneity expanded (age structure and the force of infection) by Matt Ferrari (second half)\n\n\n11 - 12am\nR Session 2: Heterogeneity and age structure by Deepit Bhatia\n\n\n12 - 1:30pm\nlunch on your own / SISMID-wide optional professional development module\n\n\n1:30 - 2:30pm\nLecture 6: Parameter Estimation (estimating \\(R_0\\)) by Matt Ferrari\n\n\n2:30 - 3:00pm\nR Session 3: Parameter estimation by Deepit Bhatia\n\n\n3pm - 3:30pm\nCoffee break\n\n\n3:30pm - 4:15pm\nFinish R exercises from the day and work on group assignment for R Sessions 2 and 3\n\n\n4:15pm - 5pm\nGroup Assignment debrief for R Session 2\n\n\n5pm - 7pm\nSISMID-wide social activity",
    "crumbs": [
      "Workshop Schedule"
    ]
  },
  {
    "objectID": "schedule.html#day-03-wednesday",
    "href": "schedule.html#day-03-wednesday",
    "title": "Workshop Schedule",
    "section": "Day 03 – Wednesday",
    "text": "Day 03 – Wednesday\n\n\n\n\n\n\n\nTime\nSection\n\n\n\n\n8:30am - 9:30am\nLecture 7: Confronting models with data by Micaela Martinez\n\n\n9:30am - 10am\nLecture 8: Stochasticity and Uncertainty lecture by Micaela Martinez and Matt Ferrari (first half)\n\n\n10am - 10:30am\nCoffee break\n\n\n10:30am - 11am\nLecture 8: Stochasticity and Uncertainty lecture by Micaela Martinez and Matt Ferrari (first half)\n\n\n11am - 12am\nGroup Assignment debrief for R Session 3 and any other discussion points\n\n\n\nClass Ends Noon on Wednesday",
    "crumbs": [
      "Workshop Schedule"
    ]
  },
  {
    "objectID": "schedule.html#group-assignments",
    "href": "schedule.html#group-assignments",
    "title": "Workshop Schedule",
    "section": "Group assignments",
    "text": "Group assignments\n\nEach individual should create a slide deck to address each of the following questions for each of the day’s R exercises.\nEach group should develop consensus slides to present when we come together as a class.\nGroups will be asked to share their intuition gained.\n\nQuestions:\n\nWhat were the top two take-away messages you gained from this R exercise?\nWhat was the most unexpected/non-intuitive thing you learned from this R exercise?",
    "crumbs": [
      "Workshop Schedule"
    ]
  },
  {
    "objectID": "exercise-requirements.html",
    "href": "exercise-requirements.html",
    "title": "Exercise Requirements",
    "section": "",
    "text": "R Packages\nIf you use renv to manage package dependencies in your projects, you can visit the GitHub repository for this project and download the renv.lock, .Rprofile, and renv/activate.R files, before running the command renv::restore(). Alternatively, if you already use GitHub, you could clone the project and just run renv::restore().\nIf you would prefer to just install the packages manually to avoid the complications associated with using renv, you can install the packages printed below.\nCodeinstall.packages(c(\n    \"tidyverse\",\n    \"deSolve\",\n    \"diagram\",\n    \"gt\",\n    \"ggtext\",\n    \"here\",\n    \"rio\"\n))",
    "crumbs": [
      "Pre-Requisites",
      "Exercise Requirements"
    ]
  },
  {
    "objectID": "exercise-requirements.html#data-files",
    "href": "exercise-requirements.html#data-files",
    "title": "Exercise Requirements",
    "section": "Data Files",
    "text": "Data Files\nTo complete the exercises, some data files are required. The exercises should load the datafiles via urls, but if you would prefer to download them to your own machine, your are welcome to do so. To download the files, go to the GitHub repository and download the files in the data/ folder of your repository. You should then uncomment the lines of code that run ...  &lt;- rio::import(here::here(\"data\", ...)), and comment out the lines that include the URLs.",
    "crumbs": [
      "Pre-Requisites",
      "Exercise Requirements"
    ]
  },
  {
    "objectID": "install-r.html",
    "href": "install-r.html",
    "title": "Install and Setup R & RStudio",
    "section": "",
    "text": "R\nTo start using R, you first need to install it, as it does not come bundled with your computer. The easiest way to do this is to visit CRAN and click on the link for your operating system (there are versions for Windows, Mac, and Linux).\nCRAN (Comprehensive R Archive Network) is a network of servers around the world that store identical, up-to-date, versions of code and documentation for R. This is where we will download R from, but also all the packages that we will use in this course. When you first try to install a package, you will be prompted to select your CRAN mirror. You can select any mirror, but it is best to choose one that is close to your location, as this will make the download faster.",
    "crumbs": [
      "Pre-Requisites",
      "Install and Setup R & RStudio"
    ]
  },
  {
    "objectID": "install-r.html#r",
    "href": "install-r.html#r",
    "title": "Install and Setup R & RStudio",
    "section": "",
    "text": "Note\n\n\n\nYou could also use a small application called {rig} to install R. {rig} is a small cross-platform application (i.e. works on Windows, Mac, and Linux) that downloads and installs R for you. While this may seem pointless to install an application to install R, it is actually quite useful as it makes it far easier to download and install multiple versions or R. As R is updated, bugs are fixed and new exciting packages do not support older versions of R, you will eventually need to update your installation. This is normally a massive pain due to the way R and the associated packages are installed on your computer. {rig} makes this process much easier (although you still shouldn’t upgrade R versions mid-project unless you are OK losing a couple of hours getting set up again).",
    "crumbs": [
      "Pre-Requisites",
      "Install and Setup R & RStudio"
    ]
  },
  {
    "objectID": "install-r.html#rstudio",
    "href": "install-r.html#rstudio",
    "title": "Install and Setup R & RStudio",
    "section": "RStudio",
    "text": "RStudio\n\nInstallation\nR is the programming language, but we need a way of interacting with R. We can do it directly by typing R into the terminal/command prompt, but this will give us a very pared down experience that is missing many of the essential features that make our development experience much more productive (as well as more enjoyable). For that, we want to install a Graphical User Interface (GUI), or more specifically, an Interactive Development Environment. The most suitable one for most R users is RStudio. RStudio is an easy-to-use IDE that allows us to write scripts (so we can save our analysis and rerun it easily, without needing to re-type it all), use the R console to check things quickly, provides a plotting window to easily manipulate and visualize the data, as well as an environment viewer to quickly understand what packages we have loaded and objects we have created.\nTo download RStudio, simply visit this link, which should provide you with a button to download the appropriate version for your operating system (there is also the full list of versions below the download button, in case it doesn’t recognize your OS correctly).\n\n\nSetup\nOnce you’ve installed RStudio, you can get going straight away - that’s the beauty of it. However, spending a few minutes getting accustomed and adjusting the layout will make your development a little smoother.\n\nTheme\nThe first thing that’s worth doing is adding a theme - the default white background can be a little harsh when you spend a long time staring at code. Open up the global preferences (ctrl/cmd + ,), go to “Appearance &gt; Editor theme”, and select a theme that works for you. The “Cobalt” theme is usually a nice default that work for many. Here, Callum is using the “Catppuccin” theme, that can be downloaded from here, and installed by clicking on “Add” at the “Appearance” screen.\n\n\n\nRStudio editor theme\n\n\n\n\nPane layout\nThe next RStudio thing that you may want to customize is the default layout. In the “Pane Layout” section of the global preferences, you can determine what you want to show in each quadrant of RStudio. The default layout will show you all the necessary parts, but most of the time you will be using the “Source” section, as this is where you write your scripts, and the “Environment” panel, which is where you can see what objects have been created, as well as exploring their properties e.g., columns names in a dataframe. For this reason, you may like to place the “Environment” panel under the “Source” panel, so they take up the majority of your screen, and the “Files/Plots/Help” and “Console/Terminal” panels are off to the side as you will interact with them less.\n\n\n\nRStudio pane layout settings\n\n\n\n\n\nRStudio pane layout\n\n\n\n\nRData\nThe final thing to do, that’s actually quite important, is to turn off the “Restore .RData into workspace at startup”. The reason being, if you do not, objects from previous sessions will be loaded into your new working environment, putting things where they shouldn’t be, making it very difficult to catch bugs as your code may reference something that doesn’t exist by that point in your script as it is created later on, but you wouldn’t catch that mistake as it was loaded into your environment on startup.\n\n\n\nRStudio .RData settings",
    "crumbs": [
      "Pre-Requisites",
      "Install and Setup R & RStudio"
    ]
  },
  {
    "objectID": "just-enough-rstudio.html",
    "href": "just-enough-rstudio.html",
    "title": "Just Enough RStudio",
    "section": "",
    "text": "RStudio projects\nRStudio gives you the ability to turn a directory (that you are hopefully using to contain all your project, as mentioned in the next section), into an RStudio project. One of the benefits of using RStudio projects is that you can easily switch between different projects and RStudio will start a new R session, meaning that objects you created in your first project won’t stick around, causing issues by existing in a place where they shouldn’t. The other key benefit is that you can use the {here} package to create relative file paths, for easier code sharing and increased reproducibility. See this section for more details about the {here} package.\nTo create a new RStudio projet, simply open up RStudio, and click on the “Project: (None)” button in the top right corner.\nYou will then be given the option to create the project in a “New Directory”, and “Existing Directory”, or from “Version Control”. If you’ve already created a project just for this workshop, then select “Existing Directory”, otherwise, create a new one. If you know about Git, then please feel free to use the last option, but options 1 & 2 are most relevant for new users. Either way, we’d recommend reading through our project organization tips about what this directory should look like/include. If you select “New Directory”, you probably want to select “New Project” on the next option, unless you have something specific in mind (like a “Quarto Book”, which is used for this website!).\nFinally, choose the directory name and location, and you’re in business (you should click the button in the bottom-left corner to “Open in new session” to make sure you’re starting in a fresh environment). From here on out, when you open up a project, all the files you’ve created will be easily accessibly, both from the “Files” pane, as well as using relative paths.",
    "crumbs": [
      "Pre-Requisites",
      "Just Enough RStudio"
    ]
  },
  {
    "objectID": "just-enough-rstudio.html#rstudio-projects",
    "href": "just-enough-rstudio.html#rstudio-projects",
    "title": "Just Enough RStudio",
    "section": "",
    "text": "Creating a new RStudio project - 1\n\n\n\n\n\n\nCreating a new RStudio project - 2\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo reiterate our comments from the project organization tips page, it’s useful to have all your project directories in a single location on your computer, and make sure you do not have any spaces in the file or folder names.",
    "crumbs": [
      "Pre-Requisites",
      "Just Enough RStudio"
    ]
  },
  {
    "objectID": "just-enough-rstudio.html#rstudio-keyboard-shortcuts",
    "href": "just-enough-rstudio.html#rstudio-keyboard-shortcuts",
    "title": "Just Enough RStudio",
    "section": "RStudio keyboard shortcuts",
    "text": "RStudio keyboard shortcuts\nThere are many shortcuts available to RStudio users, but here are the key ones:\n\nRStudio keyboard shortcuts\n\n\n\n\n\n\nShortcut\nCommand\n\n\n\n\ncmd/ctrl + enter\nSend the section of code to the console to be run\n\n\ncmd/ctrl + opt/alt + r\nRun all code\n\n\ncmd/ctrl + opt/alt + b\nRun all code from beginning to selected line\n\n\ncmd/ctrl + shift + enter\nRun current chunk (when within a Rmd or Quarto notebook)\n\n\ncmd/ctrl + shift + p\nOpen the command palette (a place where you can search for different commands)\n\n\ncmd/ctrl + shift + a\nReformat selected code (useful to help keep things readable)\n\n\ncmd/ctrl + shift + c\nComment the selected lines\n\n\ncmd/ctrl + shift + m\nInsert the pipe (%&gt;%) operator (or |&gt; if you have set up RStudio to use the base pipe by default)\n\n\nopt/alt + -\nInsert the assignment operator (&lt;-)\n\n\n\nThe common shortcuts for saving and opening files/selecting all etc. also exist in RStudio with the standard keybindings.\nTo see the full list of keyboard shortcuts, you can go to “Tools &gt; Keyboard Shortcuts Help”.\n\n\n\nRStudio keyboard shortcuts",
    "crumbs": [
      "Pre-Requisites",
      "Just Enough RStudio"
    ]
  },
  {
    "objectID": "just-enough-r.html",
    "href": "just-enough-r.html",
    "title": "Just Enough R",
    "section": "",
    "text": "Objects & types introduction\nAn object is anything you can create in R using code, whether that is a table you import from a csv file (that will get converted to a dataframe), or a vector you create within a script. Each object you create has a type. We’ve already mentioned two (dataframes and vectors), but there are plenty more. But before we get into object types, let’s take a step back and look at types in general, thinking about individual elements and the fundamentals.",
    "crumbs": [
      "Pre-Requisites",
      "Just Enough R"
    ]
  },
  {
    "objectID": "just-enough-r.html#element-types",
    "href": "just-enough-r.html#element-types",
    "title": "Just Enough R",
    "section": "Element types",
    "text": "Element types\nGenerally in programming, we have two broad types of numbers: floating point and integer numbers, i.e., numbers with decimals, and whole numbers, respectively. In R, we have these number types, but a floating point number is called a double. The floating point number is the default type R assigns to number: look at the types assigned when we leave off a decimal place vs. specify type integer by ending a number with an L.\n\ntypeof(1)\n\n[1] \"double\"\n\ntypeof(1L)\n\n[1] \"integer\"\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nTechnically type double is a subset of type numeric, so you will often see people convert numbers to floating points using as.numeric(), rather than as.double(), but the different is semantics. You can confirm this using the command typeof(as.numeric(10)) == typeof(as.double(10))semantics. You can confirm this using the commandtypeof(as.numeric(10)) == typeof(as.double(10))`.\n\n\n\nInteger types are not commonly used in R, but there are occasions when you will want to use them e.g., when you need whole numbers of people in a simulation you may want to use integers to enforce this. Integers are also slightly more precise (unless very big or small), so when exactness in whole number is required, you may want to use integers.\n\n\n\n\n\n\nNote\n\n\n\n\n\nR has some idiosyncrasies when it comes to numbers. For the most part, doubles are produced, but occasionally an integer will be produced when you are expecting a double.\nFor example:\n\ntypeof(1)\n\n[1] \"double\"\n\ntypeof(1:10)\n\n[1] \"integer\"\n\ntypeof(seq(1, 10))\n\n[1] \"integer\"\n\ntypeof(seq(1, 10, by = 1))\n\n[1] \"double\"\n\n\n\n\n\nOutside of numbers, we have characters (strings) and boolean types.\nA boolean (also known as a logical in R) is a TRUE/FALSE statement. In R, as in many programming languages, TRUE is equal to a value of 1, and FALSE equals 0. There are times when this comes in handy e.g. you need to calculate the number of people that responded to a question, and their responses is coded as TRUE/FALSE, you can just sum the vector of responses (more on vectors shortly).\n\nTRUE == 1\n\n[1] TRUE\n\nFALSE == 0\n\n[1] TRUE\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nCan you figure out what value will be returned for the command (TRUE == 0) == FALSE?\n\n\nA character is anything in quotation marks. This would typically by letter, but is occasionally a number, or other symbol. Other languages make a distinction between characters and strings, but not R.\n\ntypeof(\"a\")\n\n[1] \"character\"\n\ntypeof(\"1\")\n\n[1] \"character\"\n\n\nIt is important to note that characters are not parsed i.e., they are not interpreted by R as anything other than a character. This means that despite \"1\" looking like the number 1, it behaves like a character in R, not a double, so we can’t do addition etc. with it.\n\n\"1\" + 1\n\nError in \"1\" + 1: non-numeric argument to binary operator",
    "crumbs": [
      "Pre-Requisites",
      "Just Enough R"
    ]
  },
  {
    "objectID": "just-enough-r.html#object-types",
    "href": "just-enough-r.html#object-types",
    "title": "Just Enough R",
    "section": "Object types",
    "text": "Object types\nVectors\nAs mentioned, anything you can create in R is an object. For example, we can create an character object with the assignment operator (&lt;-).\n\nmy_char_obj &lt;- \"a\"\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nIn other languages, = is used for assignment. In R, this is generally avoided to distinguish between creating objects (assignment), and specifying argument values (see the section on functions). However, despite what some purists may say, it really doesn’t matter which one you use, from a practical standpoint.\n\n\n\nYou will note that when we created our object, it did not return a value (unlike the previous examples, a value was not printed). To retrieve the value of the object (in this case, just print it), we just type out the object name.\n\nmy_char_obj\n\n[1] \"a\"\n\n\nIn this case, we just create an object with only one element. We can check this using the length() function.\n\nlength(my_char_obj)\n\n[1] 1\n\n\nWe could also create an atomic vector (commonly just called a vector, which we’ll use from here-on in). In fact, my_char_obj is actually an vector, i.e., it is a vector of length 1, as we’ve just seen. Generally, a vector is an object that contains multiple elements that each have the same type.\n\nmy_char_vec &lt;- c(\"a\", \"b\", \"c\")\n\nAs we’ll see in the example below, we can give each element in a vector a name, and to highlight that vectors must contain elements of the same type, watch what happens here.\n\nmy_named_char_vec &lt;- c(a = \"a\", b = \"b\", c = \"c\", d = 1)\nnames(my_named_char_vec)\n\n[1] \"a\" \"b\" \"c\" \"d\"\n\nmy_named_char_vec\n\n  a   b   c   d \n\"a\" \"b\" \"c\" \"1\" \n\n\nBecause R saw the majority of the first elements in the vector were of type character it coerced the number to a character. This is super important to be aware of, as it can cause errors, particularly when coercion goes in the other direction i.e. trying to create a numeric vector.\nFactors\nAll the vector types we’ve mentioned so far map nicely to their corresponding element types. But there is an extension of the character vector used frequently: the factor (and, correspondingly, the ordered vector).\nA factor is a vector where there are distinct groups that exist within a vector i.e., they are nominal categorical data. For example, we often include gender as a covariate in epidemiological analysis. There is no intrinsic order, but we would want to account for the groups in the analysis.\nAn ordered vector is when there is an intrinsic order to the grouping i.e., we have ordinal categorical data. If, for example, we were interested in how the frequency of cigarette smoking is related to an outcome, and we wanted to use binned groups, rather than treating it as a continuous value, we would want to create an ordered vector as the ordering of the different groupings is important.\nLet’s use the mtcars dataset (that comes installed with R), and turn the number of cylinders (cyl) into an ordered vector, as there are discrete numbers of cylinders a car engine can have, and the ordering matters. Don’t worry about what $ is doing; we’ll come to that later\n\nmy_mtcars &lt;- mtcars\nmy_mtcars$cyl\n\n [1] 6 6 4 6 8 6 8 4 4 6 6 8 8 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 8 6 8 4\n\nmy_mtcars$cyl &lt;- ordered(my_mtcars$cyl)\nmy_mtcars$cyl\n\n [1] 6 6 4 6 8 6 8 4 4 6 6 8 8 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 8 6 8 4\nLevels: 4 &lt; 6 &lt; 8\n\n\nIf we wanted to directly specify the ordering of the groups, we can do this using the levels argument i.e.\n\nmy_mtcars$cyl &lt;- ordered(my_mtcars$cyl, levels = c(8, 6, 4))\nmy_mtcars$cyl\n\n [1] 6 6 4 6 8 6 8 4 4 6 6 8 8 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 8 6 8 4\nLevels: 8 &lt; 6 &lt; 4\n\n\nTo create a factor, just replace the ordered() call with factor()\nLists\nThere is another type of vector: the list. Most people do not refer to lists as type of vectors, so we will only refer to them as lists, and atomic vectors will just be referred to as vectors.\nUnlike vectors there are no requirements about the form of lists i.e., each element of the list can be completely different. One element could store a vector of numbers, another a model object, another a dataframe, and another a list (i.e. a nested list).\n\nmy_list &lt;- list(\n    c(1, 2, 3, 4, 5),\n    glm(mpg ~ ordered(cyl) + disp + hp, data = mtcars),\n    data.frame(column_1 = 1:5, column_2 = 6:10)\n)\nmy_named_list &lt;- list(\n    my_vec = c(1, 2, 3, 4, 5),\n    my_model = glm(mpg ~ ordered(cyl) + disp + hp, data = my_mtcars),\n    my_dataframe = data.frame(column_1 = 1:5, column_2 = 6:10)\n)\nmy_list\n\n[[1]]\n[1] 1 2 3 4 5\n\n[[2]]\n\nCall:  glm(formula = mpg ~ ordered(cyl) + disp + hp, data = mtcars)\n\nCoefficients:\n   (Intercept)  ordered(cyl).L  ordered(cyl).Q            disp              hp  \n      28.98802        -1.71963         2.31169        -0.02604        -0.02114  \n\nDegrees of Freedom: 31 Total (i.e. Null);  27 Residual\nNull Deviance:      1126 \nResidual Deviance: 225.1    AIC: 165.2\n\n[[3]]\n  column_1 column_2\n1        1        6\n2        2        7\n3        3        8\n4        4        9\n5        5       10\n\nmy_named_list\n\n$my_vec\n[1] 1 2 3 4 5\n\n$my_model\n\nCall:  glm(formula = mpg ~ ordered(cyl) + disp + hp, data = my_mtcars)\n\nCoefficients:\n   (Intercept)  ordered(cyl).L  ordered(cyl).Q            disp              hp  \n      28.98802         1.71963         2.31169        -0.02604        -0.02114  \n\nDegrees of Freedom: 31 Total (i.e. Null);  27 Residual\nNull Deviance:      1126 \nResidual Deviance: 225.1    AIC: 165.2\n\n$my_dataframe\n  column_1 column_2\n1        1        6\n2        2        7\n3        3        8\n4        4        9\n5        5       10\n\n\nSimilar to vectors, lists can be named, or unnamed, and also that we they display in slightly different ways: when unnamed, we get the notation [[1]] ... [[3]] to denote the different list elements, and with the named list we get $my_vec ... $my_dataframe. It is often useful to name them, though, as it gives you some useful options when it comes to indexing and extracting values later.\n\n\n\n\n\n\nNote\n\n\n\n\n\nIf you’re wondering why we are creating our list elements with the = operator, that’s because we can think of this as an argument in the list() function, where the argument name is the name we want the element to have, and the argument value is the element itself.\n\n\n\nDataframes\nDataframes are the last key object type to learn about. A dataframe is technically a special type of list. Effectively, it is a 2-D table where every column has to have elements of the same type (i.e., is a vector), but the columns can be different types to each other. The other important restriction is that all columns must be the same length, i.e. we have a rectangular dataframe.\nAs we’ve seen before, we can create a dataframe using this code, where 1:5 is shorthand for a vector that contains the sequence of numbers from 1 to 5, inclusive (i.e., c(1, 2, 3, 4, 5)). We could also write this sequence as seq(1, 5, by = 1), allowing us more control over the steps in the sequence.\n\nmy_dataframe &lt;- data.frame(\n    column_int = 1:5,\n    column_dbl = seq(6, 10, 1),\n    column_3 = letters[1:5]\n)\n\nLike with every other object type, we can just type in the dataframe’s name to return it’s value, but this tim, let’ explore the structure of the dataframe using the str() function. This function can be used on any of the objects we’ve seen so far, and is particularly helpful when exploring lists. One nice feature of dataframes is that it will explicitly print the columns types.\n\nstr(my_dataframe)\n\n'data.frame':   5 obs. of  3 variables:\n $ column_int: int  1 2 3 4 5\n $ column_dbl: num  6 7 8 9 10\n $ column_3  : chr  \"a\" \"b\" \"c\" \"d\" ...\n\n\nMatrices\nMatrices are crucial to many scientific fields, including epidemiology, as they are the basis of linear algebra. This course will use matrix multiplication extensively (notably R Session 2), so it is worth knowing how to create matrices.\nMuch like vectors, all elements in a matrix should be the same type (or they will be coerced if possible, resulting in NA if not). It is unusual to have a non-numeric matrix e.g., a character matrix, but it is possible. When we create our matrix, notice that it fills column-first, much like how we think of matrices in math (i.e., i then j).\n\nmy_matrix &lt;- matrix(1:8, nrow = 2)\nmy_matrix\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    3    5    7\n[2,]    2    4    6    8",
    "crumbs": [
      "Pre-Requisites",
      "Just Enough R"
    ]
  },
  {
    "objectID": "just-enough-r.html#indexing-objects",
    "href": "just-enough-r.html#indexing-objects",
    "title": "Just Enough R",
    "section": "Indexing objects",
    "text": "Indexing objects\nIndexing operators\nWe’ve got our objects, but now we want to do stuff with them. Without getting into too much detail about Object-Oriented Programming (e.g., the S3 class system in R), there are three mains ways of indexing in R:\n\nThe single bracket []\n\nThe double bracket [[]]\n\nThe dollar sign $\n\n\nWhich method we use depends on the type of object we have. Handily, [] will work for pretty much everything, and we typically only use use [[]] for lists.\nIndexing vectors\nWith both [] and [[]], we can use the indices i.e., the numbered position of the specific values/elements we want to extract, but if we have named objects, we can pass the names to the [] in a vector.\n\n# Extract elements 1 through 3 inclusively\nmy_char_vec[1:3]\n\n[1] \"a\" \"b\" \"c\"\n\n# Extract the same elements but using their names in a vector\nmy_named_char_vec[c(\"a\", \"b\", \"c\")]\n\n  a   b   c \n\"a\" \"b\" \"c\" \n\n\nNotice that when we index the named vector we get both the name and the value returned. Many times this is OK, but if we only wanted the value, then you’d index with [[]], but it is important to note that you can only pass one value to the brackets.\n\nmy_named_char_vec[[c(\"a\", \"b\")]]\n\nError in my_named_char_vec[[c(\"a\", \"b\")]]: attempt to select more than one element in vectorIndex\n\nmy_named_char_vec[[\"a\"]]\n\n[1] \"a\"\n\n\nIf you’re wondering why go through the hassle, it’s because values can change position in the list when we update inputs, such as csv datafiles, or needing to restructure code to make something else work. If we only index with the numeric indices, we run the risk of a silent error being returned i.e., a value is provided to us, but we don’t know that it’s referring to the wrong thing. Indexing with names mean that the element’s position in the vector doesn’t matter, and if it’s accidentally been removed when we updated code, and error will be explicitly thrown as it won’t be able to find the index.\nLists and Dataframes\nWhen it comes to indexing lists and dataframes (remember, dataframes are just special lists, so the same methods are available to us), it is more common to use [[]] and $, though there are obviously occasions when [] is useful. Let’s look at my_named_list first.\n\nmy_named_list[1]\n\n$my_vec\n[1] 1 2 3 4 5\n\nmy_named_list[\"my_vec\"]\n\n$my_vec\n[1] 1 2 3 4 5\n\nmy_named_list[[1]]\n\n[1] 1 2 3 4 5\n\nmy_named_list[[\"my_vec\"]]\n\n[1] 1 2 3 4 5\n\nmy_named_list$my_vec\n\n[1] 1 2 3 4 5\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the examples above, notice how both [] methods returned the name of the element as well as the values (as it did before with the named vector). This is important as it means we need to extract the values from what is returned before we can do any further indexing i.e., to get the value 3 from the list element my_vec.\n\n\nWe can do the same with the unnamed list, except the last two methods are not available as we do not have a name to use.\n\nmy_list[1]\n\n[[1]]\n[1] 1 2 3 4 5\n\nmy_list[[1]]\n\n[1] 1 2 3 4 5\n\n\nBecause a dataframe is a type of list where the column headers are the element names, we can use [[]] and $ as with the named list.\n\nmy_dataframe[1]\n\ndata.frame [5, 1]\ncolumn_int int 1 2 3 4 5\n\nmy_dataframe[[1]]\n\n[1] 1 2 3 4 5\n\nmy_dataframe[\"column_int\"]\n\ndata.frame [5, 1]\ncolumn_int int 1 2 3 4 5\n\nmy_dataframe$column_int\n\n[1] 1 2 3 4 5\n\n\nIf we wanted to extract a particular value from a column, we can use the following methods.\n\n# indexes i then j, just like in math\nmy_dataframe[2, 1]\n\n[1] 2\n\n# Extract the second element from the first column\nmy_dataframe[[1]][2]\n\n[1] 2\n\n# Extract the second element from column_int, using the i, j procedure as before\nmy_dataframe[2, \"column_int\"]\n\n[1] 2\n\n# Extract the second element from column_int\nmy_dataframe$column_int[2]\n\n[1] 2",
    "crumbs": [
      "Pre-Requisites",
      "Just Enough R"
    ]
  },
  {
    "objectID": "just-enough-r.html#packages",
    "href": "just-enough-r.html#packages",
    "title": "Just Enough R",
    "section": "Packages",
    "text": "Packages\nUp until now, we’ve been getting to grips with the core concepts of objects, and indexing them. But when you’re writing code, you’ll want to do things that are relatively complicated to implement, such as solve a set of differential equations. Fortunately, for many areas of computing (and, indeed, epidemiology and statistics), many others have also struggled with the same issues and some have gone one to document their solutions in a way others can re-use them. This is the basis for packages. Someone has packaged up a set of functions for others to re-use.\nWe’ve mentioned the word function a number of time so far, and we haven’t defined it, but that’s coming soon. For the moment, let’s just look at how we can find, install, and load packages.\nFinding packages\nAs mentioned previously CRAN is a place where many pieces of R code is documents and stored for others to download and use. Not only are the R programming language executables stored in CRAN, but so are user-defined functions that have been turned into packages.\nTo find packages, you can go to the CRAN website and search by name, but there are far too many for that to be worthwhile - just Google what you want to do and add “r” to the end of your search query, and you’ll likely find what you’re looking for. Once you’ve found a package you want to download, next you need to install it.\nInstalling packages\nBarring any super-niche packages, you should be able to use the following command(s):\n\ninstall.packages(\"package to download\")\n# Download multiple by passing a vector of package names\ninstall.packages(c(\"package 1\", \"package 2\"))\n\nIf for some reason you get an error message saying the package isn’t available on CRAN, first, check for typos, and if you still get an error, you may need to download it directly from GitHub. Read here for more information about using the pak package to download packages from other sources.\nLoading packages\nNow you have your packages installed, you just need to load them to get any of their functionality. The easiest way is to place this code at the top of your script.\n\n# Quotations are not required, but can be used\nlibrary(package to download)\n\n\nMost of the time, this is fine, but occasionally you will run in to an issue where a function doesn’t work as expected. Sometimes this is because of what’s called a namespace conflict i.e., you have two functions with the same name loaded, and potentially you’re using the wrong verion.\nFor example, in base R (i.e, these functions come pre-installed when you set up R), there is a filter() function from the {stats} package (as mentioned, we’ll denote this as stats::filter()). Throughout this workshop, you will see library(tidyverse) at the top of the pages to indicate the tidyverse set of packages are being loaded (this is actually a package that installs a bunch of related and useful packages for us). In dplyr (one of the packages loaded by tidyverse) there is also a function called filter(). Because dplyr was loaded after {stats} was loaded (because {stats} is automatically loaded when R is started), the dplyr::filter() function will take precedence. If we wanted to specifically use the {stats} version, we could write this:\n\n# Set the seed for the document so we get the same random numbers sampled\n# each time we run the script (assuming it's run in its entirety from start\n# to finish)\nset.seed(1234)\n\n# Create a cosine wave with random noise\nraw_timeseries &lt;- cos(pi * seq(-2, 2, length.out = 1000)) + rnorm(1000, sd = 0.5)\n\n# Calculate 20 day moving average using stats::filter()\nsmooth_timeseries &lt;- stats::filter(raw_timeseries, filter = rep(1/20, 20), sides = 1)\n\n# Plot raw data\nplot(raw_timeseries, col = \"grey80\")\n\n# Overlay smoothed data\nlines(smooth_timeseries, col = \"red\", lwd = 2)",
    "crumbs": [
      "Pre-Requisites",
      "Just Enough R"
    ]
  },
  {
    "objectID": "just-enough-r.html#functions",
    "href": "just-enough-r.html#functions",
    "title": "Just Enough R",
    "section": "Functions",
    "text": "Functions\nAs we’ve alluded to, functions are core to gaining functionality in R. We can always hand-write the code to complete a task, but if we have to repeat a task more than once, it can be tiresome to repeat the same code, particularly if it is a particularly complex task that requires many lines of code. This is where functions come in: they provide us with a mechanism to wrap up code into something that can be re-used. Not only does this reduce the amount of code we need to write, but by minimize code duplication, debugging becomes a lot easier as we only need to remember to make changes and correct one section of our codebase. Say, for example, you want to take a vector of numbers and calculate the cumulative sum e.g.;\n\nmy_dbl_vec &lt;- 1:10\n\ncumulative_sum &lt;- 0\n\nfor(i in seq_along(my_dbl_vec)) {\n    cumulative_sum &lt;- cumulative_sum + i\n}\n\ncumulative_sum\n\n[1] 55\n\n\nThis is OK if we only do this calculation once, but it’s easy to imagine us wanting to repeat this calculation; for example, we might use calculate the cumulative sum of daily cases to get a weekly incidence over every week of a year. In this situation, we would want to create a function.\n\nmy_cumsum &lt;- function(vector) {\n    cumulative_sum &lt;- 0\n\n    for(i in seq_along(my_dbl_vec)) {\n        cumulative_sum &lt;- cumulative_sum + i\n    }\n\n    cumulative_sum\n}\n\nmy_cumsum(my_dbl_vec)\n\n[1] 55\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nThis is obviously a contrived example because, as with many basic operations in R, there is already a function written to perform this calculation that does it in a much more performant and safer manner: cumsum()\n\n\n\nFor many of the manipulations we will want to perform, a function has already been written by someone else and put into a package that we can download, as we’ve already seen.\nAnonymous functions\nThere is a special class of functions called anonymous functions that are worth being aware of, as we will use them quite extensively throughout this workshop. As the name might suggest, anonymous functions are functions that are not named, and therefore, not saved for re-use. You may, understandably, be wondering why we would want to use them, given we just make the case for functions replacing repeatable blocks of code. In some instances, we want to be able to perform multiple computations that require creating intermediate objects, but because we only need to use them once, we don’t save them save to our environment, potentially causing issues with conflicts (e.g., accidentally using an object we didn’t mean to, or overwriting existing ones by re-using the same object name). This gets into the broader concept of local vs global scopes, but that is too far beyond the scope of this workshop: see Hands-On Programming with R and Advanced R for more information. Let’s look at an example to see when we might want to use an anonymous function.\nThroughout this workshop, we will make use of the map_*() series of functions from the purrr package. We’ll go into more detail about purr::map() shortly, but for now, imagine we have a vector of numbers, and we want to add 5 to each value before and multiplying by 10. The map_dbl() function takes a vector and a function, and outputs a double vector. We could write a function to perform this multiplication, but if we’re only going to do this operation once, it seems unnecessary.\n\npurrr::map_dbl(\n    .x = my_dbl_vec,\n    .f = function(.x) {\n        add_five_val &lt;- .x + 5\n\n        add_five_val * 10\n    }\n)\n\n [1]  60  70  80  90 100 110 120 130 140 150\n\n# only exists within the function\nadd_five_val\n\nError in eval(expr, envir, enclos): object 'add_five_val' not found\n\n\nHere, we’ve specified the anonymous function to take the input .x and multiple each value by 10, and we did it without saving the function. This would be equivalent to writing this:\n\nadd_five_multiply_ten &lt;- function(x) {\n    add_five_val &lt;- x + 5\n    add_five_val * 10\n}\n\npurrr::map_dbl(\n    .x = my_dbl_vec,\n    .f = ~add_five_multiply_ten(.x)\n)\n\n [1]  60  70  80  90 100 110 120 130 140 150\n\n# only exists within the function\nadd_five_val\n\nError in eval(expr, envir, enclos): object 'add_five_val' not found\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNotice the ~ used: this specifies that we want to pass arguments into our named function. Without it, we will get an error about .x not being found.\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nIn this example, because we are doing standard arithmetic, R will vectorize our function so that it can automatically be applied to each element of the object, so this example was merely to illustrate the point.\n\nadd_five_multiply_ten(my_dbl_vec)\n\n [1]  60  70  80  90 100 110 120 130 140 150",
    "crumbs": [
      "Pre-Requisites",
      "Just Enough R"
    ]
  },
  {
    "objectID": "just-enough-r.html#tidy-data",
    "href": "just-enough-r.html#tidy-data",
    "title": "Just Enough R",
    "section": "Tidy data",
    "text": "Tidy data\nBefore we look at the common packages and functions we use throughout this workshop, let’s take a second to talk about how our data is structured. For much of what we do, it is convenient to work with dataframes, and many functions we will use are designed to work with long dataframes. What this means is that each column represents a variable, and each row is a unique observation.\nLet’s first look at a wide dataframe to see how data may be represented. Here, we have one column representing a number for each of the states in the US, and then we have two columns representing some random incidence: one for July and one for August.\n\nwide_df &lt;- data.frame(\n    state_id = 1:52,\n    july_inc = rbinom(52, 1000, 0.4),\n    aug_inc = rbinom(52, 1000, 0.6)\n)\n\nwide_df\n\ndata.frame [52, 3]\nstate_id int 1 2 3 4 5 6\njuly_inc int 399 409 381 381 387 372\naug_inc  int 613 578 604 607 603 614\n\n\nInstead, we reshape this into a long dataframe so that there is a column for the state ID, a column for the month, and a column for the incidence (that is associated with both the state and the month). Using the tidyr package, we could reshape this wide dataframe to be a long dataframe (see this section for more information about the pivot_*() functions)\n\nlong_df &lt;- tidyr::pivot_longer(\n    wide_df,\n    cols = c(july_inc, aug_inc),\n    names_to = \"month\",\n    values_to = \"incidence\",\n    # Extract only the month using regex\n    names_pattern = \"(.*)_inc\"\n)\n\n{paint} masked print.tbl_df\n\nlong_df\n\ntibble [104, 3]\nstate_id  int 1 1 2 2 3 3\nmonth     chr july aug july aug july aug\nincidence int 399 613 409 578 381 604\n\n\nYou will notice that our new dataframe contains three columns still, but is longer than previously; two time as long, in fact.\n\n\n\n\n\n\nNote\n\n\n\n\n\nParticularly keen-eyed reader may also notice that long_df is also has class tibble, not a data.frame. A tibble effectively is a data.frame, but is an object commonly used and output by tidyverse functions, as it has a few extra safety features over the base data.frame.",
    "crumbs": [
      "Pre-Requisites",
      "Just Enough R"
    ]
  },
  {
    "objectID": "just-enough-r.html#core-code-used",
    "href": "just-enough-r.html#core-code-used",
    "title": "Just Enough R",
    "section": "Core code used",
    "text": "Core code used\nWe’re finally ready to talk about the functions that are used throughout this workshop. The first package to mention is the tidyverse package, which actually a collection of packages: the core packages can be found here. The reason why are using the tidyverse packages throughout this workshop is that they are relatively easily to learn, compared to base R and data.table (not that they are mutually exclusive), and what most people are familiar with. They also are well designed and powerful, so you should be able to do most things you need using their packages.\nYou can find a list of cheatsheets for all of these packages (and more) here.\nLet’s load the tidyverse packages and then go through the key functions used. Unless stated explicitly, these packages will be available to you after loading the tidyverse with the following command.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\ntibble()\nThe tibble is a modern reincarnation of the dataframes that is slightly safer i.e., is more restricted in what you can do with it, and will throw errrors more frequently, but very rarely for anything other than a bug. We will use the terms interchangeably, as most people will just talk about dataframes, as for the most part, they can be treated identically. Use the same syntax as the data.frame() function to create the tibble.\ndplyr::filter()\nIf we wanted to take a subset of rows of a dataframe, we would use the dplyr::filter() function. Here, we’re listing the package it’s coming from, as there are some other packages that also export their own version of the filter() function. However, for all the code in this workshop, there aren’t any concerns about namespace conflicts, so we won’t use it from here on in.\nThe filter() function is relatively simple to work with: you specify the dataframe variable you want to subset by, the filtering criteria, and that’s it. If we include multiple arguments, they get treated as AND statements (&), so all conditions need to be met.\n\nfilter(\n    long_df,\n    month == \"july\",\n    incidence &gt; 410\n    # equivalent to: month == \"july\" & incidence &gt; 410\n)\n\ntibble [8, 3]\nstate_id  int 10 14 34 38 42 45\nmonth     chr july july july july july july\nincidence int 422 421 422 426 441 417\n\n\nWe can filter using OR statements (|), so if either condition returns TRUE, then it will be included in the subset.\n\nfilter(\n    long_df,\n    month == \"july\" | incidence &gt; 600\n)\n\ntibble [78, 3]\nstate_id  int 1 1 2 3 3 4\nmonth     chr july aug july july aug july\nincidence int 399 613 409 381 604 381\n\n\nselect()\nIf, instead, we wanted to subset of columns of a dataframe, we would use the dplyr::select() function.\nLet’s say, from our wide incidence data, we only want the state’s ID and their August incidence. We can directly select the columns this way.\n\nselect(\n    wide_df,\n    state_id, aug_inc\n)\n\ndata.frame [52, 2]\nstate_id int 1 2 3 4 5 6\naug_inc  int 613 578 604 607 603 614\n\n\nBut in this case, it would be more efficient (for us) to tell R the columns we don’t want. We can do that using the - sign.\n\nselect(\n    wide_df,\n    -july_inc\n)\n\ndata.frame [52, 2]\nstate_id int 1 2 3 4 5 6\naug_inc  int 613 578 604 607 603 614\n\n\nIf there were multiple columns we didn’t want, we would pass them in a vector.\n\nselect(\n    wide_df,\n    -c(july_inc, aug_inc)\n)\n\ndata.frame [52, 1]\nstate_id int 1 2 3 4 5 6\n\n\nWhen it comes to selecting columns, the tidyselect package has a few very handy functions for us. To understand when they are most useful, let’s first look at the mutate() function, and then we’ll highlight how to use the different column selection functions available to use through tidyselect.\nmutate()\nIf we have a dataframe and want to add or edit a column, we use the mutate() function. Usually the mutate() function is used to add a column that is related to the existing data, but it is not necessary. Below are examples of both.\n\n# add September incidence that is based on August incidence\nmutate(\n    wide_df,\n    sep_inc = round(aug_inc * 1.2 + rnorm(52, 0, 10), digits = 0)\n)\n\ndata.frame [52, 4]\nstate_id int 1 2 3 4 5 6\njuly_inc int 399 409 381 381 387 372\naug_inc  int 613 578 604 607 603 614\nsep_inc  dbl 735 692 725 733 733 740\n\n# add random September incidence\nmutate(\n    wide_df,\n    sep_inc = rbinom(52, 1000, 0.7)\n)\n\ndata.frame [52, 4]\nstate_id int 1 2 3 4 5 6\njuly_inc int 399 409 381 381 387 372\naug_inc  int 613 578 604 607 603 614\nsep_inc  int 702 722 711 709 684 682\n\n\nIf we wanted to update a column, we can do that by specifying the column on both sides of the equals sign.\n\n# Update the August incidence to add random noise\nmutate(\n    wide_df,\n    aug_inc = aug_inc + round(rnorm(52, 0, 10), digits = 0)\n)\n\ndata.frame [52, 3]\nstate_id int 1 2 3 4 5 6\njuly_inc int 399 409 381 381 387 372\naug_inc  dbl 609 587 614 616 577 605\n\n\nOne crucial thing to note is that mutate() applies our function/operation to each row simultaneously, so the new column’s value only depends on the row’s original values (or the vector in the case of the second example that didn’t use the values from the data).\npaste0()\nThe paste0() function is useful for manipulating objects and coercing them into string, allowing us to do string interpolation. It comes installed with base R, so there’s nothing to install, and because of the way mutate() works, apply functions to each row simultaneously, we can modify whole columns at once, depending on the row’s original values. It works to squish all the values together, without any separators by default. If you wanted spaces between your words, for example, you can use the paste(..., sep = \" \") function, which takes the sep argument.\n\nchar_df &lt;- mutate(\n    long_df,\n    # Notice that text is in commas, and object values being passed to paste0()\n    # are unquoted.\n    state_id = paste0(\"state_\", state_id)\n)\n\nchar_df\n\ntibble [104, 3]\nstate_id  chr state_1 state_1 state_2 state_2 state_3 state_3\nmonth     chr july aug july aug july aug\nincidence int 399 613 409 578 381 604\n\n\nglue::glue()\nglue() is a function that comes installed with tidyverse, but is not loaded automatically, so you have to reference it explicitly by either using library(glue) or the :: notation shown below. It serves the same purpose as the base paste0(), but in a slightly different syntax. Instead of using a mix of quotations and unquoted object names, glue() requires everything to be in quotation marks, with any value being passed to the string interpolation being enclosed in { }. It is worth learning glue() as it is used throughout the tidyverse packages, such as in the pivot_wider() function.\n\nchar_df &lt;- mutate(\n    long_df,\n    state_id = glue::glue(\"state_{state_id}\")\n)\n\nchar_df\n\ntibble [104, 3]\nstate_id  chr state_1 state_1 state_2 state_2 state_3 state_3\nmonth     chr july aug july aug july aug\nincidence int 399 613 409 578 381 604\n\n\nstr_replace_all()\nIf we want to replace characters throughout the whole of a string vector, we can do that with the str_replace_all() function. And because dataframes are made up of individual vectors, we can use this to modify vectors.\n\nmutate(\n    char_df,\n    # pass in the vector (a column, here), the pattern to remove, and the replacement\n    clean_state_id = str_replace_all(state_id, \"state_\", \"\")\n)\n\ntibble [104, 4]\nstate_id       chr state_1 state_1 state_2 state_2 state_3 state_3\nmonth          chr july aug july aug july aug\nincidence      int 399 613 409 578 381 604\nclean_state_id chr 1 1 2 2 3 3\n\n\nacross()\nAbove, we were only mutating a single column at a time, which is what we often do. But, sometimes we want to apply the exact same transformation to multiple columns. For example, say we wanted to turn our monthly incidence data into the average weekly incidence. We could write out each transformation by hand, but when there are more than two columns, this gets rather tedious and introduces the opportunity for mistakes when copying code (one of our motivations for using functions). The tidyselect::across() function allows us to specify the columns we want to apply the transformation, and the function (can be named or anonymous), and that’s it.\nThere are a couple of points to understand about the code below:\n\nNote the . preceding the cols, fns, and x\n\nEach column is passed to the .x value in the function argument\n\n~ is required to pass arguments into the function. In this case it is an anonymous function using the map_*() syntax.\n\n\nmutate(\n    wide_df,\n    across(\n        .cols = c(july_inc, aug_inc),\n        .fns = ~.x * 7 / 30\n    )\n)\n\ndata.frame [52, 3]\nstate_id int 1 2 3 4 5 6\njuly_inc dbl 93.1 95.433333 88.9 88.9 90.3 86.8\naug_inc  dbl 143.033333 134.866667 140.933333 141.633333 140.7 143.266667\n\n\neverything()\nIf we wanted to select every column in a dataframe, we would use the everything() function. This may not seem helpful initially, but there are occasions when it’s very useful. For instance, in the previous example we still specified the exact columns we wanted to transform. However, if there were five times as many, we wouldn’t want to do that. Do note that if we replaced this with everything(), we would also mutate() our state_id column, which we probably don’t want to do, so we could combine it with the - selection seen previously.\ncontains()\nAnother very handy function is the tidyselect::contains() function. This allows us to specify a string that the column names must contain for them to be selected. We could change the above example to look like this:\n\nmutate(\n    wide_df,\n    across(\n        .cols = contains(\"_inc\"),\n        .fns = ~.x * 7 / 30\n    )\n)\n\ndata.frame [52, 3]\nstate_id int 1 2 3 4 5 6\njuly_inc dbl 93.1 95.433333 88.9 88.9 90.3 86.8\naug_inc  dbl 143.033333 134.866667 140.933333 141.633333 140.7 143.266667\n\n\nrename_with()\nIf we wanted to rename columns of a dataframe, we can use the rename() function. However, like the previous tidyselect examples, sometimes we want to apply the same renaming scheme (function) to the columns. rename_with() allows us to pass a function to multiple columns at once, achieving what we want with minimal effort, and without needing to use across().\n\nrename_with(\n    wide_df,\n    .cols = contains(\"_inc\"),\n    .fn = ~str_replace_all(.x, \"_inc\", \"_incidence\")\n)\n\ndata.frame [52, 3]\nstate_id       int 1 2 3 4 5 6\njuly_incidence int 399 409 381 381 387 372\naug_incidence  int 613 578 604 607 603 614\n\n\n\n\n\n\n\n\nImportant\n\n\n\nHopefully you are noticing a pattern between the tidyselect-type functions. When you need to apply a function to multiple columns in a dataframe, you will select the columns with the .cols argument, and pass the function to the .fn(s) argument with the ~ symbol indicating you are using the .x to represent the column in the function (yes, there is a touch of ambiguity between .fns and .fn, but the general pattern holds). This will be useful when we look at the map_*() family of functions.\n\n\nmagrittr::%&gt;%\nThe %&gt;% operator is an interesting and very useful function that comes installed (and loaded) with the tidyverse package (technically from the magrittr package from within the tidyverse). It allows us to chain together operations without needing to create intermediate objects. Say for example we have our wide incidence data and want to add data for September before turning it into a long dataframe, we could create and intermediate object before using the pivot_longer() function from before, but we might not want to create another object that we don’t really care about. This is when we would want to use a pipe, as it takes the output of one operation and pipes it into the next one.\n\nmutate(\n    wide_df,\n    sep_inc = round(aug_inc * 1.2 + rnorm(52, 0, 10), digits = 0)\n    ) %&gt;%\n    pivot_longer(\n        cols = c(july_inc, aug_inc, sep_inc),\n        names_to = \"month\",\n        values_to = \"incidence\",\n        names_pattern = \"(.*)_inc\",\n        data = .\n    )\n\ntibble [156, 3]\nstate_id  int 1 1 1 2 2 2\nmonth     chr july aug sep july aug sep\nincidence dbl 399 613 725 409 578 685\n\n\nBy default, the previous object gets input into the first argument of the next function, but here we’ve shown that you can manipulate the position the object is piped into by specify the argument using the . syntax.\n|&gt;\nIn R version 4.1.0, the |&gt; was added as the base pipe operator. It works slightly differently to %&gt;%, and frankly, is less powerful and less common (at the moment), so we won’t use it in this workshop.\ngroup_by()\nIf we have groups in our dataframe and want to apply some function to each group’s data, we can use the group_by() function. For example, if we wanted to calculate the mean and median incidence in our fake data from earlier, but group it by the month.\n\ngroup_by(long_df, month) %&gt;%\n    summarize(mean = mean(incidence), median = median(incidence))\n\ntibble [2, 3]\nmonth  chr aug july\nmean   dbl 599.057692 396.730769\nmedian dbl 600.5 398\n\n\npivot_*()\nWe’ve already seen the purpose of the pivot_longer() function: taking wide data and reshaping it to be long. There is an equivalent to go from long to wide: pivot_wider(). Occassionally this is useful (though it is less common than creating long data).\n\npivot_wider(\n    long_df,\n    names_from = month,\n    values_from = incidence,\n    names_glue = \"{month}_inc\"\n)\n\ntibble [52, 3]\nstate_id int 1 2 3 4 5 6\njuly_inc int 399 409 381 381 387 372\naug_inc  int 613 578 604 607 603 614\n\n\nHere, the names_glue argument is making use of the glue::glue() function (see above) that is installed with tidyverse, but not loaded automatically for use by the users.\nmap_*()\nThe map_*() functions come from the purrr package (a core part of the tidyverse), and are incredibly useful. They are relatively complicated, so there isn’t enough space to go into full detail, but here we’ll just outline enough so you can read more and understand what’s going on.\nWe’ve already seen we can apply functions to each element of a vector (atomic or list vectors). The key points to note are the . preceding the x and f arguments. If we use map() we get a list returned, map_dbl() a double vector, map_char() a character vector, map_dfr() a dataframe etc.\nIn the example below, we’ll walk through map_dfr() as it’s one of the more confusing variants due to the return requirements.\n\nmap_dfr_example &lt;- map_dfr(\n    .x = my_dbl_vec,\n    .f = function(.x) {\n        # Note we don't use , at the end of each line - it's as if we were\n        # running the code in the console\n        times_ten &lt;- .x * 10\n        divide_ten &lt;- .x / 10\n\n        # construct a tibble as normal (requires , between arguments)\n        tibble(\n            original_val = .x,\n            times_ten = times_ten,\n            divide_ten = divide_ten\n        )\n    }\n)\n\nmap_dfr_example\n\ntibble [10, 3]\noriginal_val int 1 2 3 4 5 6\ntimes_ten    dbl 10 20 30 40 50 60\ndivide_ten   dbl 0.1 0.2 0.3 0.4 0.5 0.6\n\n\nWhat’s happening under the hood is that map_dfr() is applying the anonymous function we defined to each element in our vector and returning a list of dataframes that contains one row and three columns, i.e. for the first element, we would get this:\n\nlist(map_dfr_example[1, ])\n\n[[1]]\ntibble [1, 3]\noriginal_val int 1\ntimes_ten    dbl 10\ndivide_ten   dbl 0.1\n\n\nIt then calls the bind_rows() function to squash all of those dataframes together, one row stacked on top of the next, to create one large dataframe. We could write the equivalent code like this:\n\nbind_rows(\n    map(\n    .x = my_dbl_vec,\n    .f = function(.x) {\n        # Note we don't use , at the end of each line - it's as if we were\n        # running the code in the console\n        times_ten &lt;- .x * 10\n        divide_ten &lt;- .x / 10\n\n        # construct a tibble as normal (requires , between arguments)\n        tibble(\n            original_val = .x,\n            times_ten = times_ten,\n            divide_ten = divide_ten\n        )\n    }\n)\n)\n\ntibble [10, 3]\noriginal_val int 1 2 3 4 5 6\ntimes_ten    dbl 10 20 30 40 50 60\ndivide_ten   dbl 0.1 0.2 0.3 0.4 0.5 0.6\n\n\nmap_dfc() does exactly the same thing, but calls bind_cols() instead, to place the columns next to each other.\nThere is one more important variant to go through: pmap_*(). If map_*() takes one vector as an argument, pmap_*() takes a list of arguments. What this means is that we can iterate through the elements of as many arguments as we’d like, in sequence. For example, let’s multiply the elements of two double vectors together.\n\n# Create a second vector of numbers\nmy_second_dbl_vec &lt;- rnorm(length(my_dbl_vec), 20, 20)\nmy_second_dbl_vec\n\n [1] 45.583594  7.463083 20.505265 46.030180 15.004206 22.699967 17.066535\n [8] 44.678612 22.708520 21.344806\n\n# Remind ourselves what our original vector looks like\nmy_dbl_vec\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\npmap_dbl(\n    .l = list(first_num = my_dbl_vec, sec_num = my_second_dbl_vec),\n    .f = function(first_num, sec_num) {\n        first_num * sec_num\n    }\n)\n\n [1]  45.58359  14.92617  61.51580 184.12072  75.02103 136.19980 119.46575\n [8] 357.42890 204.37668 213.44806\n\n\nThere are a couple of important points to note here:\n\nAll vectors need to be the same length\nThe function is applied to each element index of the input vectors, i.e., the first elements of the vectors are multiplied together, the second element of the vectors are multiplied together, and so on, until the last elements are reached.\nWe use .l instead of .x to denote we are passing a list() of vectors.\nOur function specifies the names of the vectors in the list(), which are then used within the function itself (similar to how we used .x in our map_*() functions)\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nAs before, this is an unnecessary approach as R would vectorize the operation, but it is useful to demonstrate the principle.\n\nmy_dbl_vec * my_second_dbl_vec\n\n [1]  45.58359  14.92617  61.51580 184.12072  75.02103 136.19980 119.46575\n [8] 357.42890 204.37668 213.44806\n\n\n\n\n\nnest()\nNesting is a relatively complex, but powerful, concept, particularly when combined with the map_*() functions. Commonly, as in this workshop, it is used to apply a model function to multiple different datasets, and store them all in one dataframe for easy of manipulation. What it effectively does is group your existing dataframe by a variable, and then shrink all the columns (except the grouping column), into a single list column, leaving you with as many rows as there are distinct groups. Each element of the new list column is itself a small dataframe that contains all the original variables and data, but only those that are relevant for the group. Hopefully this example will make it clearer. Here, we’ll take the mtcars dataset, and like before, we’ll group by the cyl variable, but this time we’ll nest the rest of the data.\n\nnested_mtcars &lt;- nest(mtcars, data = -cyl)\nnested_mtcars\n\ntibble [3, 2]\ncyl  dbl 6 4 8\ndata lst tibble [7, 10] tibble [11, 10] tibble [14, 10]\n\n\nWe can see we’ve nested all columns, except cyl. Looking at the data column for just the first row (cyl == 6), we see we have a list with one item: the rest of the data that’s relevant to the rows where cyl == 6 (notice the [[1]] above the tibble).\n\nnested_mtcars[1, ]$data\n\n[[1]]\ntibble [7, 10]\nmpg  dbl 21 21 21.4 18.1 19.2 17.8\ndisp dbl 160 160 258 225 167.6 167.6\nhp   dbl 110 110 110 105 123 123\ndrat dbl 3.9 3.9 3.08 2.76 3.92 3.92\nwt   dbl 2.62 2.875 3.215 3.46 3.44 3.44\nqsec dbl 16.46 17.02 19.44 20.22 18.3 18.9\nvs   dbl 0 0 1 1 1 1\nam   dbl 1 1 0 0 0 0\ngear dbl 4 4 3 3 4 4\ncarb dbl 4 4 1 1 4 4\n\n\nNow we can use map to fit a model to this subsetted data.\n\nmutate(\n    nested_mtcars,\n    model_fit = map(data, ~glm(mpg ~ hp + wt + ordered(carb), data = .x))\n)\n\ntibble [3, 3]\ncyl       dbl 6 4 8\ndata      lst tibble [7, 10] tibble [11, 10] tibble [14, 10]\nmodel_fit lst glm [30, 1] glm [30, 1] glm [30, 1]\n\n\nThis creates a list column (because we used the map() function, which returns a list) that contains the relevant model fits.\n\n\n\n\n\n\nImportant\n\n\n\nIt is important to note that there is also a function called nest_by(). However, it returns a rowwise tibble, i.e., any later manipulations will be applied on a row-by-row basis, unlike a standard tibble that applies the manipulation to every row all at once, so we would need to use normal mutate() syntax (and explicitly return a list column) to get the same effect as before.\n\nnest_by(mtcars, .by = cyl) %&gt;%\n    mutate(model_fit = list(glm(mpg ~ hp + wt + ordered(carb), data = data)))\n\ntibble [3, 3]\nrowwise grouped by: .by\n.by       dbl               4 6 8\ndata      list&lt;tibble[,11]&gt; 22.800, 24.400, 22.800, 32.400, 30.400, 33.900, 21.500, 27.300, 26.000,~\nmodel_fit lst               glm [30, 1] glm [30, 1] glm [30, 1]\n\n\n\n\nggplot()\nTo create out plots, we can use the base plot() functions, but ggplot2 package provides a clean and consistent interface to plotting that has many benefits. In essence, plots are built up in layers, with each stacking on top of the previous.\nTo initialize a plot, we simply use the ggplot() function call, that creates the background of a figure. Now we need to add data, and geoms to interpret that data.\nLet’s use the mtcars dataset again.\n\nmtcars\n\ndata.frame [32, 11]\nmpg  dbl 21 21 22.8 21.4 18.7 18.1\ncyl  dbl 6 6 4 6 8 6\ndisp dbl 160 160 108 258 360 225\nhp   dbl 110 110 93 110 175 105\ndrat dbl 3.9 3.9 3.85 3.08 3.15 2.76\nwt   dbl 2.62 2.875 2.32 3.215 3.44 3.46\nqsec dbl 16.46 17.02 18.61 19.44 17.02 20.22\nvs   dbl 0 0 1 1 0 1\nam   dbl 1 1 1 0 0 0\ngear dbl 4 4 4 3 3 3\ncarb dbl 4 4 1 1 2 1\n\n\nLooking at the data, we might be interested in how the mpg of a car is affected by it horsepower (hp). To add data, we just use the ggplot() function argument data = mtcars. We also need to tell ggplot() how to map the data points to the figure, i.e., the values for the x and y axes.\nBecause this depends on the underlying data, this must go within an argument called aes() i.e., aes(x = hp, y = mpg).\nTo add a layer to show the data, we add a geom. In this case, because we have continuous independent and dependent variables, we could use the geom_point() geom, that will give us a scatter plot. Much like basic arithmetic, we add layers using the + operator.\n\nggplot(data = mtcars, aes(x = hp, y = mpg)) +\n    geom_point()\n\n\n\n\n\n\n\nNow let’s imagine we wanted to explore this relationship, but separated by engine type (the vs column). We can use color to separate these points. Because this is an argument that depends on the underlying data, again, this must be placed within aes().\n\nggplot(data = mtcars, aes(x = hp, y = mpg, color = vs)) +\n    geom_point()\n\n\n\n\n\n\n\nWhat you’ll notice here is that despite vs being a binary choice, because it is of type double, ggplot() interprets this as a number, so provides a continuous color scale. To correct this, let’s convert vs into a factor before plotting.\n\nmtcars %&gt;%\n    mutate(vs = factor(vs)) %&gt;%\n    ggplot(aes(x = hp, y = mpg, color = vs)) +\n    geom_point()\n\n\n\n\n\n\n\nWe can change the theme by layering in more information, as we did with the other plotting layers. Here, let’s change the background to white, and add some different colors. We’ll also change the size of the points.\n\nmtcars %&gt;%\n    mutate(vs = factor(vs)) %&gt;%\n    ggplot(aes(x = hp, y = mpg, color = vs)) +\n    geom_point(size = 5) +\n    theme_minimal() +\n    # We don't need to specify the relationship between the levels and the colors\n    # and labels, but it means we're less likely to make a mistake in interpretation\n    # and labelling\n    scale_color_manual(\n        values = c(\"0\" = \"#6b3df5ff\", \"1\" = \"#f5c13cff\"),\n        labels = c(\"0\" = \"V-Shaped\", \"1\" = \"Straight\")\n    )\n\n\n\n\n\n\n\nImagine we wanted to use one more grouping: automatic vs manual transmission (am). Rather than adding yet another color, we could do something called a facet_wrap(), which creates separate panels for each group. Adding this to a ggplot() is very easy - it’s just another + operation! As before, we will add labels for easier interpretation.\n\nmtcars %&gt;%\n    mutate(vs = factor(vs)) %&gt;%\n    ggplot(aes(x = hp, y = mpg, color = vs)) +\n    geom_point(size = 5) +\n    theme_minimal() +\n    # We don't need to specify the relationship between the levels and the colors\n    # and labels, but it means we're less likely to make a mistake in interpretation\n    # and labelling\n    scale_color_manual(\n        values = c(\"0\" = \"#6b3df5ff\", \"1\" = \"#f5c13cff\"),\n        labels = c(\"0\" = \"V-Shaped\", \"1\" = \"Straight\")\n    ) +\n    facet_wrap(~am, labeller = as_labeller(c(\"0\" = \"Automatic\", \"1\" = \"Manual\")))\n\n\n\n\n\n\n\nThis is looking much better, but we might want to add a line to show the trends within the groups. Again, this is as simple as adding another layer. One thing to note about the plot below, because we specified the data and aes() arguments in the original ggplot() function call, those data relationships will also be applied to our new geom. We could just as easily write them within the geom_*() explicitly, but then we would have to do that for each geom_*() in our plot, which is unnecessary when they all have the same data relationships. To demonstrate this, let’s also make a small modification so that only the points are colored, and the lines are all red. To do that, we will remove color = vs from the global aes(), and add it to one specific to geom_point(). But because we still want to fit a linear model to the different engine types (vs) separately, we will add group = vs to the geom_smooth(aes(), ...) call, to let ggplot() know to treat them as separate groups for the geom_smooth() Because the line color doesn’t depend on the data, it is not in an aes() argument call.\n\nmtcars %&gt;%\n    mutate(vs = factor(vs)) %&gt;%\n    ggplot(aes(x = hp, y = mpg)) +\n    geom_point(aes(color = vs), size = 5) +\n    geom_smooth(aes(group = vs), color = \"red\", method = \"lm\") +\n    theme_minimal() +\n    # We don't need to specify the relationship between the levels and the colors\n    # and labels, but it means we're less likely to make a mistake in interpretation\n    # and labelling\n    scale_color_manual(\n        values = c(\"0\" = \"#6b3df5ff\", \"1\" = \"#f5c13cff\"),\n        labels = c(\"0\" = \"V-Shaped\", \"1\" = \"Straight\")\n    ) +\n    facet_wrap(~am, labeller = as_labeller(c(\"0\" = \"Automatic\", \"1\" = \"Manual\")))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nAs you can see, once you get used to it, the layering system makes it relatively intuitive to build complex and interesting plots. We’ve only stratched the surface here, so be sure to read the suggested books and the {ggplot2} cheatsheet for more information.\n%*%\nThis is the matrix multiplication operator. It works exactly as you’d expect given matrix multiplication rules. As such, you can use it on any combination of vectors and matrices.\n\n\n\n\n\n\nImportant\n\n\n\nAs you can see below, R treats vectors as dimensionless, and will try to convert it to either a row or column vector, depending on what makes sense for the matrix multiplication\n\n\n\nmy_dbl_vec %*% my_second_dbl_vec\n\n         [,1]\n[1,] 1412.086\n\n\n\nmy_matrix &lt;- matrix(1:60, nrow = 10)\nmy_matrix\n\n      [,1] [,2] [,3] [,4] [,5] [,6]\n [1,]    1   11   21   31   41   51\n [2,]    2   12   22   32   42   52\n [3,]    3   13   23   33   43   53\n [4,]    4   14   24   34   44   54\n [5,]    5   15   25   35   45   55\n [6,]    6   16   26   36   46   56\n [7,]    7   17   27   37   47   57\n [8,]    8   18   28   38   48   58\n [9,]    9   19   29   39   49   59\n[10,]   10   20   30   40   50   60\n\nmy_dbl_vec\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nmy_dbl_vec %*% my_matrix\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]  385  935 1485 2035 2585 3135\n\nmy_matrix %*% my_dbl_vec\n\nError in my_matrix %*% my_dbl_vec: non-conformable arguments\n\nmy_matrix %*% t(my_dbl_vec)\n\nError in my_matrix %*% t(my_dbl_vec): non-conformable arguments\n\nt(my_matrix) %*% my_dbl_vec\n\n     [,1]\n[1,]  385\n[2,]  935\n[3,] 1485\n[4,] 2035\n[5,] 2585\n[6,] 3135",
    "crumbs": [
      "Pre-Requisites",
      "Just Enough R"
    ]
  },
  {
    "objectID": "project-management.html",
    "href": "project-management.html",
    "title": "Organizing A Project",
    "section": "",
    "text": "Project Structure\nThere are many ways to structure a project, but we would recommend that each project has its own folder (directory), and all your project directories sit in a single place. For example, you could have a directory called Repos that holds all of your projects. That way, when you want to find a specific project, it’s easy and in one place. As part of this, we recommend you read the section about RStudio projects.\nA suggested layout could look like this (where ${HOME} denotes your home directory, i.e., ~/ on MacOS/Linux, and C:/ on Windows):",
    "crumbs": [
      "Pre-Requisites",
      "Organizing A Project"
    ]
  },
  {
    "objectID": "project-management.html#project-structure",
    "href": "project-management.html#project-structure",
    "title": "Organizing A Project",
    "section": "",
    "text": "Important\n\n\n\nIt is crucial that this folder does not live in a cloud-synced folder e.g., in OneDrive. Cloud accounts have an unfortunate habit of creating sync errors and often rename files to circumvent issues with merging differing copies. This will ruin any chance you have of using Git in the future, which is highly recommended, as Git relies on the file names being the same.\n\n\n\n${HOME}/\n└── Documents/\n    └── Repos/\n        └── Proj/\n            ├── data/\n            ├── figs/\n            ├── funs/\n            ├── out/\n            └── src/\n                ├── cleaning.R\n                └── analysis.R\n\ndata/\nAn important idea is that you should treat your data as read-only. You and your team have likely worked hard to collect the data and it’s easy to make a changes along the way that you either forget about, or need to reverse. As most projects span a long time between the data collection and analysis stages, it can be very difficult and time-consuming to try and reverse engineer exactly what changes have been made if the data files are directly edited. Therefore, to save yourself effort and help make your work reproducible, once the data is collected it should not be edited; all the work should happen in your code, allowing it to be easily checked.\n\n\n\n\n\n\n\nTip\n\n\n\nWhen you are reading in data files in your cleaning and analysis scripts, it is good practice to use relative paths. This means that if you share your code with others, everything should still work for them. If you use explicit paths e.g. read_csv(\"/Users/callumarnold/Documents/Repos/SISMID_2023/data/niamey.csv\") then this won’t work for your collaborators, as they don’t have the same computer set up as you!\nA package we recommend using in R is the {here} package, which would turn the above code into read_csv(here::here(\"data\", \"niamey.csv\")). Not only is this easier to read, but it leverages the principle that our projects are self-contained in their own folders and uses file paths that are relative to the root of the project, so it works regardless of where people install the project folder to.\n\n\n\n\nsrc/\nIt is common practice to keep your scripts (source code) in a folder named src/. Following this practice will make it easier for others to navigate your code, helping create a reproducible work environment. The files in here may be scripts to clean the data (remember, we are treating data as read-only), and others to produce the analysis. In our workshop, it would be a good idea to have a different file for each exercise e.g., r-session-01.R\n\n\nOther subdirectories\n\nfuns/: this contains the functions you write and might want to reference. The idea is to create functions so that can give code a meaningful name. It also helps if you need to repeat a code chunk multiple times, especially if you need to edit it at some point, as you can just call the function rather than typing it out each time.\nout/: this contains files that are produced from the original data e.g. cleaned data files. You can then call them in your analysis scripts.\nfigs/: this contains figures that may be generated from your scripts.",
    "crumbs": [
      "Pre-Requisites",
      "Organizing A Project"
    ]
  },
  {
    "objectID": "project-management.html#naming-files",
    "href": "project-management.html#naming-files",
    "title": "Organizing A Project",
    "section": "Naming Files",
    "text": "Naming Files\nPart of structuring a project is having creating file names that are easy to read; both for you and the computer. On that note, get rid of any spaces in your file and folder names! They make it much trickier to work with when you want to use them in code, whether that’s using bash/zsh for moving files quickly or using Git via the command line, or loading them in analysis scripts.\nJenny Bryan (of University of British Columbia and RStudio/Posit) has great slides here on the topic, but in summary:\n\nKISS (Keep It Simple Stupid): use simple and consistent file names\n\nIt needs to be machine readable\nIt needs to be human readable\nIt needs to order well in a directory (e.g., left-pad numbers)\n\nNo special characters and no spaces!\nUse YYYY-MM-DD date format\n\nFile systems will automatically order them sensibly\nUnambiguous, which is particularly important with international collaborators\n\nUse - to delimit words and _ to delimit sections\n\ni.e. 2019-01-19_my-data.csv\n\nLeft-pad numbers\n\ni.e. 01_my-data.csv vs 1_my-data.csv\nIf you don’t, file orders get messed up when you get to double-digits",
    "crumbs": [
      "Pre-Requisites",
      "Organizing A Project"
    ]
  },
  {
    "objectID": "project-management.html#other-resources",
    "href": "project-management.html#other-resources",
    "title": "Organizing A Project",
    "section": "Other Resources",
    "text": "Other Resources\n\nGit\nGit is an essential component of reproducible computation research, although it is very much out of the scope of this workshop. Think of it as a more powerful version of tracked changes for your code, merged with some of the collaborative abilities of Google Docs. If you would like to learn more about what it is, and how you could add it to your workflow, Callum created a small online book to accompany a Git and GitHub workshop he developed for Penn State. The link can be found here, and this will be continuously updated to add more complicated workflows and troubleshooting tips.\nJenny Bryan and co put togther a fantastic resource about using Git with R here. It has more of a focus on R and the use of R-specific tools and packages to help with Git (e.g. the {usethis} package), but is still plenty general for anyone to learn about Git and best practices.",
    "crumbs": [
      "Pre-Requisites",
      "Organizing A Project"
    ]
  },
  {
    "objectID": "project-management.html#project-structure-1",
    "href": "project-management.html#project-structure-1",
    "title": "Organizing A Project",
    "section": "Project Structure",
    "text": "Project Structure\n\nCallum wrote a short blog post about reproducible work that can be found here. The section about Jupyter notebooks are unlikely to be relevant to R users, but the rest is still useful.",
    "crumbs": [
      "Pre-Requisites",
      "Organizing A Project"
    ]
  },
  {
    "objectID": "project-management.html#renv",
    "href": "project-management.html#renv",
    "title": "Organizing A Project",
    "section": "{renv}",
    "text": "{renv}\n{renv} is a package that helps manage your project’s dependencies by creating a self-contained environment for your project. What this means is that each project will have a list of the required packages and their versions, and when you share your project with others, they can install the packages you used in your project with a single command (renv::restore()). To find out more about {renv}, visit the website here.",
    "crumbs": [
      "Pre-Requisites",
      "Organizing A Project"
    ]
  },
  {
    "objectID": "L01_intro-to-modeling.html",
    "href": "L01_intro-to-modeling.html",
    "title": "1  Intro to Modeling",
    "section": "",
    "text": "Download slides",
    "crumbs": [
      "Day 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to Modeling</span>"
    ]
  },
  {
    "objectID": "L02_sir-basics.html",
    "href": "L02_sir-basics.html",
    "title": "2  Basics of SIR Models",
    "section": "",
    "text": "Download slides",
    "crumbs": [
      "Day 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basics of SIR Models</span>"
    ]
  },
  {
    "objectID": "r-session-01.html",
    "href": "r-session-01.html",
    "title": "\n3  R Session 01\n",
    "section": "",
    "text": "3.1 Interactive Plot\nCodeinit_beta = 0.3\ninit_dur_inf = 6.0\ninit_I0 = 0.01\ninit_births = 0.0\ninit_tmax = 200\nCodefunction set(input, value) {\n  input.value = value;\n  input.dispatchEvent(new Event(\"input\", {bubbles: true}));\n}",
    "crumbs": [
      "Day 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Session 01</span>"
    ]
  },
  {
    "objectID": "r-session-01.html#sec-interactive-plots",
    "href": "r-session-01.html#sec-interactive-plots",
    "title": "\n3  R Session 01\n",
    "section": "",
    "text": "Codeviewof reset = Inputs.button([\n  [\"Reset all sliders\", () =&gt; {\n    set(viewof beta, init_beta)\n    set(viewof dur_inf, init_dur_inf)\n    set(viewof I0, init_I0)\n    set(viewof births, init_births)\n    set(viewof tmax, init_tmax)\n  }]\n])\n\nviewof beta = Inputs.range(\n  [0.0, 2.0],\n  {value: init_beta, step: 0.01, label: \"Transmission rate (per day)\"}\n)\n\nviewof dur_inf = Inputs.range(\n  [0.0, 20],\n  {value: init_dur_inf, step: 0.5, label: \"Duration of Infection (days)\"}\n)\n\nviewof I0 = Inputs.range(\n  [0.0, 1.0],\n  {value: init_I0, step: 0.01, label: \"Initial fraction infected\"}\n)\n\nviewof births = Inputs.range(\n  [0, 0.05],\n  {value: init_births, step: 0.001, label: \"Birth rate\"}\n)\n\nviewof tmax = Inputs.range(\n  [200, 600],\n  {value: init_tmax, step: 10.0, label: \"Maximum simulation time (years)\"}\n)\n\nviewof area = Inputs.toggle(\n  {label: \"Cumulative Area plot\", value: false}\n)\n\nmd`${tex`R_0 = ${R0_str}`}`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode{\n  if (births == 0){\n    var finalsize = sir_sol.get(\"R\", sir_sol.numRows()-1)\n    var finalsize_str = finalsize.toLocaleString(undefined, {minimumFractionDigits: 2})\n\n    return md`${tex`\\text{Final size} = ${finalsize_str}`}`\n  } else {\n    return md``\n  }\n}\n\n\n\n\n\n\n\n\n\n\nCodegamma = 1 / dur_inf\ndt = 0.01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodeimport {odeRK4} from '@rreusser/integration@3064'\nimport { aq, op } from '@uwdata/arquero'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodefunction sir(dydt, y, t) {\n  dydt[0] = - beta * y[0] * y[1] + births * (1 - y[0])\n  dydt[1] = beta * y[0] * y[1] - gamma * y[1] - births * y[1]\n  dydt[2] = gamma * y[1] - births * y[2]\n}\n\n\n\n\n\n\n\nCodefunction simulate(f, t0, y0, dt, tmax) {\n  var t = t0\n  var y = y0\n  var i = 0\n\n  var tsim = [t0]\n  var ysim = [y0]\n\n  for (t = t0 + dt; t &lt;= tmax; t += dt) {\n    ysim.push(odeRK4([], ysim[i], f, dt))\n    tsim.push(t)\n    i += 1\n  }\n  \n  return aq.table({\n    Time: tsim,\n    S: ysim.map(d =&gt; d[0]),\n    I: ysim.map(d =&gt; d[1]),\n    R: ysim.map(d =&gt; d[2])\n    })\n}\n\n\n\n\n\n\n\nCodesir_sol = simulate(sir, 0, [1.0-I0, I0, 0.0], dt, tmax)\nsir_sol_long = sir_sol.fold(aq.not('Time'), {as: ['State', 'Fraction']})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodeSIRcolors = [\"#1f77b4\", \"#ff7f0e\", \"#FF3851\"]\n\n\n\n\n\n\n\nCodeR0 = beta / (gamma + births)\nR0_str = R0.toLocaleString(undefined, {minimumFractionDigits: 2})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodefunction calculate_equil(R0){\n  if (births &gt; 0) {\n    var eq_S = 1 / R0\n    var eq_I = births / beta * (R0 - 1)\n    var eq_R = 1 - (eq_S + eq_I)\n\n    const eq_vals = aq.table({\n      State: [\"S\", \"I\", \"R\"],\n      Fraction: [eq_S, eq_I, eq_R]\n    })\n\n    return eq_vals\n  } else {\n    return null\n  }\n}\n\n\n\n\n\n\n\nCodeeq_vals = calculate_equil(R0)\n\n\n\n\n\n\n\n\n\nCodePlot.plot({\n  color: {\n    legend: true,\n    domain: [\"S\", \"I\", \"R\"],\n    range: SIRcolors\n  },\n  style: {fontSize: \"20px\"},\n  marginLeft: 65,\n  marginTop: 40,\n  marginBottom: 55,\n  grid: true,\n  width: 800,\n  height: 670,\n  y: {domain: [0, 1]},\n  marks: [\n    area ?\n      Plot.areaY(sir_sol_long, {x: \"Time\", y: \"Fraction\", fill: \"State\"}) :\n      [\n        R0 &gt;= 1.0 && births &gt; 0 ?\n        Plot.ruleY(\n          eq_vals,\n          {y: \"Fraction\", stroke: \"State\", strokeWidth: 2, strokeDasharray: [10]}\n        ) :\n        null,\n        Plot.lineY(\n          sir_sol_long,\n          {x: \"Time\", y: \"Fraction\", stroke: \"State\", strokeWidth: 6}\n        )\n      ]\n  ]\n})",
    "crumbs": [
      "Day 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Session 01</span>"
    ]
  },
  {
    "objectID": "r-session-01.html#closed-population-model-intuition",
    "href": "r-session-01.html#closed-population-model-intuition",
    "title": "\n3  R Session 01\n",
    "section": "\n3.2 Closed Population Model Intuition",
    "text": "3.2 Closed Population Model Intuition\nWe’ll first get develop an intuition for the closed population model. We’ll then extend this intuition to the open population model. When the Birth rate slider in the side panel of the interactive figure is set to 0, we have no births or deaths, so their is no replenishment of the susceptible population i.e., it is a closed population.\n\n\n\n\n\n\nSET\n\n\n\nTransmission rate = 1\nDuration of infection = 4.\n\n\n\n3.2.1 What is \\(R_0\\)?\n\n3.2.2 What is epidemic final size?\n\n3.2.3 Does this make sense given our definition of \\(R_0\\)?\n\n\n\n\n\n\nInstruction\n\n\n\nToggle on the cumulative area button and see what the epidemic final size is (approximately)\n\n\n\n3.2.4 At approximately what time does the epidemic end?\n\n\n\n\n\n\nSET\n\n\n\nDuration of infection = 8 days\nTransmission rate so you get the same \\(R_0\\) in Section 3.2.1\n\n\n\n3.2.5 How does the epidemic final size compare?\n\n3.2.6 At what time (approx) does the epidemic end?\n\n\n\n\n\n\nNote\n\n\n\nSize is determined by \\(R_0\\), duration is determined by recovery rate \\(\\left(\\gamma = \\frac{1}{\\text{duration of infection}}\\right)\\)\n\n\nNow, imagine that we have a drug (or vaccine) available to everyone that either reduced transmission OR shortened the duration of infection.\n\n\n\n\n\n\nSET\n\n\n\nTransmission rate = 1\nDuration of infection = 8 days\n\n\n\n3.2.7 Note the epidemic final size and the time until the epidemic is over.\nNow, imagine everyone has access to the drug (unrealistic) that reduces transmission by \\(P \\%\\)\n\n3.2.8 What happens to the final size and outbreak duration?\nNow, imagine everyone has access to a drug that reduces the duration of infection from 8 to 2 days (75% reduction).\n\n3.2.9 What happens to the final size and outbreak duration?\n\n3.2.10 Which assumption would you prefer and why?\n\n\n\n\n\n\nNote\n\n\n\nThis is the only really open ended question, but should be pretty straightforward discussion",
    "crumbs": [
      "Day 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Session 01</span>"
    ]
  },
  {
    "objectID": "r-session-01.html#demographic-model-intuition",
    "href": "r-session-01.html#demographic-model-intuition",
    "title": "\n3  R Session 01\n",
    "section": "\n3.3 Demographic Model Intuition",
    "text": "3.3 Demographic Model Intuition\n\n\n\n\n\n\nSET\n\n\n\nTransmission = 1\nDuration = 8\nBirth rate = .002\n\n\n\n3.3.1 What is \\(R_0\\)?\n\n3.3.2 What is the equilibrium proportion that is susceptible?\n\n3.3.3 If you were to test for antibodies against infection in the population, what proportion would you expect to be positive?\n\n\n\n\n\n\nNote\n\n\n\nAssume a perfectly accurate serological test\n\n\n\n3.3.4 At what time (approximately) does the system reach equilibrium?\n\n\n\n\n\n\nSET\n\n\n\nBirth rate = 0.005\n\n\n\n3.3.5 What is the new \\(R_0\\)?\n\n3.3.6 At what time (approximately) does the system reach equilibrium?\n\n\n\n\n\n\nSET\n\n\n\nTransmission rate so you get the same \\(R_0\\) in Section 3.3.1\n\n\n\n3.3.7 What is the new equilibrium proportion that is susceptible?\n\n3.3.8 What is different about the prevalence of infection (equilibrium proportion that is infected) in the scenarios Section 3.3.1 and Section 3.3.5 i.e. higher birth rate with the same \\(R_0\\)?",
    "crumbs": [
      "Day 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Session 01</span>"
    ]
  },
  {
    "objectID": "r-session-01.html#model-building-with-r",
    "href": "r-session-01.html#model-building-with-r",
    "title": "\n3  R Session 01\n",
    "section": "\n3.4 Model Building With R",
    "text": "3.4 Model Building With R\n\n3.4.1 Setting Up A Script\nNow we have some intuition behind how the different parameters affect the dynamics of the SIR system, let’s look at how we can implement this in R. Below is some R code that implements the basic closed-population SIR model. The purpose of the questions in this exercise is to guide you through the code and help you understand how it works so you can modify it to answer your own questions.\n\n\n\n\n\n\n\nInstruction\n\n\n\nCopy the code below into a new R script. You can open a new script in RStudio using ctrl+shift+N (Windows) or cmd+shift+N (macOS). Save it with the name S01_basic-sir.R. This script should live in your SISMID directory, as described previously. Run it to check you get the same figure output as above.\n\n\n\nCodelibrary(tidyverse)\nlibrary(deSolve)\n\ntheme_set(theme_minimal())\n\nsir_model &lt;- function(time, state, params, ... ){\n  transmission &lt;- params[\"transmission\"]\n  recovery &lt;- 1 / params[\"duration\"]\n\n  S &lt;- state[\"S\"]\n  I &lt;- state[\"I\"]\n  R &lt;- state[\"R\"]\n\n  dSdt &lt;- -transmission * S * I\n  dIdt &lt;- (transmission * S * I) - (recovery * I)\n  dRdt &lt;- recovery * I\n\n  return(list(c(dSdt, dIdt, dRdt)))\n}\n\nsir_params &lt;- c(transmission = 0.3, duration = 6)\nsir_init_states &lt;- c(S = 0.99, I = 0.01, R = 0)\nsim_times &lt;- seq(0, 200, by = 0.1)\n\nsir_sol &lt;- deSolve::ode(\n  y = sir_init_states,\n  times = sim_times,\n  func = sir_model,\n  parms = sir_params\n)\n\n# Turn the output from the ODE solver into a tibble (dataframe)\n# so we can manipulate and plot it easily\nsir_sol_df &lt;- as_tibble(sir_sol) %&gt;%\n  # Convert all columns to numeric (they are currently type\n  # deSolve so will produce warnings when plotting etc)\n  mutate(\n    # Rather than repeatedly type the same function for every\n    # column, use the across() function to apply the function\n    # to a selection of columns\n    across(\n      # The cols argument takes a selection of columns to apply\n      # a function to. Here, we want to apply the as.numeric()\n      # function to all columns, so we use the function\n      # everything() to select all columns.\n      .cols = everything(),\n      .fns = as.numeric\n    )\n  ) %&gt;%\n  # Convert the dataframe from wide to long format, so we have a\n  # column for the time, a column for the state, and a column\n  # for the proportion of the population in that state at that\n  # time\n  pivot_longer(\n    # Don't pivot the time column\n    cols = -time,\n    names_to = \"state\",\n    values_to = \"proportion\"\n  ) %&gt;%\n  # Update the state column to be a factor, so the plot will\n  # show the states in the correct order\n  mutate(state = factor(state, levels = c(\"S\", \"I\", \"R\")))\n\nSIRcolors &lt;- c(S = \"#1f77b4\", I = \"#ff7f0e\", R = \"#FF3851\")\n\nggplot(sir_sol_df, aes(x = time, y = proportion, color = state)) +\n  geom_line(linewidth = 1.5) +\n  scale_color_manual(values = SIRcolors) +\n  labs(\n    x = \"Time\",\n    y = \"Fraction\",\n    color = \"State\"\n  ) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n3.4.2 Commenting the code\nFor this part of the exercise, go through the basic SIR code and add comments to each section of code explaining what it does. To get you started, we’ve added some comments to the creating of the dataframe object sir_sol_df between lines 32-62 in the code block above, as some of the functions used there are a bit more complicated.\n\n\n\n\n\n\nNote\n\n\n\nNormally you would not use nearly as extensive comments. Here, we’ve gone overboard to help you understand what each line does, as some may not be familiar with all the functions used. We’ve also broken up the comments into multiple lines so that it is easier to read on this website. For your code that you view in RStudio (or some other text editor), use one line per sentence of the comment i.e., start a new comment line after each period.\nGenerally, you want to use comments to explain why you are doing something, not what you are doing. Sometimes that is unavoidable (e.g., you had to look up how to do a particular thing in R and need the hints to be able to understand the code), but try to stick to this guideline where possible.\n\n\nAs you’re going through the code, if you don’t understand what a particular function does, try looking up the documentation for it! You can do this by clicking on the function within the website (as described in the intro), or by typing ?function_name into the R console (Google also is your friend here!).\n\n3.4.3 Adding in demographics\nNow we have a better sense of how the code works, let’s add in some demographic structure. To recreate the demographic model from the interactive plot in Section 3.1, we just need to add births and deaths to the system.\nRecall the equations for the demographic model:\n\\[\n\\begin{aligned}\n\\frac{dS}{dt} &= \\mu N - \\beta S I - \\mu S \\\\\n\\frac{dI}{dt} &= \\beta S I - \\gamma I - \\mu I \\\\\n\\frac{dR}{dt} &= \\gamma I - \\mu R\n\\end{aligned}\n\\tag{3.1}\\]\n\n3.4.3.1 Create a new R script called S01_demographic-sir.R and copy the code from S01_basic-sir.R into it.\n\n3.4.3.2 Rename the function sir_model() to demographic_sir_model() in your new script (S01_demographic-sir.R).\n\n3.4.3.3 Adapt the function demographic_sir_model() to match the above equations (Equation 3.1).\n\n\n\n\n\n\nSET\n\n\n\nbirth rate = 0.05\n\n\n\n3.4.3.4 Rename the variables to reflect that we are now working with a demographic model, not the basic SIR model.\n\n3.4.3.5 Run the code in the S01_demographic-sir.R file. Plot the results of your demographic model. Does it look like this?\n\n\n\n\n\n\n\n\n\n3.4.3.6 Update the comments in your code to reflect the changes you have made.",
    "crumbs": [
      "Day 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Session 01</span>"
    ]
  },
  {
    "objectID": "r-session-01.html#exercise-solutions",
    "href": "r-session-01.html#exercise-solutions",
    "title": "\n3  R Session 01\n",
    "section": "\n3.5 Exercise Solutions",
    "text": "3.5 Exercise Solutions\n\n\n\n\n\n\nNote\n\n\n\nHere, we’re using the roxygen2 package to create the comments for the functions we’ve created, i.e., sir_model &lt;- function(...). This provides a consistent framework for commenting functions, and if we wanted, we could use the comments to create documentation for our functions. The main benefit for our purposes is that the framework allows us to quickly understand exactly what the function does, as well as the context it should be used in.\n\n\n\n3.5.1 Section 3.4.2: Commented basic SIR code\n\nCode# Load packages\nlibrary(tidyverse)\nlibrary(deSolve)\n\n# Set the ggplot2 theme\ntheme_set(theme_minimal())\n\n#' Basic SIR model\n#'\n#' A basic SIR model with no demographic structure to be used in deSolve\n#'\n#' @param time deSolve passes the time parameter to the function.\n#' @param state A vector of states.\n#' @param params A vector of parameter values .\n#' @param ... Other arguments passed by deSolve.\n#'\n#' @return A deSolve matrix of states at each time step.\n#' @examples\n#' sir_params &lt;- c(transmission = 0.3, duration = 6)\n#' sir_init_states &lt;- c(S = 0.99, I = 0.01, R = 0)\n#' sim_times &lt;- seq(0, 200, by = 0.1)\n#' \n#' sir_sol &lt;- deSolve::ode(\n#'    y = sir_init_states,\n#'    times = sim_times,\n#'    func = sir_model,\n#'    parms = sir_params\n#' ))\nsir_model &lt;- function(time, state, params, ... ){\n  # Extract parameters for cleaner calculations\n  transmission &lt;- params[\"transmission\"]\n  recovery &lt;- 1 / params[\"duration\"]\n\n  # Extract states for cleaner calculations\n  S &lt;- state[\"S\"]\n  I &lt;- state[\"I\"]\n  R &lt;- state[\"R\"]\n\n  # Differential equations of the SIR model\n  dSdt &lt;- -transmission * S * I\n  dIdt &lt;- (transmission * S * I) - (recovery * I)\n  dRdt &lt;- recovery * I\n\n  # Return a list whose first element is a vector of the\n  # state derivatives - must be in the same order as the\n  # state vector (S, I, R)\n  return(list(c(dSdt, dIdt, dRdt)))\n}\n\n# Create the parameter, initial state, and time vectors\nsir_params &lt;- c(transmission = 0.3, duration = 6)\nsir_init_states &lt;- c(S = 0.99, I = 0.01, R = 0)\nsim_times &lt;- seq(0, 200, by = 0.1)\n\n# Solve the SIR model with deSolve's ode() function\nsir_sol &lt;- deSolve::ode(\n  y = sir_init_states,\n  times = sim_times,\n  func = sir_model,\n  parms = sir_params\n)\n\n# Turn the output from the ODE solver into a tibble (dataframe)\n# so we can manipulate and plot it easily\nsir_sol_df &lt;- as_tibble(sir_sol) %&gt;%\n  # Convert all columns to numeric (they are currently type\n  # deSolve so will produce warnings when plotting etc)\n  mutate(\n    # Rather than repeatedly type the same function for every\n    # column, use the across() function to apply the function\n    # to a selection of columns\n    across(\n      # The cols argument takes a selection of columns to apply\n      # a function to. Here, we want to apply the as.numeric()\n      # function to all columns, so we use the function\n      # everything() to select all columns.\n      .cols = everything(),\n      .fns = as.numeric\n    )\n  ) %&gt;%\n  # Convert the dataframe from wide to long format, so we have a\n  # column for the time, a column for the state, and a column\n  # for the proportion of the population in that state at that\n  # time\n  pivot_longer(\n    # Don't pivot the time column\n    cols = -time,\n    names_to = \"state\",\n    values_to = \"proportion\"\n  ) %&gt;%\n  # Update the state column to be a factor, so the plot will\n  # show the states in the correct order\n  mutate(state = factor(state, levels = c(\"S\", \"I\", \"R\")))\n\n# Save the colors to a vector\nSIRcolors &lt;- c(S = \"#1f77b4\", I = \"#ff7f0e\", R = \"#FF3851\")\n\n# Plot the results\nggplot(sir_sol_df, aes(x = time, y = proportion, color = state)) +\n  geom_line(linewidth = 1.5) +\n  scale_color_manual(values = SIRcolors) +\n  labs(\n    x = \"Time\",\n    y = \"Fraction\",\n    color = \"State\"\n  ) +\n  theme(legend.position = \"top\")\n\n\n\n3.5.2 Section 3.4.3: Commented demographic SIR code\n\nCode# Load packages\nlibrary(tidyverse)\nlibrary(deSolve)\n\n# Set the ggplot2 theme\ntheme_set(theme_minimal())\n\n#' Demographic SIR model\n#'\n#' An SIR model with births and deaths (constant pop) to be used in deSolve\n#'\n#' @param time deSolve passes the time parameter to the function.\n#' @param state A vector of states.\n#' @param params A vector of parameter values .\n#' @param ... Other arguments passed by deSolve.\n#'\n#' @return A deSolve matrix of states at each time step.\n#' @examples\n#' sir_params &lt;- c(transmission = 0.3, duration = 6, birth_rate = 0.05)\n#' sir_init_states &lt;- c(S = 0.99, I = 0.01, R = 0)\n#' sim_times &lt;- seq(0, 200, by = 0.1)\n#' \n#' sir_sol &lt;- deSolve::ode(\n#'    y = sir_init_states,\n#'    times = sim_times,\n#'    func = sir_demog_model,\n#'    parms = sir_params\n#' ))\nsir_demog_model &lt;- function(time, state, params, ... ){\n  transmission &lt;- params[\"transmission\"]\n  recovery &lt;- 1 / params[\"duration\"]\n  birth_rate &lt;- params[\"birth_rate\"]\n\n  S &lt;- state[\"S\"]\n  I &lt;- state[\"I\"]\n  R &lt;- state[\"R\"]\n\n  dSdt &lt;- birth_rate -transmission * S * I - (birth_rate * S)\n  dIdt &lt;- (transmission * S * I) - (recovery * I) - (birth_rate * I)\n  dRdt &lt;- (recovery * I) - (birth_rate * R)\n\n  return(list(c(dSdt, dIdt, dRdt)))\n}\n\n# Create the parameter, initial state, and time vector\nsir_demog_params &lt;- c(transmission = 0.3, duration = 6, birth_rate = 0.05)\nsir_init_states &lt;- c(S = 0.99, I = 0.01, R = 0)\nsim_times &lt;- seq(0, 200, by = 0.1)\n\n# Solve the SIR model with deSolve's ode() function\nsir_demog_sol &lt;- deSolve::ode(\n  y = sir_init_states,\n  times = sim_times,\n  func = sir_demog_model,\n  parms = sir_demog_params\n)\n\n# Turn the output from the ODE solver into a tibble (dataframe)\n# so we can manipulate and plot it easily\nsir_demog_sol_df &lt;- as_tibble(sir_demog_sol) %&gt;%\n  # Convert all columns to numeric (they are currently type\n  # deSolve so will produce warnings when plotting etc)\n  mutate(\n    # Rather than repeatedly type the same function for every\n    # column, use the across() function to apply the function\n    # to a selection of columns\n    across(\n      # The cols argument takes a selection of columns to apply\n      # a function to. Here, we want to apply the as.numeric()\n      # function to all columns, so we use the function\n      # everything() to select all columns.\n      .cols = everything(),\n      .fns = as.numeric\n    )\n  ) %&gt;%\n  # Convert the dataframe from wide to long format, so we have a\n  # column for the time, a column for the state, and a column\n  # for the proportion of the population in that state at that\n  # time\n  pivot_longer(\n    # Don't pivot the time column\n    cols = -time,\n    names_to = \"state\",\n    values_to = \"proportion\"\n  ) %&gt;%\n  # Update the state column to be a factor, so the plot will\n  # show the states in the correct order\n  mutate(state = factor(state, levels = c(\"S\", \"I\", \"R\")))\n\n# Save the colors to a vector\nSIRcolors &lt;- c(S = \"#1f77b4\", I = \"#ff7f0e\", R = \"#FF3851\")\n\n# Plot the results\nggplot(sir_demog_sol_df, aes(x = time, y = proportion, color = state)) +\n  geom_line(linewidth = 1.5) +\n  scale_color_manual(values = SIRcolors) +\n  labs(\n    x = \"Time\",\n    y = \"Fraction\",\n    color = \"State\"\n  ) +\n  theme(legend.position = \"top\")",
    "crumbs": [
      "Day 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Session 01</span>"
    ]
  },
  {
    "objectID": "L03_hit-and-vaccinations.html",
    "href": "L03_hit-and-vaccinations.html",
    "title": "4  Herd Immunity & Vaccination",
    "section": "",
    "text": "Download slides",
    "crumbs": [
      "Day 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Herd Immunity & Vaccination</span>"
    ]
  },
  {
    "objectID": "L04_heterogeneity.html",
    "href": "L04_heterogeneity.html",
    "title": "5  Heterogeneity in Models",
    "section": "",
    "text": "Download slides",
    "crumbs": [
      "Day 2",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Heterogeneity in Models</span>"
    ]
  },
  {
    "objectID": "L05_age-structure.html",
    "href": "L05_age-structure.html",
    "title": "6  Age Structured Models",
    "section": "",
    "text": "Download slides",
    "crumbs": [
      "Day 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Age Structured Models</span>"
    ]
  },
  {
    "objectID": "r-session-02.html",
    "href": "r-session-02.html",
    "title": "\n7  R Session 02\n",
    "section": "",
    "text": "7.1 Load Packages\nCodelibrary(diagram)\nlibrary(deSolve)\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(rio)\nCodetheme_set(theme_minimal())",
    "crumbs": [
      "Day 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Session 02</span>"
    ]
  },
  {
    "objectID": "r-session-02.html#a-model-with-2-classes",
    "href": "r-session-02.html#a-model-with-2-classes",
    "title": "\n7  R Session 02\n",
    "section": "\n7.2 A Model With 2 Classes",
    "text": "7.2 A Model With 2 Classes\nWe’ll start with the simplest mechanistic model of two classes we can think of, which has separate classes for two groups \\(a\\) and \\(b\\). These groups could represent different socioeconomic classes, for example.\n\n\n\n\n\n\n\n\nWhich can be written in equations as, \\[\n\\begin{aligned}\n    \\frac{\\dd{S_a}}{\\dd{t}} &= -\\lambda_a\\,S_a \\phantom{-\\gamma\\,I_b}\\\\\n    \\frac{\\dd{S_b}}{\\dd{t}} &= -\\lambda_b\\,S_b \\phantom{-\\gamma\\,I_b}\\\\\n    \\frac{\\dd{I_a}}{\\dd{t}} &= \\phantom{-}\\lambda_a\\,S_a -\\gamma\\,I_a\\\\\n    \\frac{\\dd{I_b}}{\\dd{t}} &= \\phantom{-}\\lambda_b\\,S_b-\\gamma\\,I_b\\\\\n    \\frac{\\dd{R_a}}{\\dd{t}} &= \\phantom{-\\lambda_a\\,S_b}+\\gamma\\,I_a\\\\\n    \\frac{\\dd{R_b}}{\\dd{t}} &= \\phantom{-\\lambda_a\\,S_b}+\\gamma\\,I_b\\\\\n  \\end{aligned}\n\\]\nThe \\(\\lambda\\)s denote the group-specific force of infections:\n\\[\n\\begin{aligned}\n        \\lambda_a &= \\beta_{aa}\\,I_a+\\beta_{ab}\\,I_b\\\\\n        \\lambda_b &= \\beta_{ba}\\,I_a+\\beta_{bb}\\,I_b\n\\end{aligned}\n\\]\nIn this model, each population can infect each other but the infection moves through the populations separately. Let’s simulate such a model. To make things concrete, we’ll assume that the transmission rates \\(\\beta\\) are greater within groups than between them.\n\nCode# Create a named parameter vector that we can index by name in the model\nab_params &lt;- c(\n    beta_within = 0.025,\n    beta_between = 0.005,\n    recovery = 10\n)\n\n\n\nCode# Here we set up the ODE model that matches the equations above\nab_model &lt;- function (t, x, p, ...) {\n    # Unpack the state variables\n    Sa &lt;- x[\"Sa\"]\n    Sb &lt;- x[\"Sb\"]\n    Ia &lt;- x[\"Ia\"]\n    Ib &lt;- x[\"Ib\"]\n    \n    # Unpack the parameters\n    beta_within &lt;- p[\"beta_within\"]\n    beta_between &lt;- p[\"beta_between\"]\n    recovery &lt;- p[\"recovery\"]\n\n    # group A force of infection\n    lambda_a &lt;- beta_within * Ia + beta_between * Ib\n\n    # group B force of infection\n    lambda_b &lt;- beta_within * Ib + beta_between * Ia\n    \n    # The ODEs\n    dSadt &lt;- - lambda_a * Sa\n    dSbdt &lt;- - lambda_b * Sb\n    dIadt &lt;- lambda_a * Sa - recovery * Ia\n    dIbdt &lt;- lambda_b * Sb - recovery * Ib\n    dRadt &lt;- recovery * Ia\n    dRbdt &lt;- recovery * Ib\n\n    # Return the derivatives\n    list(c(\n        dSadt,\n        dSbdt,\n        dIadt,\n        dIbdt,\n        dRadt,\n        dRbdt\n    ))\n}\n\n\n\nCode# initial conditions\nab_yinit &lt;- c(Sa = 1000, Sb = 2000, Ia = 1, Ib = 1, Ra = 0, Rb = 0)\n\n# Run the ODE solver from the deSolve package\nab_sol &lt;- deSolve::ode(\n    y = ab_yinit,\n    times = seq(0, 2, by = 0.001),\n    func = ab_model,\n    parms = ab_params,\n)\n\n\n\nCodeab_df &lt;- ab_sol %&gt;%\n    # Convert the solution to a tibble for manipulation\n    as_tibble() %&gt;%\n    # Create and modify columns\n    mutate(\n        # Convert all columns into type numeric\n        across(everything(), as.numeric),\n        # Create new columns to track pop sizes in each group\n        Na = Sa + Ia + Ra,\n        Nb = Sb + Ib + Rb\n    ) %&gt;%\n    # Go from a wide to long dataframe for ggplot\n    pivot_longer(\n        cols = -time,\n        names_to = c(\"state\", \"group\"),\n        names_sep = 1,\n        values_to = \"value\"\n    ) %&gt;%\n    # Clean pivoted columns for ordered plots\n    mutate(\n        state = factor(state, levels = c(\"S\", \"I\", \"R\", \"N\")),\n        group = paste(\"Group\", str_to_upper(group))\n    )\n\n\n\nCode# Create a vector of colors to be used throughout the ggplots\nSIRcolors &lt;- c(\"#1f77b4\", \"#ff7f0e\", \"#FF3851\", \"#591099\")\n\nggplot(ab_df, aes(x = time, y = value, color = state)) +\n    geom_line(linewidth = 1.5) +\n    facet_wrap(~group, scales = \"free_y\") +\n    scale_color_manual(\n        values = SIRcolors,\n        labels = c(\"Susceptible\", \"Infected\", \"Recovered\", \"Total\")\n    ) +\n    labs(\n        x = \"Time\",\n        y = \"Number of individuals\",\n        color = \"State\"\n    ) +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nDespite using the same transmission rates, the epidemic in group B is much larger than in group A. Why do you think this is?\n\nNow let’s plot the proportion of individuals in each state for the two groups.\n\nCodeab_df_props &lt;- ab_df %&gt;%\n    # Remove total pop count as we only want the group-specific values\n    filter(state != \"N\") %&gt;%\n    mutate(\n        # Concatenate the state variable and the group letter for each row\n        state_group = paste0(state, str_extract_all(group, \"[^Group ]\")),\n        # Factor new variable for nicer plotting\n        state_group = factor(\n            state_group,\n            levels = c(\"RA\", \"RB\", \"IA\", \"IB\", \"SA\", \"SB\")\n        )\n    ) %&gt;%\n    # Group by time and state_group so we can calculate the relevant\n    # proportions over time\n    group_by(time, state_group) %&gt;%\n    mutate(\n        prop = value / sum(ab_yinit)\n    ) %&gt;%\n    ungroup()\n\n\n\nCode# Create new vectors of colors as using 6: one of each for A and J groups\nScolors &lt;- RColorBrewer::brewer.pal(3, \"Blues\")[c(2, 3)]\nIcolors &lt;- RColorBrewer::brewer.pal(3, \"Oranges\")[c(2, 3)]\nRcolors &lt;- RColorBrewer::brewer.pal(3, \"Greens\")[c(2, 3)]\n\nggplot(ab_df_props, aes(x = time, y = prop, fill = state_group)) +\n    geom_area() +\n    scale_fill_manual(\n        values = c(Scolors, Icolors, Rcolors),\n        limits = c(\"SA\", \"SB\", \"IA\", \"IB\", \"RA\", \"RB\"),\n    ) +\n    labs(\n        x = \"Time\",\n        y = \"Proportion of individuals\",\n        fill = \"State\"\n    ) +\n    theme(legend.position = \"bottom\")",
    "crumbs": [
      "Day 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Session 02</span>"
    ]
  },
  {
    "objectID": "r-session-02.html#a-model-with-2-age-classes",
    "href": "r-session-02.html#a-model-with-2-age-classes",
    "title": "\n7  R Session 02\n",
    "section": "\n7.3 A Model With 2 Age Classes",
    "text": "7.3 A Model With 2 Age Classes\nNote that age is a special kind of heterogeneity in an epidemic model because individuals necessarily move from one class (younger) to another class (older) in a directional fashion that is independent of the infection and recovery process.\nWe’ll start by introducing age into the model above. So now \\(a\\) becomes juveniles and \\(b\\) becomes adults. And, independent of the disease process, juveniles (of any category) age into adults. Additionally, new juveniles are added through births (always first susceptible) and old individuals are lost to death.\n\n\n\n\n\n\n\n\nWe can do this very simply using the same ingredients that go into the basic SIR model. In that model, the waiting times in the S and I classes are exponential. Let’s assume the same thing about the aging process. We’ll also add in births into the juvenile susceptible class and deaths from the adult classes.\n\\[\n  \\begin{aligned}\n    \\frac{\\dd{S_J}}{\\dd{t}} &= B -\\lambda_J\\,S_J \\phantom{- \\gamma\\,I_A} -\\alpha\\,S_J \\phantom{-\\mu\\,S_A}\\\\\n    \\frac{\\dd{S_A}}{\\dd{t}} &= \\phantom{B} - \\lambda_A\\,S_A \\phantom{- \\gamma\\,I_A} +\\alpha\\,S_J -\\mu\\,S_A\\\\\n    \\frac{\\dd{I_J}}{\\dd{t}} &= \\phantom{B} +\\lambda_J\\,S_J - \\gamma\\,I_J -\\alpha\\,I_J \\phantom{-\\mu\\,S_A}\\\\\n    \\frac{\\dd{I_A}}{\\dd{t}} &= \\phantom{B} +\\lambda_A\\,S_A - \\gamma\\,I_A + \\alpha\\,I_J - \\mu\\,I_A\\\\\n    \\frac{\\dd{R_J}}{\\dd{t}} &= \\phantom{B - \\lambda_J\\,S_A} + \\gamma\\,I_J - \\alpha\\,R_J \\phantom{- \\mu\\,S_A}\\\\\n    \\frac{\\dd{R_A}}{\\dd{t}} &= \\phantom{B - \\lambda_J\\,S_A} + \\gamma\\,I_A + \\alpha\\,R_J -\\mu\\,R_A\\\\\n  \\end{aligned}\n\\]\nNow, let’s simulate this model, under the same assumptions about transmission rates as above.\n\nCode# define the parameters for the demographic model\ndemog_params &lt;- c(\n    beta_within = 0.004,\n    beta_between = 0.002,\n    recovery = 10,\n    births = 100,\n    # Width of age bands in years\n    age_band_j = 20,\n    age_band_a = 60\n)\n\n\n\nCodedemog_model &lt;- function (t, x, p, ...) {\n    # Unpack states\n    Sj &lt;- x[\"Sj\"]\n    Sa &lt;- x[\"Sa\"]\n    Ij &lt;- x[\"Ij\"]\n    Ia &lt;- x[\"Ia\"]\n    Rj &lt;- x[\"Rj\"]\n    Ra &lt;- x[\"Ra\"]\n\n    # Unpack parameters from vector\n    beta_within &lt;- p[\"beta_within\"]\n    beta_between &lt;- p[\"beta_between\"]\n    recovery &lt;- p[\"recovery\"]\n    births &lt;- p[\"births\"]\n    # Calculate rate of aging from each age group\n    aging_j &lt;- 1 / p[\"age_band_j\"]\n    aging_a &lt;- 1 / p[\"age_band_a\"]\n\n    # juv. force of infection\n    lambda_j &lt;- beta_within * Ij + beta_between * Ia\n\n    # adult. force of infection\n    lambda_a &lt;- beta_within * Ia + beta_between * Ij\n\n    # Calculate the ODEs\n    dSjdt &lt;- births - (lambda_j * Sj) - (aging_j * Sj)\n    dSadt &lt;- -(lambda_a * Sa) + (aging_j * Sj) - (aging_a * Sa)\n    dIjdt &lt;- (lambda_j * Sj) - (recovery * Ij) - (aging_j * Ij)\n    dIadt &lt;- (lambda_a * Sa) - (recovery * Ia) +\n        (aging_j * Ij) - (aging_a * Ia)\n    dRjdt &lt;- (recovery * Ij) - (aging_j * Rj)\n    dRadt &lt;- (recovery * Ia) + (aging_j * Rj) - (aging_a * Ra)\n\n    # Return the ODEs\n    list(c(\n        dSjdt,\n        dSadt,\n        dIjdt,\n        dIadt,\n        dRjdt,\n        dRadt\n    ))\n}\n\n\nNote that in this function, \\(\\mu=\\) aging_a \\(=\\) 1 / p[\"age_band_a\"], i.e., death, is just like another age class.\n\nCode# initial conditions\ndemog_yinit &lt;- c(Sj = 2000, Sa = 3000, Ij = 0, Ia = 1, Rj = 0, Ra = 1000)\n\n# Solve the demographic model\ndemog_sol &lt;- deSolve::ode(\n    y = demog_yinit,\n    times = seq(0, 200, by = 0.1),\n    func = demog_model,\n    parms = demog_params\n)\n\ndemog_df &lt;- demog_sol %&gt;%\n    as_tibble() %&gt;%\n    mutate(\n        across(everything(), as.numeric),\n        Nj = Sj + Ij + Rj,\n        Na = Sa + Ia + Ra,\n        # Calculate total population as need for proportional area plots\n        N = Nj + Na\n    ) %&gt;%\n    pivot_longer(\n        cols = -c(time, N),\n        names_to = c(\"state\", \"group\"),\n        names_sep = 1,\n        values_to = \"value\"\n    ) %&gt;%\n    mutate(\n        state = factor(state, levels = c(\"S\", \"I\", \"R\", \"N\")),\n        group = paste(\"Group\", str_to_upper(group))\n    )\n\n\n\n7.3.1 Exercise 1: Use this code to plot the number of susceptible, infected, and recovered individuals over time\n\nCodeggplot(demog_df, aes(x = time, y = value, color = state)) +\n    geom_line(linewidth = 1.5) +\n    facet_wrap(\n        ~group, nrow = 2, scales = \"free_y\",\n        labeller = as_labeller(c(\n            `Group A` = \"Adults\",\n            `Group J` = \"Juveniles\"\n        ))\n    ) +\n    scale_color_manual(\n        values = SIRcolors,\n        labels = c(\"Susceptible\", \"Infected\", \"Recovered\", \"Total\")\n    ) +\n    labs(\n        x = \"Time\",\n        y = \"Number of individuals\",\n        color = \"State\"\n    ) +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nNote that now that births are replenishing susceptibles, infection persists. The results of the above are plotted here:\nNow let’s plot the proportion of individuals in each state for the two groups.\n\nCode# Calculate the proportions in each state and group at each time point\ndemog_df_props &lt;- demog_df %&gt;%\n    filter(state != \"N\") %&gt;%\n    mutate(\n        state_group = paste0(state, str_extract_all(group, \"[^Group ]\")),\n        state_group = factor(\n          state_group,\n          levels = c(\"RJ\", \"RA\", \"IJ\", \"IA\", \"SJ\", \"SA\")\n        )\n    ) %&gt;%\n    group_by(time, state_group) %&gt;%\n    mutate(\n        # Calculate the proportion of the total population, not the group pop\n        prop = value / N\n    ) %&gt;%\n    ungroup()\n\n\n\nCodeggplot(demog_df_props, aes(x = time, y = prop, fill = state_group)) +\n    geom_area() +\n    scale_fill_manual(\n        values = c(Scolors, Icolors, Rcolors),\n        limits = c(\"SJ\", \"SA\", \"IJ\", \"IA\", \"RJ\", \"RA\")\n    ) +\n    labs(\n        x = \"Time\",\n        y = \"Proportion of individuals\",\n        fill = \"State\"\n    ) +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nNow let’s plot the equilibrium seroprevalence for each age group.\n\nCode# Select the last row (time point) of the data frame\ndemog_equil_seroprev &lt;- tail(demog_df) %&gt;%\n    mutate(\n        # Calculate the proportion of individuals in each state and age group\n        prop = value / sum(value),\n        # Relabel groups for plots\n        group = case_when(group == \"Group J\" ~ \"Juveniles\", TRUE ~ \"Adults\"),\n        group = factor(group, levels = c(\"Juveniles\", \"Adults\")),\n        .by = group\n    ) %&gt;%\n    filter(state == \"R\")\n\n\n\nCode# Create vector of colors to distinguish between age groups\nage_group_colors &lt;- c(\"#2980B9\", \"#154360\")\n\nggplot(demog_equil_seroprev, aes(x = group, y = prop, fill = group)) +\n    geom_col(position = \"identity\") +\n    scale_fill_manual(\n        values = age_group_colors\n    ) +\n    labs(\n        x = \"Age group\",\n        y = \"Equilibrium seroprevalence\",\n        fill = \"Age group\"\n    ) +\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\nOne thing we are often interested in is the \\(R_0\\) of a system. The details are beyond the scope of this workshop and are not required to complete the exercises in this worksheet, but we have outlined them in Section 7.6.1.1, particularly in Equation 7.3, at the end of this page.\nIn our system, \\(R_0 =\\) 2.66.",
    "crumbs": [
      "Day 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Session 02</span>"
    ]
  },
  {
    "objectID": "r-session-02.html#getting-more-realistic-adding-more-age-classes",
    "href": "r-session-02.html#getting-more-realistic-adding-more-age-classes",
    "title": "\n7  R Session 02\n",
    "section": "\n7.4 Getting more realistic: adding more age classes",
    "text": "7.4 Getting more realistic: adding more age classes\nIn the models above, the aging process follows an exponential distribution, which means that whether an individual is 1~year old or 10 years old, the chance of them becoming an adult is the same! To improve on this, we can assume that the time a juvenile must wait before becoming an adult follows a gamma distribution. This is equivalent to saying that the waiting time is a sum of some number of exponential distributions. This suggests that we can achieve such a distribution by adding age classes to the model, so that becoming an adult means passing through some number of stages. We’ll use 30 age classes, and since they don’t have to be of equal duration, we’ll assume that they’re not. Specifically, we’ll have 20 1-yr age classes to take us up to adulthood and break adults into 10 age classes of 5~yr duration each. The last age class covers age 66-80.\nNow, when we had just two age classes, we could write out each of the equations easily enough, but now that we’re going to have 30, we’ll need to be more systematic. In particular, we’ll need to think of \\(\\beta\\) as a matrix of transmission rates. Let’s see how to define such a matrix in R. So that we don’t change too many things all at once, let’s keep the same contact structure as in the juvenile-adult model.\n\nCode# Set up the parameters for model that incorporates a more realistic age matrix\nages_params &lt;- c(\n    beta_j = 0.02,\n    beta_a = 0.01,\n    beta_aj = 0.01 / 2,\n    recovery = 10,\n    births = 100\n)\n\n# Create a vector of ages\nages &lt;- c(seq(1, 20, by = 1), seq(25, 65, by = 5), 80)\n\n# Calculate the widths of the age bands\nda_ages &lt;- diff(c(0, ages))\n\n# set up a matrix of contact rates between classes: more contact\n# within juveniles and adults than between\nages_beta_mat &lt;- matrix(nrow = 30, ncol = 30)\n\n# transmission rate for juveniles\nages_beta_mat[1:20, 1:20] &lt;- ages_params[\"beta_j\"]\n\n# transmission rate for adults\nages_beta_mat[21:30, 21:30] &lt;- ages_params[\"beta_a\"]\n\n# lower transmission rate between juveniles and adults\nages_beta_mat[1:20, 21:30] &lt;- ages_params[\"beta_aj\"]\n\n# lower transmission rate between juveniles and adults\nages_beta_mat[21:30, 1:20] &lt;- ages_params[\"beta_aj\"]\n\n\n\n\nCodeages_beta_mat %&gt;%\n    # Turn into a data.frame so we can use ggplot()\n    as.data.frame.table() %&gt;%\n    mutate(\n        age_contactor = rep(ages, 30),\n        # Repeat each age in ages vector 30 times before moving to next\n        age_contactee = rep(ages, each = 30)\n    ) %&gt;%\n    ggplot(aes(x = age_contactor, y = age_contactee, z = Freq)) +\n    geom_contour_filled(bins = 8) +\n    scale_fill_brewer(\n        palette = \"Reds\",\n        # Don't drop unused bins as useful for ensuring extremities properly\n        # binned\n        drop = FALSE\n    ) +\n    labs(x = \"Age of Contactor\", y = \"Age of Contactee\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nYou could also use the filled.contour() base-R function to plot the beta matrix without needing to do any dataframe modifications, but it doesn’t look as nice …\n\nCodefilled.contour(\n    ages, ages, ages_beta_mat,\n    plot.title = title(\n        xlab = \"Age of Contactor\",\n        ylab = \"Age of Contactee\"\n    )\n)\n\n\n\n\n\n\n\n\n\n\nWe’ll assume that, at the time of introduction, all children are susceptible, as are adults over 45, but that individuals aged 20–45 have seen the pathogen before and are immune. The vector yinit expresses these initial conditions.\n\nCode# Create a long vector of initial states with only one\n# initial infection in age 50\ndemog_yinit_ages &lt;- c(\n    S = c(rep(100, 20), rep(0, 5), rep(200, 5)),\n    I = c(rep(0, 25), 1, rep(0, 4)),\n    R = c(rep(0, 20), rep(1000, 5), rep(0, 5))\n)\n\n\nNote that we’re starting out with 1 infected individual in the 26th age class (age 50).\nThe codes that follow will be a bit easier to follow if we introduce some indexes that will allow us to pick out certain bits of the yinit vector.\n\nCode# Create vectors of indices relating to each state\n# (ordered S1-80, I1-80, R1-80)\nsindex &lt;- 1:30\niindex &lt;- 31:60\nrindex &lt;- 61:90\n# Create vectors of indices relating to age group\njuvies &lt;- 1:20\nadults &lt;- 21:30\n\n\nNow, to capture the aging process, it’s convenient to define another matrix to hold the rates of movement between age classes. Generally, this matrix would look like this:\n\\[\n\\begin{pmatrix}\n    -\\alpha_1 & 0 & 0 & \\cdots & 0\\\\\n    \\alpha_1 & -\\alpha_2 & 0 & \\cdots & 0\\\\\n    0 & \\alpha_2 & -\\alpha_3 & \\cdots & 0\\\\\n    \\vdots &  & \\ddots & \\ddots & \\vdots \\\\\n    0 & \\cdots & & \\alpha_{29} & -\\alpha_{30}\\\\\n\\end{pmatrix}\n\\tag{7.1}\\]\n\nCode# Create a diagonal matrix that holds the rates of aging out of each age class\n# The rows represent the age class you're in, the columns represent the age\n# class you're moving to\naging_mat &lt;- diag(-1 / da_ages)\n\n# Fill in the rates of aging into each age class\naging_mat[row(aging_mat) - col(aging_mat) == 1] &lt;- 1 / head(da_ages, -1)\n\n\nHave a look at the aging matrix, for example by doing:\n\nCode# Move fast through the 1-year age classes - negatives are moves out, positives\n# are moves in. Cannot move between non-adjacent age classes\naging_mat[1:5, 1:5]\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   -1    0    0    0    0\n[2,]    1   -1    0    0    0\n[3,]    0    1   -1    0    0\n[4,]    0    0    1   -1    0\n[5,]    0    0    0    1   -1\n\n\n\nCode# Move slowly between the wider age classes\naging_mat[25:30, 25:30]\n\n     [,1] [,2] [,3] [,4] [,5]        [,6]\n[1,] -0.2  0.0  0.0  0.0  0.0  0.00000000\n[2,]  0.2 -0.2  0.0  0.0  0.0  0.00000000\n[3,]  0.0  0.2 -0.2  0.0  0.0  0.00000000\n[4,]  0.0  0.0  0.2 -0.2  0.0  0.00000000\n[5,]  0.0  0.0  0.0  0.2 -0.2  0.00000000\n[6,]  0.0  0.0  0.0  0.0  0.2 -0.06666667\n\n\n\nCodeaging_mat %&gt;%\n    as.data.frame.table() %&gt;%\n    mutate(\n        age_recipient = rep(ages, 30),\n        # Repeat each age in ages vector 30 times before moving to next\n        age_source = rep(ages, each = 30)\n    ) %&gt;%\n    ggplot(aes(x = as.factor(age_source), y = as.factor(age_recipient), z = Freq)) +\n    geom_tile(colour = \"grey\", size = 0.4, aes(fill=Freq))  + scale_fill_gradientn(\n    colours = c(\"red\", \"white\", \"blue\")\n  ) + \n    labs(x = \"Source Age Group\", y = \"Recipient Age Group\")\n\n\n\n\n\n\n\n\n7.4.1 Exercise 2: What can you say about its structure? How are the different age groups in contact with each other?\nNow we can put the pieces together to write a simulator for the age-structured SIR dynamics.\n\nCode# Using a list instead of a vector to hold the parameters, as ages_beta_mat and\n# aging are both matrices, so we want to keep them as matrices, rather than\n# flattening\nmultistage_params &lt;- list(\n    beta_mat = ages_beta_mat,\n    recovery = ages_params[\"recovery\"],\n    births = ages_params[\"births\"],\n    aging_mat = aging_mat\n)\n\nmultistage_model &lt;- function (t, x, p, ...) {\n    # Unpack all states from the vector using the relevant indices\n    s &lt;- x[sindex]\n    i &lt;- x[iindex]\n    r &lt;- x[rindex]\n    \n    # Unpack parameters\n    beta_mat &lt;- p[[\"beta_mat\"]]\n    recovery &lt;- p[[\"recovery\"]]\n    births &lt;- p[[\"births\"]]\n    aging_mat &lt;- p[[\"aging_mat\"]]\n\n    # Calculate force of infection using matrix multiplication\n    lambda &lt;- beta_mat %*% i\n    \n    # Calculate the ODEs at every time step\n    # Note that R add element-wise for vectors i.e. lambda * s results\n    # in a vector length 30 (30 age groups), as does aging_mat %*% s,\n    # so v1[i] + v2[i] for i in 1:30\n    dsdt &lt;- -lambda * s + aging_mat %*% s\n    didt &lt;- lambda * s + aging_mat %*% i - recovery * i\n    drdt &lt;- aging_mat %*% r + recovery * i \n    # Add the birth rate to the first age group\n    dsdt[1] &lt;- dsdt[1] + births\n    \n    # Return the ODEs in a list\n    list(c(dsdt, didt, drdt))\n\n}\n\n\nWe can plug this into ode just as we did the simpler models to simulate an epidemic. We’ll then plot the epidemic curve.\n\nCode# Solve the model with a realistic age matrix\nmultistage_sol &lt;- deSolve::ode(\n    y = demog_yinit_ages,\n    times = seq(0, 100, by = 0.1),\n    func = multistage_model,\n    parms = multistage_params\n)\n\n# Extract all infected age groups at all time points into a new vector\nmultistage_infecteds &lt;- multistage_sol[, 1 + iindex]\n\n\n\nCode# Create a dataframe of the sum of infectious individuals in Juv/Adult age groups\n# at each time point\nmultistage_df &lt;- tibble(\n        # Get all times from model run\n        time = multistage_sol[, 1],\n        # At each timepoint, apply the sum function to all juvenile infected\n        # individuals\n        Juveniles = apply(multistage_infecteds[, juvies], 1, sum),\n        # At each timepoint, apply the sum function to all adult infected\n        # individuals\n        Adults = apply(multistage_infecteds[, adults], 1, sum)\n    ) %&gt;%\n    # Pivot to create a long dataframe that works with ggplot\n    pivot_longer(\n        cols = c(Juveniles, Adults),\n        names_to = \"age_group\",\n        values_to = \"infections\"\n    ) %&gt;%\n    # Turn new pivoted variable into a factor to plot nicely\n    mutate(\n        age_group = factor(age_group, levels = c(\"Juveniles\", \"Adults\"))\n    )\n\n\n\nCodeggplot(multistage_df, aes(x = time, y = infections, color = age_group)) +\n    geom_line(linewidth = 1.5) +\n    scale_color_manual(\n        values = age_group_colors\n    ) +\n    labs(\n        x = \"Time\",\n        y = \"Number of infections\",\n        color = \"Age group\"\n    )\n\n\n\n\n\n\n\nLet’s mimic a situation where we have cross-sectional seroprevalence data (e.g. measures of antibodies that tell you someone is in the R class). In using such data, we’d typically assume that the system was at equilibrium.\n\n7.4.2 Exercise 3: What does the equilibrium age-specific seroprevalence look like in this example?\nUse the code below to display the age-specific seroprevalence (i.e., the seroprevalence for each age group at equilibrium)\n\nCode# Get the last values for all individuals. drop() removes the column name,\n# [-1] removes the time value\nmultistage_equil &lt;- drop(tail(multistage_sol, 1))[-1]\n# Calculate the equilibrium pop sizes of each age group\nmultistage_equil_n &lt;- multistage_equil[sindex] +\n    multistage_equil[iindex] +\n    multistage_equil[rindex]\n\n# Calculate equilibrium seroprevalence for each age group\nmultistage_equil_seroprev &lt;- multistage_equil[rindex] / multistage_equil_n\n\n# Create a dataframe to store equilibrium seroprev for plotting\nmultistage_equil_seroprev_df &lt;- tibble(\n    age = ages,\n    seroprev = multistage_equil_seroprev,\n    width = da_ages\n)\n\n\n\nCodeggplot(multistage_equil_seroprev_df, aes(x = age, y = seroprev, fill = age)) +\n    # Set column width to width of age bands, and justify to start at\n    # lower bound\n    geom_col(\n        width = multistage_equil_seroprev_df$width,\n        just = 1.0, color = \"black\"\n    ) +\n    labs(\n        x = \"Age\",\n        y = \"Seroprevalence\"\n    ) +\n    scale_x_continuous(breaks = seq(0, 80, 10)) +\n    scale_fill_continuous(\n        low = age_group_colors[1],\n        high = age_group_colors[2]\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\nQUESTION\n\n\n\nAt what age does the seroprevalence reach 75%?\n\n\nLet’s also compute \\(R_0\\). And because we’ve added a lot of age structure, with transitions between the age groups, we can’t just copy and paste the previous Next Generation Matrix code (from Section 7.6.1.1). As before, the details of this computation are out of the workshop’s scope, but they are outlined in Section 7.6.1.2. We have created a function to calculate R0 for an age-structured SIR and have added some comments, but read Section 7.6.1.2 for the full details and reasoning.\n\nCode# Calculate the stable disease-free age distribution.\n# Could also simulate without any infections.\nmultistage_stable_n &lt;- solve(\n    aging_mat,\n    c(-1 * multistage_params[[\"births\"]], rep(0, 29))\n)\n\n\n\n\nCode#' Calculate R0\n#'\n#' Calculate the R0 of an SIR model using the next generation matrix approach\n#' 'described in @heffernanPerspectivesBasicReproductive2005\n#'\n#' @param beta_mat A matrix of beta parameter values\n#' @param stable_n_mat A matrix of the stable age distributions\n#' @param aging_mat A matrix of the aging rates between age compartments\n#' @param recovery_rate The recover rate parameter value (of type double)\n#'\n#' @return The R0 value as type double\n#' @examples\n#' calculate_R0(\n#'    beta_mat = multistage_params[[\"beta_mat\"]],\n#'    stable_n_mat = multistage_stable_n,\n#'    aging_mat = multistage_params[[\"aging_mat\"]],\n#'    recovery_rate = multistage_params[[\"recovery\"]]\n#')\ncalculate_R0 &lt;- function(beta_mat, stable_n_mat, aging_mat, recovery_rate) {\n    # evaluate new inf jac pde at dfe\n    f_mat &lt;- beta_mat * stable_n_mat\n\n    # set off-diag of non-inf transition jac pde to neg aging of prev age group\n    # (use aging matrix as already calculated in correct places)\n    v_mat &lt;- -aging_mat\n    # Update the diagonal of non-inf transition jac to add recovery rate\n    diag(v_mat) &lt;- diag(v_mat) + recovery_rate\n\n    ## Alternative method of calculating using age bands directly\n    # v_mat &lt;- diag(recovery_rate + 1 / da_ages)\n    # v_mat[row(v_mat) - col(v_mat) == 1] &lt;- - 1 / head(da_ages, -1)\n    \n    # spectral trace\n    R0 &lt;- max(Re(eigen(solve(v_mat, f_mat), only.values = TRUE)$values))   \n    \n    return(R0)\n}\n\n\n\nCodecalculate_R0(\n    beta_mat = multistage_params[[\"beta_mat\"]],\n    stable_n_mat = multistage_stable_n,\n    aging_mat = multistage_params[[\"aging_mat\"]],\n    recovery_rate = multistage_params[[\"recovery\"]]\n)\n\n[1] 6.991242\n\n\n\n7.4.3 Exercise 4: Updating the contact matrix\n\n\n\n\n\n\nImportant\n\n\n\nYou will need to read and edit the following code carefully so that it runs with your updated parameters. We have highlighted the relevant lines in the code chunks, so hopefully you won’t miss them, though make sure you do copy all the code!\n\n\n\n7.4.3.1 Change the juvenile-juvenile contact rate to be 0.025.\n\nCodeupdate_age_beta_mat &lt;- ages_beta_mat\nupdate_age_beta_mat[1:20, 1:20] &lt;- ?\n\nupdate_age_params &lt;- multistage_params\nupdate_age_params[[\"beta_mat\"]] &lt;- update_age_beta_mat\n\n\n\n7.4.3.1.1 Answer: Change the juvenile-juvenile contact rate to be 0.025.\n\nCodeupdate_age_beta_mat &lt;- ages_beta_mat\nupdate_age_beta_mat[1:20, 1:20] &lt;- 0.025\n\nupdate_age_params &lt;- multistage_params\nupdate_age_params[[\"beta_mat\"]] &lt;- update_age_beta_mat\n\n\n\n7.4.3.2 Simulate and plot the age-structured SIR dynamics under your assumptions and record how the age-specific seroprevalence has changed.\n\nCodeupdate_age_sol &lt;- deSolve::ode(\n    y = demog_yinit_ages,\n    times = seq(0, 400, by = 0.1),\n    func = multistage_model,\n    parms = ?\n)\n\n# Get the time series for each infectious age group\nupdate_age_infecteds &lt;- update_age_sol[, 1 + iindex]\n\n# Get the last values in the time series\nupdate_age_equil &lt;- drop(tail(update_age_sol, 1))[-1]\n\n# Calculate the number of individuals in each age group at the final timepoint\nupdate_age_equil_n &lt;- update_age_equil[ ? ] +\n    update_age_equil[ ? ] +\n    update_age_equil[ ? ]\n\n# Calculate final seroprevalence\n# Hint: You need PREVIOUSLY infected individuals\nupdate_age_equil_seroprev &lt;- update_age_equil[ ? ] / update_age_equil_n\n\nupdate_age_equil_seroprev_df &lt;- tibble(\n    age = ages,\n    seroprev = update_age_equil_seroprev,\n    width = da_ages\n)\n\nggplot(update_age_equil_seroprev_df, aes(x = age, y = seroprev, fill = age)) +\n    # Set column width to width of age bands, and justify to start at\n    # lower bound\n    geom_col(\n        width = update_age_equil_seroprev_df$width,\n        just = 1.0, color = \"black\"\n    ) +\n    labs(\n        x = \"Age\",\n        y = \"Seroprevalence\"\n    ) +\n    scale_x_continuous(breaks = seq(0, 80, 10)) +\n    scale_fill_continuous(\n        low = age_group_colors[1],\n        high = age_group_colors[2]\n    )\n\n\n\n7.4.3.2.1 Answer: Simulate and plot the age-structured SIR dynamics under your assumptions and record how the age-specific seroprevalence has changed.\n\nCodeupdate_age_sol &lt;- deSolve::ode(\n    y = demog_yinit_ages,\n    times = seq(0, 400, by = 0.1),\n    func = multistage_model,\n    parms = update_age_params\n)\n\n# Get the time series for each infectious age group\nupdate_age_infecteds &lt;- update_age_sol[, 1 + iindex]\n\nupdate_age_equil &lt;- drop(tail(update_age_sol, 1))[-1]\n\nupdate_age_equil_n &lt;- update_age_equil[sindex] +\n    update_age_equil[iindex] +\n    update_age_equil[rindex]\n\nupdate_age_equil_seroprev &lt;- update_age_equil[rindex] / update_age_equil_n\n\nupdate_age_equil_seroprev_df &lt;- tibble(\n    age = ages,\n    seroprev = update_age_equil_seroprev,\n    width = da_ages\n)\n\n\n\n\n\n\n\n\nQUESTION\n\n\n\nAt what age does the seroprevalence reach 75%? How does this compare to the answer in Section 7.4.2?\n\n\n\n7.4.3.3 Compute \\(R_0\\) for your assumptions.\nAs described previously, the calculation for \\(R_0\\) is difficult due to all the age categories and transitions. Use the calculate_R0() function we defined earlier to calculate \\(R_0\\) for our updated system.\n\nCodecalculate_R0(\n    beta_mat = ?,\n    stable_n_mat = ?,\n    aging_mat = ?,\n    recovery_rate = ?\n)\n\n\n\n7.4.3.3.1 Answer: Compute \\(R_0\\) for your assumptions.\n\nCodeupdate_age_R0 &lt;- round(\n    calculate_R0(\n        beta_mat = update_age_params[[\"beta_mat\"]],\n        stable_n_mat = multistage_stable_n,\n        aging_mat = update_age_params[[\"aging_mat\"]],\n        recovery_rate = update_age_params[[\"recovery\"]]\n    ),\n    digits = 2\n)\n\n\nIf you’ve done everything correctly, you should get \\(R_0 =\\) 7.29. This is higher than previously. Does this match your intuition, given our changes to the beta matrix?",
    "crumbs": [
      "Day 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Session 02</span>"
    ]
  },
  {
    "objectID": "r-session-02.html#r0-and-the-mean-age-of-infection",
    "href": "r-session-02.html#r0-and-the-mean-age-of-infection",
    "title": "\n7  R Session 02\n",
    "section": "\n7.5 R0 and the Mean Age of Infection",
    "text": "7.5 R0 and the Mean Age of Infection\nTo develop some intuition about the relationship between \\(R_0\\) and the mean age of infection, let’s play with an interactive plot. We will assume that the population is completely susceptible and that the force of infection is constant. We’ll also assume that there is heterogenous mixing i.e. no age structure.\nAs we’ve seen in Matt’s lecture on age structure, we can calculate the mean age of infection using the equation below:\n\\[\nA \\approx \\frac{L}{R_E - 1}\n\\tag{7.2}\\]\nwhere \\(L\\) is the life expectancy \\(\\left(L = \\frac{1}{\\mu}\\right)\\) and \\(R_E\\) is the effective reproductive number (\\(R_E = R_0 * (1 - p)\\) where \\(p\\) is the fraction of individuals vaccinated).\n\n\n\n\n\n\nNote\n\n\n\n\n\nSee Section 7.6.2 for code you can run in R to investigate the relationship between \\(R_0\\), vaccination coverage, life expectancy, and the mean age of infection.\n\n\n\n\n7.5.1 Exercise 5: mean age of infection interactions\n\n7.5.1.1 When you increase \\(R_0\\) from 2.0 to 4.0, what happens to the mean age of infection?\n\n7.5.1.1.1 Is there a linear change? If not, why not?\n\n7.5.1.2 With \\(R_0\\) to 4.0, approximately what level of vaccination coverage is required for a mean age of infection of 40 years?\n\n7.5.1.3 Leaving \\(R_0\\) and vaccination coverage the same, decrease the life expectancy to 50 years. What happens to the mean age of infection?\n\n7.5.1.3.1 If it changed, why do you think it did?\n\n\nCodeinit_R0 = 2.0\ninit_vacc = 0.0\ninit_lifeexp = 75\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodefunction set(input, value) {\n  input.value = value;\n  input.dispatchEvent(new Event(\"input\", {bubbles: true}));\n}\n\n\n\n\n\n\n\n\nCodeviewof reset = Inputs.button([\n  [\"Reset all sliders\", () =&gt; {\n    set(viewof R0, init_R0)\n    set(viewof vacc, init_vacc)\n    set(viewof lifeexp, init_lifeexp)\n  }]\n])\nviewof R0 = Inputs.range(\n    [1.0, 10.0],\n    {value: 2.0, step: 0.01, label: md`${tex`R_0`}`}\n)\n\nviewof vacc = Inputs.range(\n    [0.0, 1.0],\n    {value: 0.0, step: 0.01, label: \"Vaccination coverage\"}\n)\n\nviewof lifeexp = Inputs.range(\n    [50, 100],\n    {value: 75, step: 1, label: \"Life expectancy\"}\n)\n\nmd`${tex`R_E = ${Re_str}`}`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodemd`${tex`\\text{Mean age of infection} = ${Re_mean_age_str}`}`\n\n\n\n\n\n\n\n\n\n\nCodeRe = R0 * (1 - vacc)\nRe_str = Re.toPrecision(4).toLocaleString()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodefunction calc_mean_age(Re, lifeexp) {\n    if(Re &gt;= 1) {\n        var mean_age = (lifeexp / (Re - 1))\n    } else {\n        var mean_age = Infinity\n    }\n    return mean_age\n}\n\n\n\n\n\n\n\nCodeR0_mean_age = calc_mean_age(R0, lifeexp)\nRe_mean_age = calc_mean_age(Re, lifeexp)\nRe_mean_age_str = Re_mean_age.toPrecision(4).toLocaleString()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodeimport { aq, op } from '@uwdata/arquero'\n\n\n\n\n\n\n\nCodefunction calc_mean_age_arr(vacc, lifeexp, R0_min, R0_max, dR0) {\n    var R0_sim = R0_min\n\n    var R0 = []\n    var Re = []\n    var R0_mean_age = []\n    var Re_mean_age = []\n\n    for (R0_sim = R0_min; R0_sim &lt;= R0_max; R0_sim += dR0) {\n        var Re_sim = R0_sim * (1 - vacc)\n        var R0_mean_age_sim = calc_mean_age(R0_sim, lifeexp)\n        var Re_mean_age_sim = calc_mean_age(Re_sim, lifeexp)\n\n        R0.push(R0_sim)\n        Re.push(Re_sim)\n        R0_mean_age.push(R0_mean_age_sim)\n        Re_mean_age.push(Re_mean_age_sim)\n    }\n\n    return {\n        Re: aq.table({\n                R0: R0,\n                mean_age: Re_mean_age\n            }).filter((d) =&gt; d.mean_age &lt;= 100),\n        R0: aq.table({\n                R0: R0,\n                mean_age: R0_mean_age\n            }).filter((d) =&gt; d.mean_age &lt;= 100)\n    }\n}\n\n\n\n\n\n\n\nCodemean_age_arrs = calc_mean_age_arr(vacc, lifeexp, 1.0, 10.0, 0.01)\n\n\n\n\n\n\n\nCodemean_age_dots = [({\n    arrow_start: R0_mean_age &lt;= 100 ? R0_mean_age : 100,\n    arrow_end: Re_mean_age &lt;= 100 ? Re_mean_age : 100,\n    R0: R0.toPrecision(3),\n    Re: Re.toPrecision(3),\n    R0_mean_age,\n    Re_mean_age\n})]\n\n\n\n\n\n\n\n\n\nCode{\n    let R0Color = \"#1f77b4\"\n    let ReColor = \"#ff7f0e\"\n\n    let plot = Plot.plot({\n        color: {\n            legend: true,\n            domain: [\"Unvaccinated\", \"Vaccinated\"],\n            range: [\"#1f77b4\", \"#ff7f0e\"]\n        },\n        style: {fontSize: \"20px\"},\n        marginLeft: 65,\n        marginTop: 40,\n        marginBottom: 55,\n        grid: true,\n        width: 800,\n        height: 670,\n        x: {label: \"R0\", domain: [0, 10]},\n        y: {label: \"Mean Age of Infection\", domain: [0, 100]},\n        marks: [\n            Plot.line(mean_age_arrs.Re, {x: \"R0\", y: \"mean_age\", stroke: ReColor, strokeWidth: 6}),\n            Plot.line(mean_age_arrs.R0, {x: \"R0\", y: \"mean_age\", stroke: R0Color, strokeWidth: 6}),\n            Re_mean_age &lt;= 100 ?\n                [\n                    vacc &gt; 0.00 ? Plot.dot(mean_age_dots, {x: \"R0\", y: \"Re_mean_age\", r: 12, stroke: ReColor, fill:     ReColor,  fillOpacity: 0.6}) : null,\n                    Plot.text(\n                        mean_age_dots,\n                        {x: \"R0\", y: \"Re_mean_age\", text: (d) =&gt; `Re = ${d.Re}`, dx: 55, dy: -25, fontWeight: \"bold\", fill: ReColor}\n                    )\n                ] :\n            null,\n            R0_mean_age &lt;= 100 ?\n                [\n                Plot.dot(mean_age_dots, {x: \"R0\", y: \"R0_mean_age\", r: 12, stroke: R0Color, fill: R0Color, fillOpacity: 0.6}),\n                Plot.text(\n                    mean_age_dots,\n                    {x: \"R0\", y: \"R0_mean_age\", text: (d) =&gt; `R0 = ${d.R0}`, dx: -60, dy: 30, fontWeight: \"bold\", fill: R0Color}\n                )\n                ] :\n            null,\n            Plot.arrow(mean_age_dots, {x1: \"R0\", x2: \"R0\", y1: \"arrow_start\", y2: \"arrow_end\", strokeWidth: 4, headLength: 5, inset: 15}),\n        ]\n    });\n\n  return plot;\n}",
    "crumbs": [
      "Day 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Session 02</span>"
    ]
  },
  {
    "objectID": "r-session-02.html#bonus-materials",
    "href": "r-session-02.html#bonus-materials",
    "title": "\n7  R Session 02\n",
    "section": "\n7.6 Bonus Materials",
    "text": "7.6 Bonus Materials\n\n7.6.1 Calculating \\(R_0\\) with the Next Generation Matrix\n\n7.6.1.1 Simple model structure\nTo compute \\(R_0\\), we need to know the stable age distribution (the relative proportion in the juvenile and adult age classes) of the population, which we can find by solving for the disease-free equilibrium: \\(S_J^*=B/\\alpha\\) and \\(S_A^*=B/\\mu\\). With the stable age distribution, we can calculate \\(R_0\\) by constructing the next generation matrix. The code below outlines how the next generation matrix is constructed using the \\(\\alpha\\) (aging from juvenile to adult), \\(\\mu\\) (death), \\(n\\) (total births), \\(\\gamma\\) (recovery), \\(da\\) (width of age groups in years), and \\(\\beta\\) (transmission) parameters.\nThe next generation matrix is a matrix that specifies how many new age-specific infections are generated by a typical infected individual of each age class (in a fully susceptible population). For example, let’s consider an infected adult and ask how many new juvenile infections it generates: this is the product of the number of susceptible juveniles (from the stable age distribution), the per capita transmission rate from adults to juveniles and the average duration of infection, i.e. \\(S_J^* \\times \\beta_{JA} \\times 1/ (\\gamma+\\mu)\\). This forms one element of our next generation matrix. The other elements look very similar, except there are extra terms when we consider an infected juvenile because there is a (very small) chance they may age during the infectious period and therefore cause new infections as an adult:\n\\[\n\\mathrm{NGM} = \\begin{pmatrix}\n        \\frac{S_J^* \\beta_{JJ}}{(\\gamma + \\alpha)} +\n        \\frac{\\alpha}{(\\gamma+\\mu)} \\frac{S_J^* \\beta_{JA}}{(\\gamma + \\mu)} &\n        \\frac{S_J^* \\beta_{JA}}{(\\gamma + \\mu)} \\\\\n        \\frac{S_A^* \\beta_{AJ}}{(\\gamma + \\alpha)} +\n            \\frac{\\alpha}{(\\gamma + \\mu)} \\frac{S_A^*\\beta_{AA}}{(\\gamma+\\mu)} &\n            \\frac{S_A^* \\beta_{AA}}{(\\gamma + \\mu)}\n    \\end{pmatrix}\n\\tag{7.3}\\]\n\\(R_0\\) can then be computed as the dominant eigenvalue (i.e., the one with the largest real part) of this matrix. Let’s take an example from a model with 2 age classes, from above. First, let’s define the components of the next generation matrix:\n\nCodengm_params &lt;- c(\n    beta_within = 0.011,\n    beta_between = 0.005,\n    age_band_j = 20,\n    age_band_a = 60,\n    recovery = 10\n)\n\nalpha_ngm &lt;- 1 / ngm_params[\"age_band_j\"]\nmu_ngm &lt;- 1 / ngm_params[\"age_band_a\"]\nn_ngm &lt;- demog_params[\"births\"] / c(alpha_ngm, mu_ngm)\n\nbeta_ngm &lt;- matrix(c(\n    ngm_params[\"beta_within\"],\n    ngm_params[\"beta_between\"],\n    ngm_params[\"beta_between\"],\n    ngm_params[\"beta_within\"]\n    ),\n    nrow = 2,\n    ncol = 2\n)\n\n\nThe Next Generation Matrix can be calculated in R as:\n\nCodengm &lt;- matrix(\n    c(\n        n_ngm[1] * (beta_ngm[1, 1] / (ngm_params[\"recovery\"] + alpha_ngm)) +\n            alpha_ngm / (ngm_params[\"recovery\"] + mu_ngm) *\n            n_ngm[1] * beta_ngm[1, 2] / (ngm_params[\"recovery\"] + mu_ngm),\n        \n        n_ngm[2] * beta_ngm[2, 1] / (ngm_params[\"recovery\"] + alpha_ngm) +\n            alpha_ngm / (ngm_params[\"recovery\"] + mu_ngm) *\n            n_ngm[2] * (beta_ngm[2, 2] / (ngm_params[\"recovery\"] + mu_ngm)),\n\n        n_ngm[1] * beta_ngm[1, 2] / (ngm_params[\"recovery\"] + mu_ngm),\n\n        n_ngm[2] * beta_ngm[2, 2] / (ngm_params[\"recovery\"] + mu_ngm)\n    ),\n    nrow = 2,\n    ncol = 2\n)\n\n\nWe can then calculate the eigenvalues and eigenvectors of this matrix:\n\nCodeeigen(ngm)\n\neigen() decomposition\n$values\n[1] 7.191869 1.591188\n\n$vectors\n           [,1]       [,2]\n[1,] -0.1958841 -0.8560336\n[2,] -0.9806271  0.5169202\n\n\nWe can also choose to just output the eigenvalues:\n\nCodeeigen(ngm, only.values = TRUE)\n\n$values\n[1] 7.191869 1.591188\n\n$vectors\nNULL\n\n\nFinally, let’s print \\(R_0\\):\n\nCodemax(\n    Re(\n        eigen(ngm, only.values = TRUE)$values\n    )\n)\n\n[1] 7.191869\n\n\n\n7.6.1.2 Age-structured NGM\nMany times it would be impractical to write out the NGM: there are often too many compartments in an age-structured model. In this instance, we want to use a slightly different approach, but the underlying principles are the same: each element of the NGM balances the number of new infections expected to be produced with the rates of individuals coming in and out of that compartment.\n\n7.6.1.2.1 Stable Age Distribution\nThe first thing we need, as before, is the stable age distribution i.e., the disease-free equilibrium. There are two ways we can do this:\n\nSimulate the model without any infections for a sufficiently long time (simple, but less accurate)\nDo the math.\n\n\n7.6.1.2.1.1 Disease-Free Simulation\n\nCode# Set up initial conditions without any infections\nmultistage_sonly_yinit &lt;- c(\n    S = c(rep(250, 30)),\n    I = c(rep(0, 30)),\n    R = c(rep(0, 30))\n)\n\n# Solve disease free sim to get dfe\nmultistage_sonly_sol &lt;- deSolve::ode(\n    y = multistage_sonly_yinit,\n    times = seq(0, 300, by = 1),\n    func = multistage_model,\n    parms = multistage_params\n)\n\n# Calculate population size at each time point and save to dataframe\nmultistage_sonly_pop &lt;- tibble(\n    time = multistage_sonly_sol[, 1],\n    pop = apply(multistage_sonly_sol[, -1], 1 , sum)\n)\n\n\n\nCodeggplot(multistage_sonly_pop, aes(x = time, y = pop)) +\n    geom_area(fill = SIRcolors[4], alpha = 0.6) +\n    labs(\n        x = \"Time\",\n        y = \"Population size\"\n    )\n\n\n\n\n\n\n\n\n7.6.1.2.1.2 Doing the Math\nAlternatively, we can get the stable age distribution by finding the population structure that balances the birth, aging, and death processes. We have already seen the aging matrix in Equation 7.1, and at equilibrium, we have the matrix equation\n\\[\n\\begin{pmatrix}\n    -\\alpha_1 & 0 & 0 & \\cdots & 0\\\\\n    \\alpha_1 & -\\alpha_2 & 0 & \\cdots & 0\\\\\n    0 & \\alpha_2 & -\\alpha_3 & \\cdots & 0\\\\\n    \\vdots &  & \\ddots & \\ddots & \\vdots \\\\\n    0 & \\cdots & & \\alpha_{29} & -\\alpha_{30}\\\\\n\\end{pmatrix} .\n\\begin{pmatrix}\n    n_1 \\\\ n_2 \\\\ n_3 \\\\ \\vdots \\\\ n_{30}\n\\end{pmatrix} +\n\\begin{pmatrix}\n    B \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0\n\\end{pmatrix} =\n\\begin{pmatrix}\n    0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0\n\\end{pmatrix}\n\\]\nTo solve this equation in R, we can do\n\nCode# solve(a, b) solves the equation a %*% x = b for x, so rearrange equation\n# above so b is on the RHS of the equation\nmultistage_stable_n &lt;- solve(\n    aging_mat,\n    c(-1 * multistage_params[[\"births\"]], rep(0, 29))\n)\n\nmultistage_stable_n\n\n [1]  100  100  100  100  100  100  100  100  100  100  100  100  100  100  100\n[16]  100  100  100  100  100  500  500  500  500  500  500  500  500  500 1500\n\n\n\nCode# Check the final pop value of the S-only sim is equal to the sum of the\n# stable age distribution calculated above\nround(tail(multistage_sonly_pop$pop, 1)) == sum(multistage_stable_n)\n\n[1] TRUE\n\n\nThe following lines then compute \\(R_0\\) using the next generation matrix method. This calculation comes from a recipe described in detail previously (Diekmann and Heesterbeek 2000; Heesterbeek 2002; Bjørnstad 2018; Heffernan, Smith, and Wahl 2005; Hurford, Cownden, and Day 2009) (we would recommend starting with (Bjørnstad 2018; and Heffernan, Smith, and Wahl 2005)).\nThe steps below are copied from (Bjørnstad 2018)\n\nIdentify all n infected compartments\nConstruct a n × 1 matrix, \\(\\mathbf{F}\\), that contains expressions for all completely new infections entering each infected compartment\nConstruct a n × 1 matrix, \\(\\mathbf{V^−}\\), that contains expressions for all losses out of each infected compartment\nConstruct a n × 1 matrix, \\(\\mathbf{V^+}\\), that contains expressions for all gains into each infected compartment that does not represent new infections but transfers among infectious classes\nConstruct a n × 1 matrix, \\(\\mathbf{V} = \\mathbf{V^−} − \\mathbf{V^+}\\)\n\nGenerate two n × n Jacobian matrices \\(f\\) and \\(v\\) that are the partial derivatives of \\(\\mathbf{F}\\) and \\(\\mathbf{V}\\) with respect to the \\(n\\) infectious state variables\nEvaluate the matrices at the disease free equilibrium (dfe), and finally\n\n\\(R_0\\) is the spectral trace (greatest non-negative real eigenvalue) of \\(\\mathbf{fv}^{−1}|_{\\text{dfe}}\\).\n\nWorking through these steps looks like this:\n\nOur only infected compartments are the \\(I_i\\) states, for each age group (\\(i \\in [1, 30]\\)) To start, let’s write out our differential equation:\n\n\\[\\begin{equation}\n    \\frac{\\dd{I_i}}{\\dd{t}} = \\lambda_i S_i - \\gamma I_i + \\alpha_{i-1} I_{i-1} - \\alpha_i I_i\n\\end{equation}\\]\n\nWe’ll now calculate \\(\\mathbf{F}\\) and \\(\\mathbf{f}\\)\n\n\n\\[\\begin{align*}\n    \\mathbf{F} &= \\begin{pmatrix}\n        \\lambda_1 S_1 + \\cancelto{0}{\\alpha_0 I_0} \\\\\n        \\vdots \\\\\n        \\lambda_{30} S_{30} + \\alpha_{29} I_{29}\n    \\end{pmatrix} \\\\ \\\\\n    \\mathbf{F} &= \\begin{pmatrix}\n        \\left(\\beta_{1, 1} I_1 + \\cdots + \\beta_{1, 30} I_{30} \\right)S_1 \\\\\n        \\vdots \\\\\n        \\left(\\beta_{30, 1} I_1 + \\cdots + \\beta_{30, 30} I_{30} \\right) S_{30} + \\alpha_{29} I_{29}\n    \\end{pmatrix} \\\\\n    \\mathbf{f} &= \\begin{pmatrix}\n        \\frac{\\partial F_1}{\\partial I_1} & \\cdots & \\frac{\\partial F_1}{\\partial I_{30}} \\\\\n        \\vdots & \\ddots & \\vdots \\\\\n        \\frac{\\partial F_{30}}{\\partial I_1} & \\cdots & \\frac{\\partial F_{30}}{\\partial I_{30}}\n    \\end{pmatrix} & \\frac{\\partial F_1}{\\partial I_1} &= \\frac{\\partial}{\\partial I_1} \\left( \\beta_{1, 1} I_1 + \\cancelto{0}{\\beta_{1, 2} I_2 + \\cdots + \\beta_{1, 30} I_{30}}\\right) S_1 \\\\\n    & & \\frac{\\partial F_1}{\\partial I_1} &= \\beta_{1, 1} S_1 \\\\\n    \\mathbf{f} &= \\begin{pmatrix}\n        \\beta_{1, 1} S_1 & \\cdots & \\beta_{1, 30} S_1 \\\\\n        \\vdots & \\ddots & \\vdots \\\\\n        \\beta_{30, 1} S_{30} & \\cdots & \\beta_{30, 30} S_{30}\n    \\end{pmatrix}\n\\end{align*}\\]\n\nNow let’s calculate \\(\\mathbf{V^-}\\), \\(\\mathbf{V^+}\\), \\(\\mathbf{V}\\), and \\(\\mathbf{v}\\)\n\n\n\\[\\begin{align*}\n    \\mathbf{V^-} &= \\begin{pmatrix}\n        \\gamma I_1 + \\alpha_1 I_1 \\\\\n        \\gamma I_2 + \\alpha_2 I_2 \\\\\n        \\vdots \\\\\n        \\gamma I_{30} + \\alpha_{30} I_{30}\n    \\end{pmatrix} & \\mathbf{V^+} &= \\begin{pmatrix}\n        \\cancelto{0}{\\alpha_0 I_0} \\\\\n        \\alpha_1 I_1 \\\\\n        \\vdots \\\\\n        \\alpha_{29} I_{29}\n    \\end{pmatrix} \\\\ \\\\\n    \\mathbf{V} &= \\mathbf{V^-} - \\mathbf{V^+} = \\begin{pmatrix}\n        \\gamma I_1 + \\alpha_1 I_1 \\\\\n        \\gamma I_2 + \\alpha_2 I_2 - \\alpha_1 I_1\\\\\n        \\vdots \\\\\n        \\gamma I_{30} + \\alpha_{30} I_{30} - \\alpha_{29} I_{29}\n    \\end{pmatrix} \\\\ \\\\\n    \\mathbf{v} &= \\begin{pmatrix}\n        \\pdv{V_1}{I_1} & \\cdots & \\pdv{V_1}{I_{30}} \\\\\n        \\vdots & \\ddots & \\vdots \\\\\n        \\pdv{V_{30}}{I_1} & \\cdots & \\pdv{V_{30}}{I_{30}}\n    \\end{pmatrix} & \\pdv{V_1}{I_1} &= \\pdv{I_1} I_1 \\left( \\gamma + \\alpha_1  \\right)\\\\\n    & & \\pdv{V_1}{I_1} &= \\gamma + \\alpha_1 \\\\ \\\\\n    & & \\pdv{V_2}{I_1} &= \\pdv{I_1} \\left( \\cancelto{0}{I_2 \\left( \\gamma + \\alpha_2  \\right)} - \\alpha_1 I_1 \\right)\\\\\n    & & \\pdv{V_2}{I_1} &= - \\alpha_1 \\\\ \\\\\n    & & \\pdv{V_1}{I_2} &= \\pdv{I_2} \\cancelto{0}{I_1 \\left( \\gamma + \\alpha_1  \\right)} \\\\\n    & & \\pdv{V_1}{I_2} &= 0 \\\\ \\\\\n    \\mathbf{v} &= \\begin{pmatrix}\n        \\gamma + \\alpha_1 & 0 &  \\cdots  & 0\\\\\n        - \\alpha_1 & \\gamma + \\alpha_2 & \\cdots & 0 \\\\\n        \\vdots & \\ddots & \\ddots & \\vdots \\\\\n        0 & \\cdots & - \\alpha_{29} & \\gamma + \\alpha_{30}\n    \\end{pmatrix}\n\\end{align*}\\]\n\nTo evaluate \\(\\mathbf{f}\\) and \\(\\mathbf{v}\\) at the disease-free equilibrium, we can use the results from our previous calculations. \\(\\mathbf{v}\\) doesn’t have any state terms in the equation, so it is already evaluated at \\(\\text{dfe}\\). \\(\\mathbf{f}|_{\\text{dfe}}\\) involves subsituting \\(S_i\\) for the equilibrium population distribution that balances the births and aging processes.\n\nThis translates to the function we defined earlier.\n\n7.6.2 Mean age of infection R code\nNow let’s look at how we can investigate our the relationships between the mean age of infection and \\(R_0\\) and the vaccination coverage using R. Unlike the interactive plot that simply uses Equation 7.2 to calculate the mean age of infection, we will use a more realistic age-structured model.\nLet’s return to the earlier models with an age-class mixing matrix. But this time, we’ll calculate \\(R_0\\), the mean age of infection, and the number of cases that occur in individuals between 15-35 years as we increase the contact rate.\nRecall from the rubella and congenital rubella syndrome (CRS) example that the risk of severe disease outcomes depends on the risk of infection in reproductive age women (here we’ll use individuals between 15 and 35 years as a proxy; in reality we would want to account for the differential rate of reproduction at different ages, including those above 35 years). Recall also that increasing vaccination reduces \\(R_E\\) – for simplicity here, so we don’t have to add vaccination into the code, we’ll simply change \\(R_0\\) because we already know that will give us outcomes that are dynamically equivalent to increasing the proportion of children born who are vaccinated. We’ll then calculate how the mean age of infection changes, and specifically how the absolute number of cases among individuals between the ages of 15-35 (as a proxy for reproductive age women) changes. To do so, we’ll make a loop and evaluate the code for each of 10 decreaing levels of mixing (which will reduce \\(R_0\\) and we can interpret as analogous to the reduction in \\(R_E\\) that would result from increasing vaccination).\n\n\n\n\n\n\nNote about map()\n\n\n\n\n\nAs you may have noticed previously, we often use the map_*() series of functions. We’ll use that again here (map_dfr()). The full reasons are too complicated to get into here, but broadly speaking, the map_*() functions provide us guarantees over the output of our loops. If it runs, we know that something didn’t get silently skipped, and that out output vector/list/dataframe is the same length as the inputs. The same can not be said for for() loops, and the base apply functions are more awkward to work with as they don’t have a consistent syntax across the family of functions.\nTo learn more, read this section of our R primer.\n\n\n\n\nCode# Create vector of scalings to reduce R0\nscale_contact &lt;- seq(1, .2, length = 10)\n\n# Create a new transmission matrix\nbeta_low &lt;- 0.007\nbeta_medium &lt;- 0.02\nbeta_high &lt;- 0.03\n\nbeta_mat &lt;- matrix(beta_low, nrow = 30, ncol = 30)\nbeta_mat[1:20, 1:20] &lt;- beta_medium\nbeta_mat[6:16, 6:16] &lt;- beta_high\n\nscaled_params &lt;- multistage_params\n\n\n\nCode# Create a dataframe where each row relates to a different R0 value\nR0_mean_age_contacts_df &lt;- map_dfr(\n    # Apply the function to each item in the vector of R0 scaling factors\n    .x = scale_contact,\n    .f = function(.x) {\n        # Scale contacts\n        scaled_beta_mat &lt;- beta_mat * .x\n\n        # Set up parameters\n        scaled_params[[\"beta_mat\"]] &lt;- scaled_beta_mat\n\n        # Solve the model\n        sol &lt;- deSolve::ode(\n            y = demog_yinit_ages,\n            times = seq(0, 400, by = 0.1),\n            func = multistage_model,\n            parms = scaled_params\n        )\n\n        # Get stable age distribution\n        stable_n &lt;- solve(\n            scaled_params[[\"aging_mat\"]],\n            -c(scaled_params[[\"births\"]], rep(0, 29))\n        )\n\n        R0 &lt;- calculate_R0(\n            beta_mat = scaled_params[[\"beta_mat\"]],\n            stable_n_mat = stable_n,\n            aging_mat = scaled_params[[\"aging_mat\"]],\n            recovery = scaled_params[[\"recovery\"]]\n        )\n\n        sol_dims &lt;- dim(sol)\n\n        final_age_sizes &lt;- sol[sol_dims[1], 2:sol_dims[2]]\n\n        # Get final number of infected individuals for each S, I, and R class\n        susceptibles &lt;- final_age_sizes[sindex]\n        infecteds &lt;- final_age_sizes[iindex]\n        recovereds &lt;- final_age_sizes[rindex]\n\n        # Calculate mean age of infection\n        mean_age &lt;- sum(ages * infecteds / sum(infecteds))\n\n        # Calculate sum of cases between 15-35 years recall, from the figures\n        # above, that that this is the equilibrium prevalence of infection in\n        # these age classes, or the average number of individuals that are\n        # infected at any given time in these age classes. Note that we're not\n        # differentiating between individuals who can and cannot get pregnant\n        # here. So we're making an implicit assumption that there no\n        # difference in the risk of rubella infection in these groups so that\n        # if prevalence goes up in one group, it goes up in the other.\n        sum_cases &lt;- sum(infecteds[15:23])\n\n        total_15_35 &lt;- sum(susceptibles[15:23]) +\n            sum_cases +\n            sum(recovereds[15:23])\n\n        # Calculate the prevalence as a proportion per 100000 population\n        prev_perc &lt;- sum_cases * 100000 / total_15_35\n\n        # Return a dataframe with the values\n        return(tibble(R0, mean_age, sum_cases, prev_perc))\n    }\n)\n\n\nNow we can make a table of the results and plot mean age and the sum of cases between 15-35 years of age as a function of \\(R_0\\).\n\nCode# Create a table from the dataframe\ngt(R0_mean_age_contacts_df) %&gt;%\n    fmt_number(\n        columns = everything(),\n        decimals = 2\n    ) %&gt;%\n    # Relabel the column headers\n    cols_label(\n        R0 = md(\"**R0**\"),\n        mean_age = md(\"**Mean age of&lt;br&gt;infection**\"),\n        sum_cases = md(\"**Total cases between&lt;br&gt;15-35 years**\"),\n        prev_perc = md(\"**Prevalence (per 100_000) &lt;br&gt; between 15-35 years**\")\n    ) %&gt;%\n    # Apply style to the table with gray alternating rows\n    opt_stylize(style = 1, color = 'gray') %&gt;%\n    # Increate space between columns\n    opt_horizontal_padding(scale = 3) %&gt;%\n    cols_align(\"center\")\n\n\n\n\n\n\n\n\n\n\n\nR0\nMean age of\ninfection\nTotal cases between\n15-35 years\nPrevalence (per 100_000)\nbetween 15-35 years\n\n\n\n6.85\n6.08\n0.54\n25.91\n\n\n6.24\n6.61\n0.66\n31.49\n\n\n5.63\n7.28\n0.81\n38.34\n\n\n5.02\n8.15\n0.98\n46.74\n\n\n4.41\n9.30\n1.20\n56.95\n\n\n3.81\n10.87\n1.45\n69.10\n\n\n3.20\n13.11\n1.74\n82.80\n\n\n2.59\n16.40\n2.01\n95.94\n\n\n1.98\n21.50\n2.11\n100.71\n\n\n1.37\n29.72\n1.48\n70.44\n\n\n\n\n\n\n\nCodeR0_mean_age_contacts_df %&gt;%\n    select(-sum_cases) %&gt;%\n    # Convert to long data frame for facet plotting\n    pivot_longer(-R0, names_to = \"metric\", values_to = \"value\") %&gt;%\n    ggplot(aes(x = R0, y = value)) +\n    geom_line(color = \"slategray4\") +\n    geom_point(shape = 21, size = 5, fill = \"slategray4\", alpha = 0.8) +\n    facet_wrap(\n        ~metric,\n        scales = \"free_y\",\n        labeller = as_labeller(c(\n            mean_age = \"Mean Age of Infection\",\n            prev_perc = \"Prevalence (per 100_000) between 15-35 years\"\n        ))\n    ) +\n    labs(\n        x = \"R0\",\n        y = \"Value\"\n    )\n\n\n\n\n\n\n\n\n7.6.3 What do real contact networks look like?\nThe POLYMOD study (Mossong et al. 2008) was a journal-based look into the contact network in contemporary European society. Let’s have a look what these data tell us about the contact structure.\n\nCodemossong_cont_net &lt;- rio::import(\"https://raw.githubusercontent.com/arnold-c/SISMID-Module-02_2023/main/data/mossong-matrix.csv\")\n# mossong_cont_net &lt;- rio::import(here::here(\"data\", \"mossong-matrix.csv\"))\n\nmossong_ages &lt;- unique(mossong_cont_net$contactor)\nmossong_cont_net$contactor &lt;- ordered(\n    mossong_cont_net$contactor,\n    levels = mossong_ages\n)\n\nmossong_cont_net$contactee &lt;- ordered(\n    mossong_cont_net$contactee,\n    levels = mossong_ages\n)\n\n\nSince contacts are symmetric, we’ll need to estimate the symmetric contact matrix.\n\nCodemossong_mat &lt;- mossong_cont_net %&gt;%\n    pivot_wider(\n        names_from = contactor,\n        values_from = contact.rate\n    ) %&gt;%\n    select(-contactee) %&gt;%\n    as.matrix()\n\nrownames(mossong_mat) &lt;- mossong_ages\n\n# Create a symmetrical contact matrix\nmossong_mat_sym &lt;- (mossong_mat + t(mossong_mat)) / 2\n\n\nHere we’ll use the filled.contour function to visualize the contact matrix, to show you an alternative way of visualizing contact matrices. Notices that we are using the raw matrix object, not a long dataframe, as previously.\n\nCodefilled.contour(\n    ages, ages, log10(mossong_mat),\n    plot.title = title(\n        main = \"Log10 of Raw Contact Rate\",\n        xlab = \"Age of Contactor\",\n        ylab = \"Age of Contactee\"\n    )\n)\n\n\n\n\n\n\n\n\nCodefilled.contour(\n    ages, ages, log10(mossong_mat_sym),\n    plot.title = title(\n        main = \"Log10 of Symmetrical Contact Rate\",\n        xlab = \"Age of Contactor\",\n        ylab = \"Age of Contactee\"\n    )\n)\n\n\n\n\n\n\n\n\nCodemossong_cont_sums &lt;- tibble(\n        age = factor(mossong_ages, levels = mossong_ages),\n        contactees = rowSums(mossong_mat),\n        contactors = colSums(mossong_mat)\n    ) %&gt;%\n    pivot_longer(-age, names_to = \"type\", values_to = \"total_contacts\") \n\n\n\nCodeggplot(\n        mossong_cont_sums,\n        aes(\n            x = age, y = total_contacts, \n            color = type, fill = type, group = type\n        )\n    ) +\n    geom_path(linewidth = 1) +\n    geom_point(\n        position = \"identity\",\n        alpha = 0.8,\n        shape = 21,\n        size = 4\n    ) +\n    scale_color_manual(\n        values = c(\"slategray4\", \"navy\"),\n        labels = c(\"Contactees\", \"Contactors\"),\n        aesthetics = c(\"color\", \"fill\")\n    ) +\n    guides(color = \"none\") +\n    labs(\n        x = \"Age\",\n        y = \"Total contacts\",\n        fill = \"Type of contact\"\n    ) +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nWhile this matrix tells us how many contacts are made per year by an individual of each age, it doesn’t tell us anything about the probability that a contact results in communication of infection. Let’s assume that each contact has a constant probability \\(q\\) of resulting in a transmission event.\n\nCodeq &lt;- 3e-5\nmossong_beta_mat &lt;- q * mossong_mat_sym\n\n\n\nCodefilled.contour(\n    ages, ages, log10(mossong_beta_mat),\n    plot.title = title(\n        main = \"WAIFW matrix based on POLYMOD data\",\n        xlab = \"Age\",\n        ylab = \"Age\"\n    )\n)\n\n\n\n\n\n\n\nNow let’s simulate the introduction of such a pathogen into a population characterized by this contact structure.\n\nCode# Update the parameters with the POLYMOD-based beta matrix\nmossong_params &lt;- multistage_params\nmossong_params[[\"beta_mat\"]] &lt;- mossong_beta_mat\n\n# Solve the model with the updated parameters\nmossong_sol &lt;- deSolve::ode(\n    y = demog_yinit_ages,\n    times = seq(0, 200, by = 0.5),\n    func = multistage_model,\n    parms = mossong_params\n)\n\n# Extract the timeseries of infectious individuals\nmossong_infecteds &lt;- mossong_sol[, 1 + iindex]\n\n# Convert infectious individual timeseries to dataframe for plotting\nmossong_infecteds_df &lt;- tibble(\n        time = mossong_sol[, 1],\n        Juveniles = apply(mossong_infecteds[, juvies], 1, sum),\n        Adults = apply(mossong_infecteds[, adults], 1, sum)\n    ) %&gt;%\n    pivot_longer(\n        cols = c(Juveniles, Adults),\n        names_to = \"age_group\",\n        values_to = \"infections\"\n    ) %&gt;%\n    mutate(\n        age_group = factor(age_group, levels = c(\"Juveniles\", \"Adults\"))\n    )\n\n\n\nCodeggplot(\n        mossong_infecteds_df,\n        aes(x = time, y = infections, color = age_group)\n    ) +\n    geom_line(linewidth = 1.5) +\n    scale_color_manual(\n        values = age_group_colors\n    ) +\n    labs(\n        x = \"Time\",\n        y = \"Number of infections\",\n        color = \"Age group\"\n    )\n\n\n\n\n\n\n\nAs before, we can also look at the equilibrium seroprevalence\n\nCode# Get last time point\nmossong_equil &lt;- drop(tail(mossong_sol, 1))[-1]\n\n# Calculate number of individuals in each age group at end of simulation\nmossong_equil_n &lt;- mossong_equil[sindex] +\n    mossong_equil[iindex] +\n    mossong_equil[rindex]\n\n# Calculate equilibrium seroprevalence\nmossong_equil_seroprev &lt;- mossong_equil[rindex] / mossong_equil_n\n\n# Convert to dataframe for plotting\nmossong_equil_seroprev_df &lt;- tibble(\n    # We can reuse the ages vectors from before as they are the same\n    # as the POLYMOD data\n    age = ages,\n    seroprev = mossong_equil_seroprev,\n    width = da_ages\n)\n\n\n\nCodeggplot(mossong_equil_seroprev_df, aes(x = age, y = seroprev, fill = age)) +\n    geom_col(\n        width = mossong_equil_seroprev_df$width,\n        just = 1.0,\n        color = \"black\"\n    ) +\n    labs(\n        x = \"Age\",\n        y = \"Seroprevalence\"\n    ) +\n    scale_x_continuous(breaks = seq(0, 80, 10)) +\n    scale_fill_continuous(\n        low = age_group_colors[1],\n        high = age_group_colors[2]\n    )\n\n\n\n\n\n\n\nand compute the \\(R_0\\) for this infection.\n\nCodemossong_stable_n &lt;- solve(\n    mossong_params[[\"aging_mat\"]],\n    -c(mossong_params[[\"births\"]], rep(0, 29))\n)\n\ncalculate_R0(\n    beta_mat = mossong_params[[\"beta_mat\"]],\n    stable_n_mat = mossong_stable_n,\n    aging_mat = mossong_params[[\"aging_mat\"]],\n    recovery = mossong_params[[\"recovery\"]]\n)\n\n[1] 7.058675\n\n\n\n\n\n\n\n\nQUESTION\n\n\n\nHow does this R0 value compare to the R0 value obtained from Section 7.4.2?\n\n\n\n\n\n\n\n\nBjørnstad, Ottar N. 2018. “Advanced: The Next-Generation Matrix.” In Epidemics: Models and Data Using R, 51. Use R! Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-97487-3.\n\n\nDiekmann, O., and J. A. P. Heesterbeek. 2000. Mathematical Epidemiology of Infectious Diseases: Model Building, Analysis and Interpretation. Wiley Series in Mathematical & Computational Biology. Wiley. https://books.google.ca/books?id=5VjSaAf35pMC.\n\n\nHeesterbeek, J. A. P. 2002. “A Brief History of R0 and a Recipe for Its Calculation.” Acta Biotheoretica 50 (3): 189–204. https://doi.org/10.1023/A:1016599411804.\n\n\nHeffernan, J. M, R. J Smith, and L. M Wahl. 2005. “Perspectives on the Basic Reproductive Ratio.” J R Soc Interface 2 (4): 281–93. https://doi.org/10.1098/rsif.2005.0042.\n\n\nHurford, Amy, Daniel Cownden, and Troy Day. 2009. “Next-Generation Tools for Evolutionary Invasion Analyses.” Journal of The Royal Society Interface 7 (45): 561–71. https://doi.org/10.1098/rsif.2009.0448.\n\n\nKing, Aaron A, and Helen J Wearing. 2011. “Age Structured Models.” In. https://ms.mcmaster.ca/~bolker/eeid/2011_eco/waifw.pdf.\n\n\nMossong, Joël, Niel Hens, Mark Jit, Philippe Beutels, Kari Auranen, Rafael Mikolajczyk, Marco Massari, et al. 2008. “Social Contacts and Mixing Patterns Relevant to the Spread of Infectious Diseases.” PLOS Medicine 5 (3): e74. https://doi.org/10.1371/journal.pmed.0050074.",
    "crumbs": [
      "Day 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Session 02</span>"
    ]
  },
  {
    "objectID": "L06_parameter-estimation.html",
    "href": "L06_parameter-estimation.html",
    "title": "8  Parameter Estimation",
    "section": "",
    "text": "Download slides",
    "crumbs": [
      "Day 2",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "r-session-03.html",
    "href": "r-session-03.html",
    "title": "\n9  R Session 03\n",
    "section": "",
    "text": "9.1 Setup\nCodelibrary(here)\nlibrary(rio)\nlibrary(deSolve)\nlibrary(tidyverse)\nlibrary(ggtext)\nlibrary(gt)\nCodetheme_set(theme_minimal())\nCode# Loads the datasets: flu, measles, niamey, plauge\nflu &lt;- rio::import(\"https://raw.githubusercontent.com/arnold-c/SISMID-Module-02_2023/main/data/flu.csv\")\nniamey &lt;- rio::import(\"https://raw.githubusercontent.com/arnold-c/SISMID-Module-02_2023/main/data/niamey.csv\")\n# flu &lt;- rio::import(here::here(\"data\", \"flu.csv\"))\n# niamey &lt;- rio::import(here::here(\"data\", \"niamey.csv\"))",
    "crumbs": [
      "Day 2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>R Session 03</span>"
    ]
  },
  {
    "objectID": "r-session-03.html#estimating-r_0-problem-background",
    "href": "r-session-03.html#estimating-r_0-problem-background",
    "title": "\n9  R Session 03\n",
    "section": "\n9.2 Estimating \\(R_0\\) Problem Background",
    "text": "9.2 Estimating \\(R_0\\) Problem Background\nSo far in this class we have focused on the theory of infectious disease. Often, however, we will want to apply this theory to particular situations. One of the key applied problems in epidemic modeling is the estimation of \\(R_0\\) from outbreak data. In this session, we study two methods for estimating \\(R_0\\) from an epidemic curve. As a running example, we will use the data on influenza in a British boarding school.\n\nCodeggplot(flu, aes(x = day, y = flu)) +\n    geom_line(color = \"slategray4\") +\n    geom_point(shape = 21, size = 5, fill = \"slategray4\", alpha = 0.8) +\n    labs(x = \"Day\", y = \"Active Influenza Cases\")",
    "crumbs": [
      "Day 2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>R Session 03</span>"
    ]
  },
  {
    "objectID": "r-session-03.html#estimating-r_0-from-the-final-outbreak-size",
    "href": "r-session-03.html#estimating-r_0-from-the-final-outbreak-size",
    "title": "\n9  R Session 03\n",
    "section": "\n9.3 Estimating \\(R_0\\) From The Final Outbreak Size",
    "text": "9.3 Estimating \\(R_0\\) From The Final Outbreak Size\nOur first approach is to estimate \\(R_0\\) from the final outbreak size. Although unhelpful at the early stages of an epidemic (before the final epidemic size is observed), this method is nonetheless a useful tool for post hoc analysis. The method is general and can be motivated by the argument listed in (Keeling and Rohani 2008):\nFirst, we assume that the epidemic is started by a single infectious individual in a completely susceptible population. On average, this individual infects \\(R_0\\) others. The probability a particular individual escaped infection is therefore \\(e^{-R_0 / N}\\).\nIf \\(Z\\) individuals have been infected, the probability of an individual escaping infection from all potential sources is \\(e^{-Z R_0 / N}\\). It follows that at the end of the epidemic a proportion \\(R(\\infty) = Z / N\\) have been infected and the fraction remaining susceptible is \\(S(\\infty) = e^{-R(\\infty) R_0}\\), which is equal to \\(2 - R(\\infty)\\).\n\n\n\n\n\n\nNote\n\n\n\n\n\n\\(S(\\infty) = e^{-R(\\infty) R_0}\\) can be calculated by acknowledging that at equilibrium (\\(t = \\infty\\)), \\(S(\\infty) = 1 - R(\\infty) = Z / N\\), so substituting \\(R(\\infty)\\) into \\(1 - e^{-Z R_0 / N}\\) gives the desired result.\nIt could also be calculated by dividing \\(\\frac{\\dd{S}}{\\dd{t}}\\) by \\(\\frac{\\dd{R}}{\\dd{t}}\\):\n\\[\\begin{aligned}\n\\frac{\\dd{S}}{\\dd{R}} &= - \\frac{\\beta S}{\\gamma} \\\\\n&= - R_0 S\n\\end{aligned}\\]\nwhich is a separable differential equation, so can be integrated as follows:\n\\[\\begin{aligned}\n- \\int_{0}^{t} \\frac{1}{R_0 S} \\dd{S} &= \\int_{0}^{t} \\dd{R} \\\\\n- \\frac{1}{R_0} \\left(\\ln{S(t)} - \\ln{S(0)} \\right) &= R(t) - \\cancelto{0}{R(0)} \\\\\n\\ln{S(t)} &= \\ln{S(0)} - R_0 R(t) \\\\\nS(t) &= S(0) e^{-R_0 R(t)}\n\n\\end{aligned}\\]\n\n\n\nPutting this together, we get:\n\\[\n1 - R(\\infty) - e^{-R(\\infty) R_0} = 0\n\\]\nRearranging, we have the estimator\n\\[\n  \\hat{R_0} = \\frac{\\ln(1 - Z / N)}{-Z / N},\n\\]\nwhich, in this case, evaluates to \\(\\frac{\\ln(1 - 512 / 764)}{-512 / 764} = 1.655\\).\n\n9.3.1 Exercise 1\nThis equation shows the important one-to-one relationship between \\(R_0\\) and the final epidemic size. Plot the relationship between the total epidemic size and \\(R_0\\) for the complete range of values between 0 and 1.",
    "crumbs": [
      "Day 2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>R Session 03</span>"
    ]
  },
  {
    "objectID": "r-session-03.html#linear-approximation",
    "href": "r-session-03.html#linear-approximation",
    "title": "\n9  R Session 03\n",
    "section": "\n9.4 Linear Approximation",
    "text": "9.4 Linear Approximation\nThe next method we introduce takes advantage of the fact that during the early stages of an outbreak, the number of infected individuals is given approximately as \\(I(t) \\approx I_0 e^{((R_0 - 1)(\\gamma + \\mu)t)}\\). Taking logarithms of both sides, we have \\(\\ln(I(t)) \\approx \\ln(I_0) + (R_0 - 1)(\\gamma + \\mu)t\\), showing that the log of the number of infected individuals is approximately linear in time with a slope that reflects both \\(R_0\\) and the recovery rate.\nThis suggests that a simple linear regression fit to the first several data points on a log-scale, corrected to account for \\(\\gamma\\) and \\(\\mu\\), provides a rough and ready estimate of \\(R_0\\). For flu, we can assume \\(\\mu =0\\) because the epidemic occurred over a time period during which natural mortality is negligible. Further, assuming an infectious period of about 2.5 days, we use \\(\\gamma = (2.5)^{-1} = 0.4\\) for the correction. Fitting to the first four data points, we obtain the slope as follows.\n\nCode# Fit a linear model\nlinear_model &lt;- lm(log(flu[1:4]) ~ day[1:4], data = flu)\n\n# Summary statistics for fit model\nsummary(linear_model)\n\n\nCall:\nlm(formula = log(flu[1:4]) ~ day[1:4], data = flu)\n\nResiduals:\n       1        2        3        4 \n 0.03073 -0.08335  0.07450 -0.02188 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) -0.02703    0.10218  -0.265  0.81611   \nday[1:4]     1.09491    0.03731  29.346  0.00116 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08343 on 2 degrees of freedom\nMultiple R-squared:  0.9977,    Adjusted R-squared:  0.9965 \nF-statistic: 861.2 on 1 and 2 DF,  p-value: 0.001159\n\nCode# Extract slope parameter\ncoef(linear_model)[2]\n\nday[1:4] \n1.094913 \n\n\nRearranging the linear equation above and denoting the slope coefficient by \\(\\hat \\beta_1\\) we have the estimator \\(\\hat R_0 = \\hat \\beta_1 / \\gamma + 1\\) giving \\(\\hat R_0 = 1.094913 / 0.4 + 1 \\approx 3.7\\).\n\n9.4.1 Exercise 2\nOur estimate assumes that boys remained infectious during the natural course of infection. The original report on this epidemic indicates that boys found to have symptoms were immediately confined to bed in the infirmary. The report also indicates that only 1 out of 130 adults at the school exhibited any symptoms. It is reasonable, then, to suppose that transmission in each case ceased once he had been admitted to the infirmary. Supposing admission happened within 24 hours of the onset of symptoms. How does this affect our estimate of \\(R_0\\)? Twelve hours?\n\n9.4.2 Exercise 3\nBiweekly data for outbreaks of measles in three communities in Niamey, Niger are provided in the dataframe niamey. Use this method to obtain estimates of \\(R_0\\) for measles from the first community assuming that the infectious period is approximately two weeks or \\(\\frac{14}{365} \\approx 0.0384\\) years.\n\n9.4.3 Exercise 4\nA defect with this method is that it uses only a small fraction of the information that might be available, i.e., the first few data points. Indeed, there is nothing in the method that tells one how many data points to use–this is a matter of judgment. Further, there is a tradeoff in that as more and more data points are used the precision of the estimate increases, but this comes at a cost of additional bias. Plot the estimate of \\(R_0\\) obtained from \\(n=3, 4, 5, ...\\) data points against the standard error of the slope from the regression analysis to show this tradeoff.",
    "crumbs": [
      "Day 2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>R Session 03</span>"
    ]
  },
  {
    "objectID": "r-session-03.html#estimating-dynamical-parameters-with-least-squares",
    "href": "r-session-03.html#estimating-dynamical-parameters-with-least-squares",
    "title": "\n9  R Session 03\n",
    "section": "\n9.5 Estimating dynamical parameters with least squares",
    "text": "9.5 Estimating dynamical parameters with least squares\nThe objective of the previous exercise was to estimate \\(R_0\\). Knowing \\(R_0\\) is critical to understanding the dynamics of any epidemic system. It is, however, a composite quantity and is not sufficient to completely describe the epidemic trajectory. For this, we require estimates for all parameters of the model. In this exercise, we introduce a simple approach to model estimation called least squares fitting, sometimes called trajectory matching. The basic idea is that we find the values of the model parameters that minimize the squared differences between model predictions and the observed data. To demonstrate least squares fitting, we consider an outbreak of measles in Niamey, Niger, reported on by (Grais et al. 2006).\n\nCode# Replace an \"NA\"\nniamey[5, 3] &lt;- 0\n\nniamey_df &lt;- niamey %&gt;%\n    # Rename columns to remove automatic \"V1\" etc columns names\n    rename_with(., ~paste0(\"Site_\", str_remove(.x, \"V\"))) %&gt;%\n    # Add a column for the biweekly time period\n    mutate(biweek = 1:16) %&gt;%\n    # Convert to long format for plotting\n    pivot_longer(\n        cols = contains(\"Site\"),\n        names_to = \"site\",\n        values_to = \"cases\"\n    )\n\n\n\nCode# Create a vector of colors for each site in the Niamey dataset\nniamey_site_colors &lt;- RColorBrewer::brewer.pal(3, \"Dark2\")\n# Assign names to the colors\nnames(niamey_site_colors) &lt;- unique(niamey_df$site)\n\n# Create a vector of labels for each site for nicer plotting legends\nniamey_site_labels &lt;- str_replace_all(names(niamey_site_colors), \"_\", \" \")\nnames(niamey_site_labels) &lt;- names(niamey_site_colors)\n\n\n\nCodeggplot(\n        niamey_df,\n        aes(x = biweek, y = cases, color = site, fill = site, group = site)\n    ) +\n    geom_line() +\n    geom_point(shape = 21, size = 5, alpha = 0.8) +\n    scale_color_manual(\n        values = niamey_site_colors,\n        aesthetics = c(\"color\", \"fill\"),\n        labels = niamey_site_labels\n    ) +\n    guides(color = \"none\") +\n    labs(x = \"Biweek\", y = \"Number of Cases\", fill = \"Site Number\") +\n    theme(legend.position = \"bottom\")",
    "crumbs": [
      "Day 2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>R Session 03</span>"
    ]
  },
  {
    "objectID": "r-session-03.html#dynamical-model",
    "href": "r-session-03.html#dynamical-model",
    "title": "\n9  R Session 03\n",
    "section": "\n9.6 Dynamical Model",
    "text": "9.6 Dynamical Model\nFirst, we write a specialized function for simulating the SIR model in a case where the removal rate is “hard-wired” and with no demography.\n\nCode#' Basic SIR model\n#'\n#' A basic SIR model with no demographic structure to be used in deSolve\n#'\n#' @param time deSolve passes the time parameter to the function.\n#' @param state A vector of states.\n#' @param params The beta parameter\n#' @param ... Other arguments passed by deSolve.\n#'\n#' @return A deSolve matrix of states at each time step.\n#' @examples\n#' sir_params &lt;- 0.0005\n#' sir_init_states &lt;- c(S = 5000, I = 1, R = 0)\n#' sim_times &lt;- seq(0, 16 / 365, by = 0.1 / 365)\n#'\n#' sir_sol &lt;- deSolve::ode(\n#'    y = sir_init_states,\n#'    times = sim_times,\n#'    func = closed_sir_model,\n#'    parms = sir_params\n#' ))\nclosed_sir_model &lt;- function (time, state , params, ...) {\n    # Unpack states\n    S &lt;- state[\"S\"]\n    I &lt;- state[\"I\"]\n\n    # Unpack parameters\n    beta &lt;- params\n    dur_inf &lt;- 14 / 365\n    gamma &lt;- 1 / dur_inf\n\n    new_inf &lt;- beta * S * I\n\n    # Calculate the ODEs\n    dSdt &lt;- -new_inf\n    dIdt &lt;- new_inf - (gamma * I)\n\n    # Return the ODEs\n    return(list(c(dSdt, dIdt, new_inf)))\n}",
    "crumbs": [
      "Day 2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>R Session 03</span>"
    ]
  },
  {
    "objectID": "r-session-03.html#interactive-optimization",
    "href": "r-session-03.html#interactive-optimization",
    "title": "\n9  R Session 03\n",
    "section": "\n9.7 Interactive Optimization",
    "text": "9.7 Interactive Optimization\n\nCodefiltered_niamey_data = aq.table(niamey_data)\n    .filter(aq.escape(d =&gt; d.site == site_select))\n\n\n\n\n\n\n\nCodereset_S = 10000\nreset_I = 20\nreset_beta = 5.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodefunction set(input, value) {\n  input.value = value;\n  input.dispatchEvent(new Event(\"input\", {bubbles: true}));\n}\n\n\n\n\n\n\n\nCodefunction sse(obs, preds) {\n\n    if(obs.length == preds.length) {\n        var squared_errs = obs.map((e, i) =&gt; (e - preds[i])**2 )\n        return squared_errs.reduce((a, b) =&gt; a + b, 0)\n    } else {\n        return(\"lengths are not the same\")\n    }\n}\n\n\n\n\n\n\n\n\nCodeviewof reset = Inputs.button([\n  [\"Reset all sliders\", () =&gt; {\n    set(viewof S0, reset_S)\n    set(viewof I0, reset_I)\n    set(viewof beta_input, reset_beta)\n  }]\n])\nviewof S0 = Inputs.range(\n    [500, 15000],\n    {value: reset_S, step: 1, label: md`${tex`S(0)`}`}\n)\n\nviewof I0 = Inputs.range(\n    [0.001, 50],\n    {value: reset_I, step: 0.001, label: md`${tex`I(0)`}`}\n)\n\nviewof beta_input = Inputs.range(\n    [1, 100],\n    {value: reset_beta, step: 0.001, label: md`${tex`\\beta (\\times 10^{-3})`}`}\n)\n\nviewof site_select = Inputs.select(\n    [\"Site 1\", \"Site 2\", \"Site 3\"],\n    {label: \"Select a site:\"}\n)\n\n// convert to daily time scale as easier to manipulate\nbeta = (beta_input / 365) * (10 ** (-3))\n\nmd`${tex`R_0 = ${R0_str}`}`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodedur_inf = 14\ngamma = 1 / dur_inf\nR0 = beta * (S0 + I0)/ gamma\n\nR0_str = R0.toPrecision(2).toLocaleString()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodedt = 0.01\ntmax = 16 * 14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodeimport {odeRK4} from '@rreusser/integration@3064'\nimport { aq, op } from '@uwdata/arquero'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodefunction sir(dydt, y, t) {\n    dydt[3] = beta * y[0] * y[1]\n\n    dydt[0] = - dydt[3]\n    dydt[1] = dydt[3] - gamma * y[1]\n    dydt[2] = gamma * y[1]\n}\n\n\n\n\n\n\n\nCodefunction simulate(f, t0, y0, dt, tmax) {\n    var t = t0\n    var y = y0\n    var i = 0\n\n    var ysim = [y0]\n\n    for (t = t0 + dt; t &lt;= tmax; t += dt) {\n        ysim.push(odeRK4([], ysim[i], f, dt))\n        i += 1\n    }\n\n    // return cumulative infections\n    return ysim.map(d =&gt; d[3])\n}\n\n\n\n\n\n\n\nCodesir_sol = simulate(sir, 0, [S0, I0, 0.0, 0.0], dt, tmax)\n\n\n\n\n\n\n\nCodesiteColors = [\"#1b9e77\", \"#d95f02\", \"#7570b3\"]\n\n\n\n\n\n\n\nCodetimes = Array.from({length: 17}, (_, i) =&gt; i * 14).slice(1)\ntindex = times.map((e, i) =&gt; e * (1 / dt))\n\ncum_inc = tindex.map((i) =&gt; sir_sol[i])\n\npreds = [cum_inc[0] + I0, ...cum_inc.map((e, i) =&gt; cum_inc[i] - cum_inc[i-1]).slice(1)]\n\nsir_tbl = aq.table({\n    Biweek: times.map(t =&gt; t / 14),\n    \"Cumulative Incidence\": cum_inc,\n    \"Number of Individuals\": preds\n})\n\nsim_sse = [({\n    sse: sse(\n            filtered_niamey_data.array(\"Number of Individuals\"),\n            preds\n        ).toPrecision(4),\n    Biweek: 3,\n    \"Number of Individuals\": Math.max(\n        ...sir_tbl.array(\"Number of Individuals\"),\n        ...filtered_niamey_data.array(\"Number of Individuals\")\n    ) * 0.9\n})]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodePlot.plot({\n    color: {\n        legend: true,\n        domain: [\"Site 1\", \"Site 2\", \"Site 3\"],\n        range: siteColors,\n    },\n    style: {fontSize: \"20px\"},\n    marginLeft: 75,\n    marginTop: 40,\n    marginBottom: 55,\n    grid: true,\n    width: 800,\n    height: 670,\n    marks: [\n        Plot.lineY(\n            sir_tbl,\n            {x: \"Biweek\", y: \"Number of Individuals\", stroke: \"#4d4d4dff\", strokeWidth: 6, strokeOpacity: 0.8}\n        ),\n        Plot.dot(\n            filtered_niamey_data,\n            {x: \"Biweek\", y: \"Number of Individuals\", stroke: \"site\", fill: \"site\", fillOpacity: 0.6, r: 12}\n        ),\n        Plot.lineY(\n            filtered_niamey_data,\n            {x: \"Biweek\", y: \"Number of Individuals\", stroke: \"site\"}\n        ),\n        Plot.text(\n            sim_sse,\n            {x: \"Biweek\", y: \"Number of Individuals\", text: (d) =&gt; `SSE = ${d.sse}`, dx: 0, dy: 0, fontWeight: \"bold\"}\n        )\n    ]\n})",
    "crumbs": [
      "Day 2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>R Session 03</span>"
    ]
  },
  {
    "objectID": "r-session-03.html#objective-function",
    "href": "r-session-03.html#objective-function",
    "title": "\n9  R Session 03\n",
    "section": "\n9.8 Objective Function",
    "text": "9.8 Objective Function\nNow we set up a function that will calculate the sum of the squared differences between the observations and the model at any parameterization (more commonly known as “sum of squared errors”). In general, this is called the objective function because it is the quantity that optimization seeks to minimize.\n\nCode#' Calculate the Sum of Squared Errors\n#'\n#' A function to take in biweekly incidence data, and SIR parameters, and\n#' calculate the SSE\n#'\n#' @param params A vector of parameter values\n#' @param data A dataframe containing biweekly incidence data in the case column\n#'\n#' @return The SSE of type double\n#' @examples\nsse_sir &lt;- function(params, data){\n    # Convert biweekly time series into annual time scale\n    # Daily time scale has requires beta values to be too small - doesn't\n    # optimize well\n    dt &lt;- 0.01\n    max_biweek &lt;- max(data$biweek)\n    t &lt;- seq(0, max_biweek * 14, dt) / 365\n\n    # Extract the number of observed incidence\n    obs_inc &lt;- data$cases\n\n    # Note the parameters are updated throughout the optimization process by\n    # the optim() function\n    # Unpack the transmission parameter and exponentiate to fit on ln scale\n    beta &lt;- exp(params[[\"beta\"]])\n\n    # Unpack the initial states and exponentiate to fit on normal scale\n    S_init &lt;- exp(params[[\"S_init\"]])\n    I_init &lt;- exp(params[[\"I_init\"]])\n\n    # Fit SIR model to the parameters\n    sol &lt;- deSolve::ode(\n            y = c(S = S_init, I = I_init, new_inf = 0),\n            times = t,\n            func = closed_sir_model,\n            parms = beta,\n            # Use rk4 as fixed time steps, which is important for indexing\n            method = \"rk4\"\n        )\n\n    # Extract the cumulative incidence\n    cum_inc &lt;- sol[, \"new_inf\"]\n\n    # Find the indices of the cumulative incidence to extract\n    biweek_index &lt;- seq(1, max_biweek) * (14 / dt) + 1\n\n    # Index cumulative incidence to get the values at the end of the biweeks\n    biweek_cum_inc &lt;- cum_inc[biweek_index]\n\n    # Calculate the biweekly incidence by using the difference between\n    # consecutive biweeks. Need to manually prepend first week's incidence\n    # and add in the initial number of infectious individuals, as ODE model\n    # only returns the cumulative differences, which is 0 at the start.\n    biweek_inc &lt;- c(biweek_cum_inc[1] + I_init, diff(biweek_cum_inc, lag = 1))\n\n    # return SSE of predicted vs observed incidence\n    return(sum((biweek_inc - obs_inc)^2))\n}\n\n\nNotice that the code for sse_sir() makes use of the following modeling trick. We know that \\(\\beta\\), \\(S_0\\), and \\(I_0\\) must be positive, but our search to optimize these parameters will be over the entire number line. We could constrain the search using a more sophisticated algorithm, but this might introduce other problems (i.e., stability at the boundaries). Instead, we parameterize our objective function (sse_sir) in terms of some alternative variables \\(\\ln(\\beta)\\), \\(\\ln(S_0)\\), and \\(\\ln(I_0)\\). While these numbers range from \\(-\\infty\\) to \\(\\infty\\) (the range of our search) they map to our model parameters on a range from \\(0\\) to \\(\\infty\\) (the range that is biologically meaningful).",
    "crumbs": [
      "Day 2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>R Session 03</span>"
    ]
  },
  {
    "objectID": "r-session-03.html#optimization",
    "href": "r-session-03.html#optimization",
    "title": "\n9  R Session 03\n",
    "section": "\n9.9 Optimization",
    "text": "9.9 Optimization\nOur final step is to use the function optim to find the values of \\(\\beta\\), \\(S_0\\), and \\(I_0\\) that minimize the sum of squared errors as calculated using our function.\nFinally, we plot these fits against the data.\n\nCode# Initial guess\nsse_optim_params &lt;- c(beta = log(0.055), S_init = log(5000), I_init = log(1))\n\n# Create a dataframe of optimized parameters\nniamey_optims &lt;- niamey_df %&gt;%\n    # Create a nested dataframe i.e. one row for each site, and the data column\n    # now is a list column that contains a separate dataframe of times and\n    # cases for each site\n    nest(data = -site) %&gt;%\n    mutate(\n        # Map the optim() function call to each of the separate dataframes\n        # stored in the nested data column we just created\n        fit = map(data, ~optim(sse_optim_params, sse_sir, data = .x)),\n        # Map the exp() function to each of the model fits just created, and\n        # output to a dataframe instead of a list (like in map()), for easier\n        # use in the plottinge predictions later\n        map_dfr(fit, ~exp(.x$par))\n    )\n\n\n\nCodeniamey_optims %&gt;%\n    select(-c(data, fit)) %&gt;%\n    mutate(site = str_replace_all(site, \"_\", \" \")) %&gt;%\n    gt() %&gt;%\n    fmt_number(columns = -site, decimals = 3) %&gt;%\n    fmt_scientific(columns = beta, decimals = 3) %&gt;%\n    # Relabel the column headers\n    cols_label(\n        site = md(\"**Site**\"),\n        beta = md(\"**Beta**\"),\n        S_init = md(\"**Initial S**\"),\n        I_init = md(\"**Initial I**\")\n    ) %&gt;%\n    # Apply style to the table with gray alternating rows\n    opt_stylize(style = 1, color = 'gray') %&gt;%\n    # Increate space between columns\n    opt_horizontal_padding(scale = 3) %&gt;%\n    cols_align(\"center\")\n\n\n\n\n\nSite\nBeta\nInitial S\nInitial I\n\n\n\nSite 1\n5.370 × 10−3\n\n8,566.648\n1.349\n\n\nSite 2\n8.317 × 10−3\n\n5,961.388\n0.201\n\n\nSite 3\n7.134 × 10−2\n\n792.990\n0.001\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou may have noticed that you can achieve slightly different results for the optimal parameter values using the interactive plot than are being presented here (though they are very similar). This is because while the optimization code is running in R, the interactive plot and the calculation of the SSE is implemented using JavaScript. Therefore, despite using the same underlying model structure, the answers will vary slightly, because the ODE solvers are different, resulting in different model simulations. The difference is not enough to be concerned with here, but it is a point that’s worth being aware of when you build your own models - you may want to perform sensitivity to confirm that your model implementation is not driving the magnitude of the results you see, and the inferences you make.\n\n\n\nCodeniamey_predictions &lt;- niamey_optims %&gt;%\n    mutate(\n        # For each of the different site's nested dataframes, fit the SIR model\n        # with the optimal parameters to get best fit predictions\n        predictions = pmap(\n            .l = list(\n                S_init = S_init,\n                I_init = I_init,\n                beta = beta,\n                time_data = data\n            ),\n            .f = function(S_init, I_init, beta, time_data)  {\n                site_times &lt;- time_data$biweek * 14 / 365\n\n                # Return a dataframe of model solutions\n                as_tibble(ode(\n                    y = c(S = S_init, I = I_init, new_inf = 0),\n                    times = site_times,\n                    func = closed_sir_model,\n                    parms = beta,\n                    hmax = 1/120\n                )) %&gt;%\n                # Make sure all values are numeric for plotting purposes\n                mutate(across(everything(), as.numeric)) %&gt;%\n                mutate(\n                    incidence = ifelse(row_number() == 1, new_inf[1], diff(new_inf, lag = 1))\n                )\n            }\n            )\n    ) %&gt;%\n    unnest(c(data, predictions))\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAn important point to note is that our data is biweekly incidence (new cases in time period), whereas out SIR model produces prevalence (total cases at any time point). To account for this, our SIR model returns the cumulative incidence (line 32 of the model code), and our objective function extracts the biweekly incidence (lines 41-54), to ensure we are fitting the same data! This is a common source of error in interpretation when people fit models to data.\n\n\n\nCode# Create a dataframe to store the positions of the text labels\nniamey_preds_labels &lt;- tibble(\n    site = c(\"Site_1\", \"Site_2\"),\n    x_label = c(6.5, 6.5),\n    x_arrow_just = c(-0.5, -0.5),\n    x_arrow_end = c(7, 7.75),\n    y_label = c(900, 600),\n    y_arrow_just = c(-80, -70),\n    y_arrow_end = c(350, 290),\n    commentary = c(\"**Predicted\", \"**Observed\"),\n    color = c(\"grey20\", niamey_site_colors[\"Site_2\"])\n)\n\nggplot(niamey_predictions, aes(x = biweek, group = site)) +\n    # Plot the actual data in color\n    geom_line(aes(y = cases, color = site)) +\n    geom_point(aes(y = cases, color = site), size = 4, alpha = 0.8) +\n    # Plot the best-fit model predictions in black\n    geom_line(aes(y = incidence), color = \"black\") +\n    scale_color_manual(\n        values = niamey_site_colors, aesthetics = c(\"color\", \"fill\")\n    ) +\n    # Place each site on it's own subplot and change labels\n    facet_wrap(\n        ~site, ncol = 1, scales = \"free_y\",\n        labeller = as_labeller(niamey_site_labels)\n    ) +\n    labs(x = \"Biweek\", y = \"Number of Case\") +\n    theme(legend.position = \"none\") +\n    ggtext::geom_textbox(\n        data = niamey_preds_labels,\n        aes(\n            label = paste0(\n                \"&lt;span style = \\\"color:\",\n                color,\n                \"\\\"&gt;\",\n                commentary,\n                \" Cases**\",\n                \"&lt;/span&gt;\"\n            ),\n            x = x_label, y = y_label\n        ),\n        size = 4, fill = NA, box.colour = NA\n    ) +\n    geom_curve(\n        data = niamey_preds_labels,\n        aes(\n            x = x_label + x_arrow_just, xend = x_arrow_end,\n            y = y_label + y_arrow_just, yend = y_arrow_end\n        ),\n        linewidth = 0.75,\n        arrow = arrow(length = unit(0.2, \"cm\")),\n        curvature = list(0.25),\n        color = \"grey20\"\n    )\n\n\n\n\n\n\n\n\n9.9.1 Exercise 5\nTo make things easier, we have assumed the infectious period is known to be 14 days. In terms of years, \\(\\text{D} = \\frac{14}{365} \\approx 0.0384\\), and the recovery rate is the inverse i.e., \\(\\gamma = \\frac{365}{14}\\). Now, modify the code above to estimate \\(\\gamma\\) and \\(\\beta\\) simultaneously.\n\n9.9.2 Exercise 6\nWhat happens if one or both of the other unknowns (\\(S_0\\) and \\(I_0\\)) is fixed instead of \\(\\gamma\\)?",
    "crumbs": [
      "Day 2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>R Session 03</span>"
    ]
  },
  {
    "objectID": "r-session-03.html#solutions",
    "href": "r-session-03.html#solutions",
    "title": "\n9  R Session 03\n",
    "section": "\n9.10 Solutions",
    "text": "9.10 Solutions\n\n9.10.1 Exercise 1\n\nCodep_infec &lt;- (seq(0, 1, by=0.001))\nr0_p &lt;- (log(1 - p_infec))/(-p_infec)\nplot(x = p_infec, y=r0_p,\n     main = \"Relationship between final proportion infected and R0\",\n     xlab = \"Final proportion infected\",\n     ylab = \"R0\")\n\n\n\n\n\n\n\n\n9.10.2 Exercise 2\n\nCode#A: If the cases were isolated after 24 hours, then gamma would be 1/1 = 1, and if the cases were isolated after 12 hours, gamma would be 1/0.5 = 2. R0 would be calculated as the beta coefficient over gamma, below:\n\nr0_g1 &lt;- 1.094913/1 + 1\nr0_g1\n\n[1] 2.094913\n\nCoder0_g2 &lt;- 1.094913/2 + 1\nr0_g2\n\n[1] 1.547457\n\n\n\n9.10.3 Exercise 3\n\nCodeniamey[5,3]&lt;-0  #replace a \"NA\"\n#the command below organizes the data so it can be plotted and analyzed\nniamey&lt;-data.frame(biweek=rep(seq(1,16),3),site=c(rep(1,16),rep(2,16),rep(3,16)),\n                   cases=c(niamey[,1],niamey[,2],niamey[,3])) #define \"biweeks\"\n\n# As the data are reported every two weeks, this corresponds to the 10th observation. Let’s fit a linear model\n\n\n\nCode# First let's see what the outbreak looks like for the first community on a linear scale\nplot(niamey$biweek[niamey$site==1],niamey$cases[niamey$site==1],type='p',col=niamey$site,xlab='Biweek',ylab='Cases')\nlines(niamey$biweek[niamey$site==1],niamey$cases[niamey$site==1])\n\n\n\n\n\n\n\n\nCode# Now let’s try it on a log scale to see until when the outbreak is roughly linear\nplot(niamey$biweek[niamey$site==1],niamey$cases[niamey$site==1],type='p',col=niamey$site,xlab='Biweek',ylab='Cases', log='y')\nlines(niamey$biweek[niamey$site==1],niamey$cases[niamey$site==1])\n\n\n\n\n\n\n\n\nCode# here we create a \"week\" variable to run the analysis on a weekly\nniamey$week &lt;- niamey$biweek*2\n# we use the `head` command to take the first N values of a vector. In this case, we're taking the first 10 values of our outcome (cases) and predictor (time) variables.\nmodel&lt;-lm(log(head(niamey$cases[niamey$site==1], 10))~(head(niamey$week[niamey$site==1], 10)))\nsummary(model) #summary statistics for fit model\n\n\nCall:\nlm(formula = log(head(niamey$cases[niamey$site == 1], 10)) ~ \n    (head(niamey$week[niamey$site == 1], 10)))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.51796 -0.07364  0.00790  0.10727  0.36805 \n\nCoefficients:\n                                        Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                              2.59679    0.17563   14.79 4.31e-07\nhead(niamey$week[niamey$site == 1], 10)  0.21960    0.01415   15.52 2.96e-07\n                                           \n(Intercept)                             ***\nhead(niamey$week[niamey$site == 1], 10) ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2571 on 8 degrees of freedom\nMultiple R-squared:  0.9678,    Adjusted R-squared:  0.9638 \nF-statistic: 240.8 on 1 and 8 DF,  p-value: 2.963e-07\n\n\n\nCode# Now let's get the slope and display it\nslope&lt;-coef(model)[2] #extract slope parameter\nslope #print to screen\n\nhead(niamey$week[niamey$site == 1], 10) \n                              0.2196041 \n\n\nAs we ran the model by weeks, the \\(\\gamma\\) value is \\(2^{-1}\\) and \\(\\hat R_0 = \\hat \\beta_1 / \\gamma +1\\) giving \\(\\hat R_0=0.2196041/0.5+1 \\approx 1.44\\).\n\n9.10.4 Exercise 4\nHere, we can use a loop and repeat the regression procedure we used above for varying numbers of initial data points in our model.\n\nCodeslope &lt;- NULL\nse &lt;- NULL\nfor(i in 3:18){\nmodel&lt;-lm(log(head(niamey$cases[niamey$site==1], i))~(head(niamey$week[niamey$site==1], i)))\nslope&lt;-c(slope,as.numeric(coef(model)[2]))\nse &lt;- c(se, summary(model)$coefficients[4])\n}\nR0 &lt;- slope/0.5 + 1\nplot(R0, se, ylab='Standard error')\n\n\n\n\n\n\n\n\n9.10.5 Exercise 5\nFirst, let’s add in \\(\\gamma\\) estimation into the sse_sir function and create a new sse_sir function called sse_sir_g\n\nCode#' Basic SIR model\n#'\n#' A basic SIR model with no demographic structure to be used in deSolve\n#'\n#' @param time deSolve passes the time parameter to the function.\n#' @param state A vector of states.\n#' @param params The beta parameter\n#' @param ... Other arguments passed by deSolve.\n#'\n#' @return A deSolve matrix of states at each time step.\n#' @examples\n#' sir_params &lt;- 0.0005\n#' sir_init_states &lt;- c(S = 5000, I = 1, R = 0)\n#' sim_times &lt;- seq(0, 16 / 365, by = 0.1 / 365)\n#'\n#' sir_sol &lt;- deSolve::ode(\n#'    y = sir_init_states,\n#'    times = sim_times,\n#'    func = closed_sir_model,\n#'    parms = sir_params\n#' )\n\n# Create a new SIR model to include gamma\nclosed_sir_model_g &lt;- function (time, state , params, ...) {\n    # Unpack states\n    S &lt;- state[\"S\"]\n    I &lt;- state[\"I\"]\n\n    # Unpack parameters\n    beta &lt;- params[[\"beta\"]]\n    gamma &lt;- params[[\"gamma\"]]\n\n    new_inf &lt;- beta * S * I\n\n    # Calculate the ODEs\n    dSdt &lt;- -new_inf\n    dIdt &lt;- new_inf - (gamma * I)\n\n    # Return the ODEs\n    return(list(c(dSdt, dIdt, new_inf)))\n}\n\n\n#' Calculate the Sum of Squared Errors\n#'\n#' A function to take in biweekly incidence data, and SIR parameters, and\n#' calculate the SSE\n#'\n#' @param params A vector of parameter values\n#' @param data A dataframe containing biweekly incidence data in the case column\n#'\n#' @return The SSE of type double\n#' @examples\nsse_sir_g &lt;- function(params, data){\n    # Convert biweekly time series into annual time scale\n    # Daily time scale has requires beta values to be too small - doesn't\n    # optimize well\n    dt &lt;- 0.01\n    max_biweek &lt;- max(data$biweek)\n    t &lt;- seq(0, max_biweek * 14, dt) / 365\n\n    # Extract the number of observed incidence\n    obs_inc &lt;- data$cases\n\n    # Note the parameters are updated throughout the optimization process by\n    # the optim() function\n    # Unpack the transmission parameter and exponentiate to fit on ln scale\n    beta &lt;- exp(params[[\"beta\"]])\n    gamma &lt;- exp(params[[\"gamma\"]])\n\n    in_parms &lt;- c(\"beta\" = beta, \"gamma\" = gamma)\n    # Unpack the initial states and exponentiate to fit on normal scale\n    S_init &lt;- exp(params[[\"S_init\"]])\n    I_init &lt;- exp(params[[\"I_init\"]])\n\n    # Fit SIR model to the parameters\n    sol &lt;- deSolve::ode(\n            y = c(S = S_init, I = I_init, new_inf = 0),\n            times = t,\n            func = closed_sir_model_g,\n            parms = in_parms,\n            # Use rk4 as fixed time steps, which is important for indexing\n            method = \"rk4\"\n        )\n\n    # Extract the cumulative incidence\n    cum_inc &lt;- sol[, \"new_inf\"]\n\n    # Find the indices of the cumulative incidence to extract\n    biweek_index &lt;- seq(1, max_biweek) * (14 / dt) + 1\n\n    # Index cumulative incidence to get the values at the end of the biweeks\n    biweek_cum_inc &lt;- cum_inc[biweek_index]\n\n    # Calculate the biweekly incidence by using the difference between\n    # consecutive biweeks. Need to manually prepend first week's incidence\n    # and add in the initial number of infectious individuals, as ODE model\n    # only returns the cumulative differences, which is 0 at the start.\n    biweek_inc &lt;- c(biweek_cum_inc[1] + I_init, diff(biweek_cum_inc, lag = 1))\n\n    # return SSE of predicted vs observed incidence\n    return(sum((biweek_inc - obs_inc)^2))\n}\n\n\nNow we can run the code to optimize, beta, gamma, I0, and S0\n\nCodesse_optim_params_g &lt;- c(beta = log(0.055), gamma = log(365/14), S_init = log(5000), I_init = log(1))\n\n# Create a dataframe of optimized parameters\nniamey_optims_g &lt;- niamey_df %&gt;%\n    # Create a nested dataframe i.e. one row for each site, and the data column\n    # now is a list column that contains a separate dataframe of times and\n    # cases for each site\n    nest(data = -site) %&gt;%\n    mutate(\n        # Map the optim() function call to each of the separate dataframes\n        # stored in the nested data column we just created\n        fit = map(data, ~optim(sse_optim_params_g, sse_sir_g, data = .x)),\n        # Map the exp() function to each of the model fits just created, and\n        # output to a dataframe instead of a list (like in map()), for easier\n        # use in the plottinge predictions later\n        map_dfr(fit, ~exp(.x$par))\n    )\n\n\nNow we can get our optimized parameters\n\nCodeniamey_optims_g %&gt;%\n    select(-c(data, fit)) %&gt;%\n    mutate(site = str_replace_all(site, \"_\", \" \")) %&gt;%\n    gt() %&gt;%\n    fmt_number(columns = -site, decimals = 3) %&gt;%\n    fmt_scientific(columns = beta, decimals = 3) %&gt;%\n    # Relabel the column headers\n    cols_label(\n        site = md(\"**Site**\"),\n        beta = md(\"**Beta**\"),\n        gamma = md(\"**Gamma**\"),\n        S_init = md(\"**Initial S**\"),\n        I_init = md(\"**Initial I**\")\n    ) %&gt;%\n    # Apply style to the table with gray alternating rows\n    opt_stylize(style = 1, color = 'gray') %&gt;%\n    # Increate space between columns\n    opt_horizontal_padding(scale = 3) %&gt;%\n    cols_align(\"center\")\n\n\n\n\n\nSite\nBeta\nGamma\nInitial S\nInitial I\n\n\n\nSite 1\n5.854 × 10−3\n\n81.150\n17,113.450\n0.786\n\n\nSite 2\n9.066 × 10−3\n\n77.613\n11,030.375\n0.142\n\n\nSite 3\n8.517 × 10−2\n\n351.697\n4,462.286\n0.000\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIt is always worth performing a sanity check on the best fit predictions. For example, as we have now estimated gamma (the recovery rate), which has a natural interpretation as the inverse of the duration of infection, we can see if the predicted values make sense. Does it make sense that gamma is different in each site?\nConverting the values to duration of infection values, we get the following results:\n\nCodeniamey_optims_g %&gt;%\n    select(site, gamma) %&gt;%\n    mutate(site = str_replace_all(site, \"_\", \" \")) %&gt;%\n    mutate(dur_inf = 1 / gamma * 365) %&gt;%\n    gt() %&gt;%\n    fmt_number(columns = -site, decimals = 3) %&gt;%\n    # Relabel the column headers\n    cols_label(\n        site = md(\"**Site**\"),\n        gamma = md(\"**Gamma**\"),\n        dur_inf = md(\"**Duration of infection (days)**\")\n    ) %&gt;%\n    # Apply style to the table with gray alternating rows\n    opt_stylize(style = 1, color = 'gray') %&gt;%\n    # Increate space between columns\n    opt_horizontal_padding(scale = 3) %&gt;%\n    cols_align(\"center\")\n\n\n\n\n\nSite\nGamma\nDuration of infection (days)\n\n\n\nSite 1\n81.150\n4.498\n\n\nSite 2\n77.613\n4.703\n\n\nSite 3\n351.697\n1.038\n\n\n\n\n\n\nDoes it make sense that the duration of infection is only 1-5 days? In situations like this, it would make sense to put constraints on the parameter space that is being optimized over, but that is beyond the scope of this tutorial.\n\n\nGetting new predictions from the optimized parameters and a new model run for each site.\n\nCodeniamey_predictions_g &lt;- niamey_optims_g %&gt;%\n    mutate(\n        # For each of the different site's nested dataframes, fit the SIR model\n        # with the optimal parameters to get best fit predictions\n        predictions = pmap(\n            .l = list(\n                S_init = S_init,\n                I_init = I_init,\n                beta = beta,\n                gamma = gamma,\n                time_data = data\n            ),\n            .f = function(S_init, I_init, beta, gamma, time_data)  {\n                site_times &lt;- time_data$biweek * 14 / 365\n\n                in_parms &lt;- c(\"beta\" = beta, \"gamma\" = gamma)\n                # Return a dataframe of model solutions\n                as_tibble(ode(\n                    y = c(S = S_init, I = I_init, new_inf = 0),\n                    times = site_times,\n                    func = closed_sir_model_g,\n                    parms = in_parms,\n                    hmax = 1/120\n                )) %&gt;%\n                # Make sure all values are numeric for plotting purposes\n                mutate(across(everything(), as.numeric)) %&gt;%\n                mutate(\n                    incidence = ifelse(row_number() == 1, new_inf[1], diff(new_inf, lag = 1))\n                )\n            }\n            )\n    ) %&gt;%\n    unnest(c(data, predictions))\n\n\nFinally we can plot:\n\nCode# Create a dataframe to store the positions of the text labels\nniamey_preds_labels_g &lt;- tibble(\n    site = c(\"Site_1\", \"Site_2\"),\n    x_label = c(6.5, 6.5),\n    x_arrow_just = c(-0.5, -0.5),\n    x_arrow_end = c(7, 7.75),\n    y_label = c(900, 600),\n    y_arrow_just = c(-80, -70),\n    y_arrow_end = c(350, 290),\n    commentary = c(\"**Predicted\", \"**Observed\"),\n    color = c(\"grey20\", niamey_site_colors[\"Site_2\"])\n)\n\nggplot(niamey_predictions_g, aes(x = biweek, group = site)) +\n    # Plot the actual data in color\n    geom_line(aes(y = cases, color = site)) +\n    geom_point(aes(y = cases, color = site), size = 4, alpha = 0.8) +\n    # Plot the best-fit model predictions in black\n    geom_line(aes(y = incidence), color = \"black\") +\n    scale_color_manual(\n        values = niamey_site_colors, aesthetics = c(\"color\", \"fill\")\n    ) +\n    # Place each site on it's own subplot and change labels\n    facet_wrap(\n        ~site, ncol = 1, scales = \"free_y\",\n        labeller = as_labeller(niamey_site_labels)\n    ) +\n    labs(x = \"Biweek\", y = \"Number of Case\") +\n    theme(legend.position = \"none\") +\n    ggtext::geom_textbox(\n        data = niamey_preds_labels_g,\n        aes(\n            label = paste0(\n                \"&lt;span style = \\\"color:\",\n                color,\n                \"\\\"&gt;\",\n                commentary,\n                \" Cases**\",\n                \"&lt;/span&gt;\"\n            ),\n            x = x_label, y = y_label\n        ),\n        size = 4, fill = NA, box.colour = NA\n    ) +\n    geom_curve(\n        data = niamey_preds_labels_g,\n        aes(\n            x = x_label + x_arrow_just, xend = x_arrow_end,\n            y = y_label + y_arrow_just, yend = y_arrow_end\n        ),\n        linewidth = 0.75,\n        arrow = arrow(length = unit(0.2, \"cm\")),\n        curvature = list(0.25),\n        color = \"grey20\"\n    )\n\n\n\n\n\n\n\n\n9.10.6 Exercise 6\nWhat happens if one or both of the other unknowns (\\(S_0\\) and \\(I_0\\)) is fixed instead of \\(\\gamma\\)?\n\n9.10.6.1 Solutions coming soon!\nFirst we modify the sse_sir_g function again to fix \\(S_0\\) and \\(I_0\\) (or both)\n\nCode# fix S0\nsse_sir_S0 &lt;- function(params, data){\n    # Convert biweekly time series into annual time scale\n    # Daily time scale has requires beta values to be too small - doesn't\n    # optimize well\n    dt &lt;- 0.01\n    max_biweek &lt;- max(data$biweek)\n    t &lt;- seq(0, max_biweek * 14, dt) / 365\n\n    # Extract the number of observed incidence\n    obs_inc &lt;- data$cases\n\n    # Note the parameters are updated throughout the optimization process by\n    # the optim() function\n    # Unpack the transmission parameter and exponentiate to fit on ln scale\n    beta &lt;- exp(params[[\"beta\"]])\n    gamma &lt;- exp(params[[\"gamma\"]])\n\n    in_parms &lt;- c(\"beta\" = beta, \"gamma\" = gamma)\n    # Unpack the initial states and exponentiate to fit on normal scale\n    S_init &lt;- 6000          #initial susceptibles (FIXED)\n    I_init &lt;- exp(params[[\"I_init\"]])\n\n    # Fit SIR model to the parameters\n    sol &lt;- deSolve::ode(\n            y = c(S = S_init, I = I_init, new_inf = 0),\n            times = t,\n            func = closed_sir_model_g,\n            parms = in_parms,\n            # Use rk4 as fixed time steps, which is important for indexing\n            method = \"rk4\"\n        )\n\n    # Extract the cumulative incidence\n    cum_inc &lt;- sol[, \"new_inf\"]\n\n    # Find the indices of the cumulative incidence to extract\n    biweek_index &lt;- seq(1, max_biweek) * (14 / dt) + 1\n\n    # Index cumulative incidence to get the values at the end of the biweeks\n    biweek_cum_inc &lt;- cum_inc[biweek_index]\n\n    # Calculate the biweekly incidence by using the difference between\n    # consecutive biweeks. Need to manually prepend first week's incidence\n    # and add in the initial number of infectious individuals, as ODE model\n    # only returns the cumulative differences, which is 0 at the start.\n    biweek_inc &lt;- c(biweek_cum_inc[1] + I_init, diff(biweek_cum_inc, lag = 1))\n\n    # return SSE of predicted vs observed incidence\n    return(sum((biweek_inc - obs_inc)^2))\n}\n\n#fix I0\nsse_sir_I0 &lt;- function(params, data){\n    # Convert biweekly time series into annual time scale\n    # Daily time scale has requires beta values to be too small - doesn't\n    # optimize well\n    dt &lt;- 0.01\n    max_biweek &lt;- max(data$biweek)\n    t &lt;- seq(0, max_biweek * 14, dt) / 365\n\n    # Extract the number of observed incidence\n    obs_inc &lt;- data$cases\n\n    # Note the parameters are updated throughout the optimization process by\n    # the optim() function\n    # Unpack the transmission parameter and exponentiate to fit on ln scale\n    beta &lt;- exp(params[[\"beta\"]])\n    gamma &lt;- exp(params[[\"gamma\"]])\n\n    in_parms &lt;- c(\"beta\" = beta, \"gamma\" = gamma)\n    # Unpack the initial states and exponentiate to fit on normal scale\n    S_init &lt;- exp(params[[\"S_init\"]])\n    I_init &lt;- 1 # initial infecteds (FIXED)\n\n    # Fit SIR model to the parameters\n    sol &lt;- deSolve::ode(\n            y = c(S = S_init, I = I_init, new_inf = 0),\n            times = t,\n            func = closed_sir_model_g,\n            parms = in_parms,\n            # Use rk4 as fixed time steps, which is important for indexing\n            method = \"rk4\"\n        )\n\n    # Extract the cumulative incidence\n    cum_inc &lt;- sol[, \"new_inf\"]\n\n    # Find the indices of the cumulative incidence to extract\n    biweek_index &lt;- seq(1, max_biweek) * (14 / dt) + 1\n\n    # Index cumulative incidence to get the values at the end of the biweeks\n    biweek_cum_inc &lt;- cum_inc[biweek_index]\n\n    # Calculate the biweekly incidence by using the difference between\n    # consecutive biweeks. Need to manually prepend first week's incidence\n    # and add in the initial number of infectious individuals, as ODE model\n    # only returns the cumulative differences, which is 0 at the start.\n    biweek_inc &lt;- c(biweek_cum_inc[1] + I_init, diff(biweek_cum_inc, lag = 1))\n\n    # return SSE of predicted vs observed incidence\n    return(sum((biweek_inc - obs_inc)^2))\n}\n\n# fix S0 and I0\nsse_sir_S0_I0 &lt;- function(params, data){\n    # Convert biweekly time series into annual time scale\n    # Daily time scale has requires beta values to be too small - doesn't\n    # optimize well\n    dt &lt;- 0.01\n    max_biweek &lt;- max(data$biweek)\n    t &lt;- seq(0, max_biweek * 14, dt) / 365\n\n    # Extract the number of observed incidence\n    obs_inc &lt;- data$cases\n\n    # Note the parameters are updated throughout the optimization process by\n    # the optim() function\n    # Unpack the transmission parameter and exponentiate to fit on ln scale\n    beta &lt;- exp(params[[\"beta\"]])\n    gamma &lt;- exp(params[[\"gamma\"]])\n\n    in_parms &lt;- c(\"beta\" = beta, \"gamma\" = gamma)\n    # Unpack the initial states and exponentiate to fit on normal scale\n    S_init &lt;- 6000    # initial susceptibles (FIXED)\n    I_init &lt;- 1 # initial infecteds (FIXED)\n\n    # Fit SIR model to the parameters\n    sol &lt;- deSolve::ode(\n            y = c(S = S_init, I = I_init, new_inf = 0),\n            times = t,\n            func = closed_sir_model_g,\n            parms = in_parms,\n            # Use rk4 as fixed time steps, which is important for indexing\n            method = \"rk4\"\n        )\n\n    # Extract the cumulative incidence\n    cum_inc &lt;- sol[, \"new_inf\"]\n\n    # Find the indices of the cumulative incidence to extract\n    biweek_index &lt;- seq(1, max_biweek) * (14 / dt) + 1\n\n    # Index cumulative incidence to get the values at the end of the biweeks\n    biweek_cum_inc &lt;- cum_inc[biweek_index]\n\n    # Calculate the biweekly incidence by using the difference between\n    # consecutive biweeks. Need to manually prepend first week's incidence\n    # and add in the initial number of infectious individuals, as ODE model\n    # only returns the cumulative differences, which is 0 at the start.\n    biweek_inc &lt;- c(biweek_cum_inc[1] + I_init, diff(biweek_cum_inc, lag = 1))\n\n    # return SSE of predicted vs observed incidence\n    return(sum((biweek_inc - obs_inc)^2))\n}\n\n\nNow we can run the optim algorithm and find the best parameters for the SIR model for each of the scenario.\nFirst, for S0 fixed:\n\nCodesse_optim_params_S0 &lt;- c(beta = log(0.055), gamma = log(365/14), I_init = log(1))\n\n# Create a dataframe of optimized parameters\nniamey_optims_S0 &lt;- niamey_df %&gt;%\n    # Create a nested dataframe i.e. one row for each site, and the data column\n    # now is a list column that contains a separate dataframe of times and\n    # cases for each site\n    nest(data = -site) %&gt;%\n    mutate(\n        # Map the optim() function call to each of the separate dataframes\n        # stored in the nested data column we just created\n        fit = map(data, ~optim(sse_optim_params_S0, sse_sir_S0, data = .x)),\n        # Map the exp() function to each of the model fits just created, and\n        # output to a dataframe instead of a list (like in map()), for easier\n        # use in the plottinge predictions later\n        map_dfr(fit, ~exp(.x$par))\n    )\n\n\nSecond, for I0 fixed:\n\nCodesse_optim_params_I0 &lt;- c(beta = log(0.055), gamma = log(365/14), S_init = log(5000))\n\n# Create a dataframe of optimized parameters\nniamey_optims_I0 &lt;- niamey_df %&gt;%\n    # Create a nested dataframe i.e. one row for each site, and the data column\n    # now is a list column that contains a separate dataframe of times and\n    # cases for each site\n    nest(data = -site) %&gt;%\n    mutate(\n        # Map the optim() function call to each of the separate dataframes\n        # stored in the nested data column we just created\n        fit = map(data, ~optim(sse_optim_params_I0, sse_sir_I0, data = .x)),\n        # Map the exp() function to each of the model fits just created, and\n        # output to a dataframe instead of a list (like in map()), for easier\n        # use in the plottinge predictions later\n        map_dfr(fit, ~exp(.x$par))\n    )\n\n\nThird, for fixing both S0 and I0:\n\nCodesse_optim_params_S0_I0 &lt;- c(beta = log(0.055), gamma = log(365/14))\n\n# Create a dataframe of optimized parameters\nniamey_optims_S0_I0 &lt;- niamey_df %&gt;%\n    # Create a nested dataframe i.e. one row for each site, and the data column\n    # now is a list column that contains a separate dataframe of times and\n    # cases for each site\n    nest(data = -site) %&gt;%\n    mutate(\n        # Map the optim() function call to each of the separate dataframes\n        # stored in the nested data column we just created\n        fit = map(data, ~optim(sse_optim_params_S0_I0, sse_sir_S0_I0, data = .x)),\n        # Map the exp() function to each of the model fits just created, and\n        # output to a dataframe instead of a list (like in map()), for easier\n        # use in the plottinge predictions later\n        map_dfr(fit, ~exp(.x$par))\n    )\n\n\nNow let’s print the estimated parameter values to see if they make sense.\nFirst, for S0 fixed:\n\nCodeniamey_optims_S0 %&gt;%\n    select(-c(data, fit)) %&gt;%\n    mutate(\n        site = str_replace_all(site, \"_\", \" \"),\n        dur_inf = 1 / gamma * 365\n    ) %&gt;%\n    gt() %&gt;%\n    fmt_number(columns = -site, decimals = 3) %&gt;%\n    fmt_scientific(columns = beta, decimals = 3) %&gt;%\n    # Relabel the column headers\n    cols_label(\n        site = md(\"**Site**\"),\n        beta = md(\"**Beta**\"),\n        gamma = md(\"**Gamma**\"),\n        dur_inf = md(\"**Duration of infection (days)**\"),\n        I_init = md(\"**Initial I**\")\n    ) %&gt;%\n    # Apply style to the table with gray alternating rows\n    opt_stylize(style = 1, color = 'gray') %&gt;%\n    # Increate space between columns\n    opt_horizontal_padding(scale = 3) %&gt;%\n    cols_align(\"center\")\n\n\n\n\n\nSite\nBeta\nGamma\nInitial I\nDuration of infection (days)\n\n\n\nSite 1\n3.007 × 10−3\n\n0.005\n5.676\n69,729.934\n\n\nSite 2\n7.944 × 10−2\n\n1,512.318\n0.719\n0.241\n\n\nSite 3\n1.879 × 10−1\n\n64,606.491\n0.021\n0.006\n\n\n\n\n\n\nSecond, for I0 fixed:\n\nCodeniamey_optims_I0 %&gt;%\n    select(-c(data, fit)) %&gt;%\n    mutate(\n        site = str_replace_all(site, \"_\", \" \"),\n        dur_inf = 1 / gamma * 365\n    ) %&gt;%\n    gt() %&gt;%\n    fmt_number(columns = -site, decimals = 3) %&gt;%\n    fmt_scientific(columns = beta, decimals = 3) %&gt;%\n    # Relabel the column headers\n    cols_label(\n        site = md(\"**Site**\"),\n        beta = md(\"**Beta**\"),\n        gamma = md(\"**Gamma**\"),\n        dur_inf = md(\"**Duration of infection (days)**\"),\n        S_init = md(\"**Initial S**\")\n    ) %&gt;%\n    # Apply style to the table with gray alternating rows\n    opt_stylize(style = 1, color = 'gray') %&gt;%\n    # Increate space between columns\n    opt_horizontal_padding(scale = 3) %&gt;%\n    cols_align(\"center\")\n\n\n\n\n\nSite\nBeta\nGamma\nInitial S\nDuration of infection (days)\n\n\n\nSite 1\n5.576 × 10−3\n\n80.618\n17,764.298\n4.528\n\n\nSite 2\n4.999 × 10−3\n\n2.362\n4,651.438\n154.516\n\n\nSite 3\n1.889 × 10−2\n\n0.098\n833.401\n3,715.587\n\n\n\n\n\n\nFinally, for both S0 and I0 fixed:\n\nCodeniamey_optims_S0_I0 %&gt;%\n    select(-c(data, fit)) %&gt;%\n    mutate(\n        site = str_replace_all(site, \"_\", \" \"),\n        dur_inf = 1 / gamma * 365\n    ) %&gt;%\n    gt() %&gt;%\n    fmt_number(columns = -site, decimals = 3) %&gt;%\n    fmt_scientific(columns = beta, decimals = 3) %&gt;%\n    # Relabel the column headers\n    cols_label(\n        site = md(\"**Site**\"),\n        beta = md(\"**Beta**\"),\n        gamma = md(\"**Gamma**\"),\n        dur_inf = md(\"**Duration of infection (days)**\"),\n    ) %&gt;%\n    # Apply style to the table with gray alternating rows\n    opt_stylize(style = 1, color = 'gray') %&gt;%\n    # Increate space between columns\n    opt_horizontal_padding(scale = 3) %&gt;%\n    cols_align(\"center\")\n\n\n\n\n\nSite\nBeta\nGamma\nDuration of infection (days)\n\n\n\nSite 1\n5.002 × 10−3\n\n8.293\n44.013\n\n\nSite 2\n1.379 × 10−1\n\n45,103.167\n0.008\n\n\nSite 3\n1.379 × 10−1\n\n45,103.167\n0.008\n\n\n\n\n\n\n\n\n\n\n\n\nDrake, John M, and Pejman Rohani. 2019. “Estimation.” In. Seattle, Washington. http://daphnia.ecology.uga.edu/drakelab/wp-content/uploads/2019/07/estimation.pdf.\n\n\nGrais, R. F., M. J. Ferrari, C. Dubray, O. N. Bjørnstad, B. T. Grenfell, A. Djibo, F. Fermon, and P. J. Guerin. 2006. “Estimating Transmission Intensity for a Measles Epidemic in Niamey, Niger: Lessons for Intervention.” Transactions of The Royal Society of Tropical Medicine and Hygiene 100 (9): 867–73. https://doi.org/10.1016/j.trstmh.2005.10.014.\n\n\nKeeling, Matthew James, and Pejman Rohani. 2008. “Introduction to Simple Epidemic Models.” In Modeling Infectious Diseases in Humans and Animals, 21–22. Princeton: Princeton University Press.",
    "crumbs": [
      "Day 2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>R Session 03</span>"
    ]
  },
  {
    "objectID": "L07_models-and-data.html",
    "href": "L07_models-and-data.html",
    "title": "10  Confronting Models with Data",
    "section": "",
    "text": "Download slides",
    "crumbs": [
      "Day 3",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Confronting Models with Data</span>"
    ]
  },
  {
    "objectID": "L08_stochastic-models.html",
    "href": "L08_stochastic-models.html",
    "title": "11  Stochastic Models",
    "section": "",
    "text": "Download slides",
    "crumbs": [
      "Day 3",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Stochastic Models</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bjørnstad, Ottar N. 2018. “Advanced: The Next-Generation\nMatrix.” In Epidemics: Models and\nData Using R, 51. Use R!\nCham: Springer International Publishing. https://doi.org/10.1007/978-3-319-97487-3.\n\n\nDiekmann, O., and J. A. P. Heesterbeek. 2000. Mathematical\nEpidemiology of Infectious Diseases: Model Building,\nAnalysis and Interpretation. Wiley Series in Mathematical &\nComputational Biology. Wiley. https://books.google.ca/books?id=5VjSaAf35pMC.\n\n\nDrake, John M, and Pejman Rohani. 2019. “Estimation.” In.\nSeattle, Washington. http://daphnia.ecology.uga.edu/drakelab/wp-content/uploads/2019/07/estimation.pdf.\n\n\nGrais, R. F., M. J. Ferrari, C. Dubray, O. N. Bjørnstad, B. T. Grenfell,\nA. Djibo, F. Fermon, and P. J. Guerin. 2006. “Estimating\nTransmission Intensity for a Measles Epidemic in Niamey,\nNiger: Lessons for Intervention.” Transactions\nof The Royal Society of Tropical Medicine and Hygiene 100 (9):\n867–73. https://doi.org/10.1016/j.trstmh.2005.10.014.\n\n\nHeesterbeek, J. A. P. 2002. “A Brief History of\nR0 and a Recipe for Its\nCalculation.” Acta Biotheoretica 50 (3):\n189–204. https://doi.org/10.1023/A:1016599411804.\n\n\nHeffernan, J. M, R. J Smith, and L. M Wahl. 2005. “Perspectives on\nthe Basic Reproductive Ratio.” J R Soc Interface 2 (4):\n281–93. https://doi.org/10.1098/rsif.2005.0042.\n\n\nHurford, Amy, Daniel Cownden, and Troy Day. 2009. “Next-Generation\nTools for Evolutionary Invasion Analyses.” Journal of The\nRoyal Society Interface 7 (45): 561–71. https://doi.org/10.1098/rsif.2009.0448.\n\n\nKeeling, Matthew James, and Pejman Rohani. 2008. “Introduction to\nSimple Epidemic Models.” In Modeling Infectious Diseases in\nHumans and Animals, 21–22. Princeton: Princeton\nUniversity Press.\n\n\nKing, Aaron A, and Helen J Wearing. 2011. “Age Structured\nModels.” In. https://ms.mcmaster.ca/~bolker/eeid/2011_eco/waifw.pdf.\n\n\nMossong, Joël, Niel Hens, Mark Jit, Philippe Beutels, Kari Auranen,\nRafael Mikolajczyk, Marco Massari, et al. 2008. “Social\nContacts and Mixing Patterns Relevant to the\nSpread of Infectious Diseases.”\nPLOS Medicine 5 (3): e74. https://doi.org/10.1371/journal.pmed.0050074.",
    "crumbs": [
      "**References**"
    ]
  }
]